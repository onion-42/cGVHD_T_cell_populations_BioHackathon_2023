{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2332ca08-25f0-4662-b554-db8382169649",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "376b65bd-2de2-4b24-ba59-9b8f2ca7b6f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "%config IPCompleter.use_jedi = False\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "from bioreactor.plotting import *\n",
    "from bioreactor.utils import *\n",
    "from bioreactor.ssgsea import *\n",
    "from bioreactor.gsea import *\n",
    "from bioreactor.annotation import *\n",
    "from bioreactor.survival import *\n",
    "from scipy import stats\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.model_selection import train_test_split,KFold\n",
    "from sklearn import metrics\n",
    "from scipy import stats\n",
    "from sklearn.preprocessing import Binarizer\n",
    "import optuna\n",
    "from optuna.integration import OptunaSearchCV\n",
    "from BorutaShap import BorutaShap\n",
    "import shap\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "%config InlineBackend.figure_format = 'png'\n",
    "plt.rcParams['pdf.fonttype'] = 'truetype'\n",
    "plt.rcParams['svg.fonttype'] = 'none'\n",
    "plt.rcParams['figure.dpi'] = 120\n",
    "sns.set_style('white')\n",
    "\n",
    "def calculate_and_plot_correlations(series1, series2,name1='Predicted', name2='True',\n",
    "                                    verbose=True, ret=False, plot=True,title=False):   \n",
    "    from sklearn import metrics\n",
    "    from scipy import stats                                    \n",
    "    series1, series2 = to_common_samples((series1.dropna(), series2.dropna()))\n",
    "    df = pd.DataFrame({name1: series1, name2: series2})\n",
    "    pearson_corr, pearson_p = stats.pearsonr(series1, series2)\n",
    "    spearman_corr, spearman_p = stats.spearmanr(series1, series2)\n",
    "    mse = metrics.mean_squared_error(series1, series2)\n",
    "    mae = metrics.mean_absolute_error(series1, series2)\n",
    "    try:\n",
    "        slope, intercept, r_value, p_value, std_err = stats.linregress(series1, series2)\n",
    "        concordance_corr_coef = (2 * r_value * np.std(series1) * np.std(series2)) / (np.var(series1) + np.var(series2) + (np.mean(series1) - np.mean(series2))**2)\n",
    "    except:\n",
    "        concordance_corr_coef = 0\n",
    "    text =  [f\"Pearson correlation: {pearson_corr:.4f}, p-value: {pearson_p:.1e}\",\n",
    "          f\"Spearman correlation: {spearman_corr:.4f}, p-value: {spearman_p:.1e}\",\n",
    "    f\"MSE: {mse:.4f}; MAE: {mae:.4f}\",\n",
    "    f\"CCC: {concordance_corr_coef:.4f}\",\n",
    "    f\"Number of samples: {df.dropna().shape[0]}\"]\n",
    "    if verbose:\n",
    "        print('\\n'.join(text))\n",
    "    if plot:\n",
    "        g = sns.JointGrid(data=df, x=name1, y=name2, space=0)\n",
    "        g.plot_joint(sns.scatterplot, alpha=0.6,color = 'teal')\n",
    "        sns.regplot(x=name1, y=name2, data=df, ax=g.ax_joint, scatter=False, color='r')\n",
    "        g.plot_marginals(sns.histplot, kde=True, color=\"teal\")\n",
    "        if title:\n",
    "            text+=[title]\n",
    "        textstr = '\\n'.join(text)\n",
    "        props = dict(boxstyle='round', facecolor='wheat', alpha=0.5)\n",
    "        g.ax_joint.text(0.05, 0.95, textstr, transform=g.ax_joint.transAxes, fontsize=10,\n",
    "                        verticalalignment='top', bbox=props, linespacing=1.5)\n",
    "        plt.show()\n",
    "        plt.close()\n",
    "    if ret:\n",
    "        metrics = {\n",
    "        'MSE': mse,\n",
    "        'MAE': mae,\n",
    "        'Spearman':spearman_corr,\n",
    "        'CCC':concordance_corr_coef,\n",
    "        'Pearson':pearson_corr}\n",
    "        return metrics\n",
    "    \n",
    "def plt_mean_auc_curve(ax, x_list, y_list, auc_metric_list, name='Model', color='crimson', type='roc',plot_random=True, legend_out=False):\n",
    "    mean_x = np.linspace(0, 1, 100)\n",
    "    ys_interp = []\n",
    "    \n",
    "    for x, y in zip(x_list, y_list):\n",
    "        sort_idx = np.argsort(x) \n",
    "        x = np.array(x)[sort_idx]\n",
    "        y = np.array(y)[sort_idx]\n",
    "\n",
    "        y_interp = np.interp(mean_x, x, y)\n",
    "        y_interp[0] = 1.0 if type=='pr' else 0.0\n",
    "        y_interp[-1] = 1.0\n",
    "        ys_interp.append(y_interp)\n",
    "\n",
    "    mean_y = np.mean(ys_interp, axis=0)\n",
    "    std_y = np.std(ys_interp, axis=0)\n",
    "    ys_upper = np.minimum(mean_y + std_y, 1)\n",
    "    ys_lower = np.maximum(mean_y - std_y, 0)\n",
    "    \n",
    "    std_auc = np.std(auc_metric_list)\n",
    "    roc_auc = np.mean(auc_metric_list)\n",
    "    \n",
    "    ax.plot(mean_x, mean_y, color=color, label='%s\\n(AUC = %0.2fÂ±%0.2f)' % (name, roc_auc, std_auc), alpha=0.8)\n",
    "    ax.fill_between(mean_x, ys_lower, ys_upper, color=color, alpha=.2)\n",
    "    \n",
    "    if type=='roc':\n",
    "        x_c,y_c=[0, 1], [0, 1]\n",
    "        ax.set_xlabel('False Positive Rate')\n",
    "        ax.set_ylabel('True Positive Rate')\n",
    "    elif type=='pr':\n",
    "        x_c,y_c=[0, 1], [0.5, 0.5]\n",
    "        ax.set_xlabel('Recall')\n",
    "        ax.set_ylabel('Precision')\n",
    "    if plot_random:\n",
    "            ax.plot(x_c,y_c,'b-', alpha=0.8,label='Random Classificator')\n",
    "    ax.set_xlim([-0.03, 1.03])\n",
    "    ax.set_ylim([-0.03, 1.03])\n",
    "    if legend_out:\n",
    "        ax.legend(loc='upper left', bbox_to_anchor=(1.05, 1)) \n",
    "    else:\n",
    "        ax.legend(loc='lower right')\n",
    "\n",
    "\n",
    "def plt_auc_curve(ax,x,y,auc_metric, color='crimson',name='Model', type='roc',plot_random=True, legend_out=False):\n",
    "    ax.plot(x, y,'-', label='%s\\nROC (area = %0.2f)' % (name, auc_metric), c=color, alpha=0.8)\n",
    "    if type=='roc':\n",
    "        x_c,y_c=[0, 1], [0, 1]\n",
    "        ax.set_ylabel('True-positive rate')\n",
    "        ax.set_xlabel('False-positive rate')\n",
    "    elif type=='pr':\n",
    "        x_c,y_c=[0, 1], [0.5, 0.5]\n",
    "        ax.set_ylabel('Precision')\n",
    "        ax.set_xlabel('Recall')\n",
    "    if plot_random:\n",
    "        ax.plot(x_c,y_c,'b-', alpha=0.8,label='Random Classifier')\n",
    "    ax.set_xlim([-0.03, 1.03])\n",
    "    ax.set_ylim([-0.03, 1.03])\n",
    "    if legend_out:\n",
    "        ax.legend(loc='upper left', bbox_to_anchor=(1.05, 1)) \n",
    "    else:\n",
    "        ax.legend() \n",
    "\n",
    "\n",
    "def plt_confusion_matrix(y_test_bin,y_pred_bin,thr,name=False):\n",
    "        y_test_bin.name,y_pred_bin.name = 'True','Prediction'\n",
    "        matrix = pd.crosstab(y_test_bin,y_pred_bin)\n",
    "        matrix.index = [f\"Lower than {thr:.1f}\", f\"Higher than {thr:.1f}\"]\n",
    "        matrix.columns = matrix.index\n",
    "        fig, axes = plt.subplots(1,2,figsize=(10,5))\n",
    "        ax=axes[0]\n",
    "        sns.heatmap(matrix, annot=True,fmt='g',\n",
    "                cmap=\"Blues\",ax=ax)\n",
    "        ax.set_xlabel('Prediction')\n",
    "        ax.set_ylabel('True')\n",
    "        ax=axes[1]\n",
    "        matrix = matrix/matrix.sum()\n",
    "        sns.heatmap(matrix, annot=True,fmt='.1%',\n",
    "                cmap=\"Blues\",ax=ax) \n",
    "        ax.set_xlabel('Prediction')\n",
    "        ax.set_ylabel('True')\n",
    "        if name:\n",
    "            fig.suptitle(name)\n",
    "        plt.show()       \n",
    "        plt.close()\n",
    "\n",
    "def calculate_metrics(model, X_test, y_test,ret=False, verbose = True, thr=2, \n",
    "                      plot_curves=False, plot_confusion_matrices=False, plot_correlations=False,\n",
    "                     name = 'Model', classifier = False):\n",
    "    from sklearn import metrics\n",
    "    from scipy import stats\n",
    "    y_pred = pd.Series(data=model.predict(X_test),index=X_test.index)\n",
    "    if not classifier:\n",
    "        metrics_corr = calculate_and_plot_correlations(y_pred,y_test, verbose=False, ret=True,plot=plot_correlations)\n",
    "        y_test_bin = hl_tls_area.reindex(y_test.index).dropna().map(lambda x:0 if 'low' in x.lower() else 1)\n",
    "        y_pred_bin = pd.qcut(y_pred,q=[0,0.5,1],labels=[0,1], duplicates='drop')\n",
    "        probas = y_pred\n",
    "    else:\n",
    "        metrics_corr= None\n",
    "        y_test_bin = y_test\n",
    "        y_pred_bin = y_pred\n",
    "        probas = model.predict_proba(X_test)[:,1]\n",
    "    y_test_bin.name,y_pred_bin.name = f\"{name}_True\",f\"{name}_Predicted\"\n",
    "    fpr, tpr, _ = metrics.roc_curve(y_test_bin, probas)\n",
    "    roc_auc = metrics.auc(fpr, tpr)\n",
    "    f1 = metrics.f1_score(y_test_bin, y_pred_bin)\n",
    "    accuracy = metrics.accuracy_score(y_test_bin, y_pred_bin)\n",
    "    recall_score = metrics.recall_score(y_test_bin, y_pred_bin)\n",
    "    precision_score = metrics.precision_score(y_test_bin, y_pred_bin)\n",
    "    av_precision = metrics.average_precision_score(y_test_bin, y_pred_bin)\n",
    "    precision, recall, _ = metrics.precision_recall_curve(y_test_bin, probas)\n",
    "    pr_auc = metrics.auc(recall, precision)\n",
    "    if verbose:\n",
    "        if not classifier:\n",
    "            print('mse',f\"{metrics_corr['MSE']:.4f}\")\n",
    "            print('mae',f\"{metrics_corr['MAE']:.4f}\")\n",
    "            print('concordance_corr_coef',f\"{metrics_corr['CCC']:.4f}\")\n",
    "            print('spearman_corr',f\"{metrics_corr['Spearman']:.4f}\")\n",
    "            print('pearson corr', f\"{metrics_corr['Pearson']:.4f}\")\n",
    "        print('recall_score',f\"{recall_score:.4f}\")\n",
    "        print('precision_score',f\"{precision_score:.4f}\")\n",
    "        print('accuracy',f\"{accuracy:.4f}\")\n",
    "        print('f1',f\"{f1:.4f}\")\n",
    "        print('roc auc',f\"{roc_auc:.4f}\")\n",
    "        print('average precision',f\"{av_precision:.4f}\")\n",
    "        print('pr auc',f\"{pr_auc:.4f}\")\n",
    "    if plot_curves:\n",
    "        fig, axes = plt.subplots(1,2,figsize=(10,5))\n",
    "        plt_auc_curve(axes[0],fpr,tpr,roc_auc,name=name,type='roc')\n",
    "        plt_auc_curve(axes[1],recall,precision,pr_auc,name=name,type='pr')\n",
    "        plt.show()\n",
    "        plt.tight_layout()\n",
    "        plt.close()\n",
    "    if plot_confusion_matrices:\n",
    "        plt_confusion_matrix(y_test_bin,y_pred_bin,thr)\n",
    "    if ret:\n",
    "        mapper={0:f\"Lower than {thr:.1f}\",1: f\"Higher than {thr:.1f}\"}\n",
    "        if not classifier:\n",
    "            mean_pred, mean_test = round(y_pred.mean(),2),round(y_test.mean(),2)\n",
    "            std_pred, std_test  = round(y_pred.std(),2),round(y_test.std(),2)\n",
    "            metrics_array = {\n",
    "            'MSE': metrics_corr['MSE'],\n",
    "            'MAE': metrics_corr['MAE'],\n",
    "            'Spearman':metrics_corr['Spearman'],\n",
    "            'Pearson':metrics_corr['Pearson'],\n",
    "            'CCC':metrics_corr['CCC'],\n",
    "                 'Preds_mean_sd':f'{mean_pred}+-{std_pred}',\n",
    "            'Test_mean_sd':f'{mean_test}+-{std_test}','y_test':y_test,'y_pred':y_pred,'thr':thr,\n",
    "            'y_test_bin':y_test_bin,\n",
    "                         'y_pred_bin':y_pred_bin}\n",
    "        else:\n",
    "            metrics_array = {'y_test_bin':y_test_bin.map(lambda x:mapper[x]),\n",
    "                         'y_pred_bin':y_pred_bin.map(lambda x:mapper[x]),'probas':probas}\n",
    "        metrics_dict = {'ROC_AUC':roc_auc,\n",
    "                'PR_AUC':pr_auc,\n",
    "            'F1':f1,\n",
    "            'Accuracy':accuracy,\n",
    "            'Precision_score':precision_score,\n",
    "            'Average_precision':av_precision,\n",
    "            'Recall_score':recall_score,'fpr':fpr,'tpr':tpr,'precision':precision,'recall':recall}\n",
    "        for key in metrics_array.keys():\n",
    "            metrics_dict[key] = metrics_array[key]        \n",
    "        return metrics_dict\n",
    "\n",
    "from bioreactor.graphs import dense_clustering\n",
    "def clustering_profile_metrics(\n",
    "    data, threshold_mm=(0.3, 0.6), step=0.025, method='leiden'\n",
    "):\n",
    "    \"\"\"\n",
    "    Iterates threshold in threshold_mm area with step. Calculates cluster separation metrics on each threshold.\n",
    "    Returns a pd.DataFrame with the metrics\n",
    "    :param data:\n",
    "    :param threshold_mm:\n",
    "    :param step:\n",
    "    :param method:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    from sklearn.metrics import (\n",
    "        silhouette_score,\n",
    "        calinski_harabasz_score,\n",
    "        davies_bouldin_score,\n",
    "    )\n",
    "\n",
    "    cluster_metrics = {}\n",
    "\n",
    "    for tr in tqdm(\n",
    "        np.round(np.arange(threshold_mm[0], threshold_mm[1], step), 3)\n",
    "    ):\n",
    "        clusters_comb = dense_clustering(data, threshold=tr, method=method)\n",
    "        cluster_metrics[tr] = {\n",
    "            'ch': calinski_harabasz_score(data.loc[clusters_comb.index], clusters_comb),\n",
    "            'db': davies_bouldin_score(data.loc[clusters_comb.index], clusters_comb),\n",
    "            'sc': silhouette_score(data.loc[clusters_comb.index], clusters_comb),\n",
    "            'N': len(clusters_comb.unique()),\n",
    "            'perc': clusters_comb,\n",
    "        }\n",
    "    return pd.DataFrame(cluster_metrics).T\n",
    "def clustering_profile_metrics_plot(cluster_metrics, num_clusters_ylim_max):\n",
    "    \"\"\"\n",
    "    Plots a dataframe from clustering_profile_metrics\n",
    "    :param cluster_metrics:\n",
    "    :param num_clusters_ylim_max:\n",
    "    :return: axis array\n",
    "    \"\"\"\n",
    "    # necessary for correct x axis sharing\n",
    "    cluster_metrics.index = [str(x) for x in cluster_metrics.index]\n",
    "    cluster_metrics['product'] = cluster_metrics.sc.rank(pct=True)*cluster_metrics.ch.rank(pct=True)*(1/cluster_metrics.db).rank(pct=True)\n",
    "\n",
    "    af = axis_matras([3, 3, 3,3, 1, 2], sharex=True)\n",
    "\n",
    "    ax = cluster_metrics.db.plot(ax=next(af), label='Davies Bouldin', color='#E63D06')\n",
    "    ax.legend()\n",
    "\n",
    "    ax = cluster_metrics.ch.plot(\n",
    "        ax=next(af), label='Calinski Harabasz', color='#E63D06'\n",
    "    )\n",
    "    ax.legend()\n",
    "\n",
    "    ax = cluster_metrics.sc.plot(ax=next(af), label='Silhouette score', color='#E63D06')\n",
    "    ax.legend()\n",
    "    \n",
    "    ax = cluster_metrics['product'].plot(ax=next(af), label='Product of all', color='#E63D06')\n",
    "    ax.legend()\n",
    "\n",
    "    ax = cluster_metrics.N.plot(\n",
    "        kind='line', ax=next(af), label='# clusters', color='#000000'\n",
    "    )\n",
    "    ax.set_ylim(0, num_clusters_ylim_max)\n",
    "    ax.legend()\n",
    "\n",
    "    # display percentage for 10 clusters max\n",
    "    clusters_perc = pd.DataFrame(\n",
    "        [x.value_counts() for x in cluster_metrics.perc], index=cluster_metrics.index\n",
    "    ).iloc[:, :10]\n",
    "\n",
    "    ax = bot_bar_plot(\n",
    "        clusters_perc,\n",
    "        ax=next(af),\n",
    "        legend=False,\n",
    "        offset=0.5,\n",
    "        palette=lin_colors(pd.Series(clusters_perc.columns), cmap=matplotlib.cm.tab20c),\n",
    "    )\n",
    "\n",
    "    ax.set_xticks(ax.get_xticks() - 0.5)\n",
    "    ax.set_xticklabels(ax.get_xticklabels(), rotation=90)\n",
    "\n",
    "    ax.set_ylabel('Cluster %')\n",
    "\n",
    "    return af, cluster_metrics\n",
    "def clustering_select_best_tr(\n",
    "    data,\n",
    "    n_clusters=4,\n",
    "    threshold_mm=(0.3, 0.6),\n",
    "    step=0.025,\n",
    "    method='leiden',\n",
    "    num_clusters_ylim_max=7,\n",
    "    plot=True,\n",
    "):\n",
    "    \"\"\"\n",
    "    Selects the best threshold for n_clusters separation using dense_clustering with selected method\n",
    "        from threshold_mm with a paticular step\n",
    "    :param data: dataframe with processes (rows - samples, columns - signatures)\n",
    "    :param n_clusters: desired number of clusters\n",
    "    :param threshold_mm: range of thresholds\n",
    "    :param step: step to go through range of thresholds\n",
    "    :param method: clusterization method - 'leiden'|'louvain'\n",
    "    :param num_clusters_ylim_max: set y_lim for plot with number of clusters\n",
    "    :param plot: whether to plot all matrics\n",
    "    :return: the threshold to get n_clusters\n",
    "    \"\"\"\n",
    "    cl_scs = clustering_profile_metrics(\n",
    "        data, threshold_mm=threshold_mm, step=step, method=method\n",
    "    )\n",
    "\n",
    "    if plot:\n",
    "        _, x = clustering_profile_metrics_plot(cl_scs, num_clusters_ylim_max)\n",
    "        plt.show()\n",
    "        x['min_cluster'] = x['perc']\n",
    "        for i in x.index:\n",
    "            a = x.loc[i,'min_cluster']\n",
    "            a = a.value_counts().min()\n",
    "            x.at[i,'min_cluster'] = a\n",
    "\n",
    "    cl_scs_filtered = cl_scs[cl_scs.N == n_clusters]\n",
    "\n",
    "    if not len(cl_scs_filtered):\n",
    "        raise Exception('No partition with n_clusters = {}'.format(n_clusters))\n",
    "\n",
    "    cl_scs_filtered.sc += 1 - cl_scs_filtered.sc.min()\n",
    "    \n",
    "        \n",
    "    return (\n",
    "        (cl_scs_filtered.ch / cl_scs_filtered.db / cl_scs_filtered.sc)\n",
    "        .sort_values()\n",
    "        .index[-1]\n",
    "    ), x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4529cd6-ba2f-4a84-8a75-44da88ffaccd",
   "metadata": {},
   "source": [
    "## Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8ea82449-ab4f-4386-86af-f126e4af9cb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Mean non-scaled absolute values')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjwAAAFeCAYAAACB28VWAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8qNh9FAAAACXBIWXMAABJ0AAASdAHeZh94AABs6klEQVR4nO3deVxUVf/A8c+wg4AsIm4IbuDCooi7uJbgllqmZWqlpZallk+l9VRPPWVPVpaV2WKpWS75E9S0xC1NySXTxAURF1zZBNmXgZn7+wNncpxBQcGB8ft+vXwp555z7vdersyXe849V6UoioIQQgghhAWzMncAQgghhBDVTRIeIYQQQlg8SXiEEEIIYfEk4RFCCCGExZOERwghhBAWTxIeIYQQQlg8SXiEEEIIYfEk4RFCCCGExZOERwghhBAWTxIeIYQQQlg8SXiEEEIIYfEk4RFC1DoXL14kICCAWbNmVet+AgICGDduXLX1P27cOAICAqqt/8qIiooiICCAqKgoc4dSY+zbt4+AgAA+++wzc4ciqoAkPKLGCAgIICAggNatW3P+/Ply6+k+JOSHsxA1z91KRoWoLEl4RI1iY2ODoij83//9n8ntSUlJ7N+/Hxsbm7scmRBCiNpMEh5Ro3h6ehIYGEhUVBSlpaVG21evXg1A375973ZoQgghajFJeESNM2rUKNLT09mxY4dBeUlJCdHR0XTo0IEWLVqU2z4rK4uPPvqIgQMHEhwcTMeOHXn88cfZvXu3Ud3c3FwWLVrE+PHj6dWrF4GBgXTt2pUpU6Zw6NAhk/3r5nVkZmby+uuv07NnTwIDAxk8eDBr1qyp1LH269ePfv36UVBQwPvvv0+fPn0IDAzk/vvv5+uvv0ZRFJPtfvnlFx577DE6duxIcHAwQ4cO5auvvkKtVlfZPspz4sQJXnzxRfr166c/XyNGjODdd9+lpKTEoK5Go2HFihU88sgj+ljvv/9+XnvtNZKSkvT1UlNT+fzzz3nkkUfo0aMHgYGB9OzZk5kzZ3Lq1KlKxVdYWMhXX33FsGHDaN++PR06dGD06NFs2LDBZH21Ws2CBQu47777CAwMpF+/fnz88ccmz+WtREVF8fzzz9O/f3+Cg4MJDQ3lkUceYd26dTdtp1ar+fjjj/Xn9L777uPzzz83GcOBAweYMmWK/nrt0aMHo0aN4vPPPzeqm5aWxltvvWXwvXruuec4evRohY/pZvOYZs2aRUBAABcvXgTgs88+o3///gBER0frh55NDT/v2rWLp59+mi5duuiP+f333ycnJ6dCcb3xxhsEBASwdetWk9sPHz5MQEAA06ZN05edPXuWDz/8kAcffJCuXbsSGBhI3759ef3110lJSanQfuGf/1OmfPbZZwQEBLBv3z6jbadPn2bWrFn07t2bwMBAunfvzsyZMzlz5oxR3StXrvD+++8TERFB+/btCQsLIyIiglmzZnHhwoUKxyr+IeMCosYZPHgw//vf/1i9ejX33Xefvnz79u1kZGTwr3/9i3Pnzplse+nSJcaNG8elS5cICwsjPDycwsJCfvvtN5566inefvttRo0apa9/+vRpPvnkE8LCwujTpw+urq4kJyezfft2du3axcKFC+nVq5fRfnJycnj00Uexs7MjIiICtVrNpk2bePXVV7GysmLEiBEVPt6SkhImTpxIWloavXr1wtramq1bt/LRRx+hVqt57rnnDOrPmzePr776Cnd3d4YMGYKTkxO7du1i3rx57N69m2+//RY7O7s72kd5Tpw4wahRo1CpVPTr148mTZqQl5fH+fPnWbFiBTNmzMDW1hYo+xCfMmUKsbGxNGzYkCFDhuDs7MylS5fYunUrHTt2xM/PDyj7EP/mm2/o0qULAwYMwMnJiXPnzhETE8P27dtZsWIFrVu3vmV8OTk5PP744xw/fpx27drx0EMPodVq2b17NzNnziQxMZEXXnhBX19RFGbMmMG2bdto2rQpY8eOpaSkhDVr1nDy5MkKnZPr/ec//6Fly5Z06tQJLy8vsrKy2LlzJy+//DJnz55lxowZJttNnz6dI0eOEBkZiY2NDdu2beOzzz7j6NGjLFy4EJVKBcDvv//O5MmTcXZ2pl+/fnh7e5OVlcWZM2dYvny5wffxwoULjBkzhrS0NLp27crgwYNJTk5m06ZN7Nixg88++6zK75R27tyZ8ePH8/3339O6dWuD/79t2rTR//vzzz/ns88+w83NjT59+uDh4cHJkyf57rvv+P3331m1ahXOzs433deIESNYtWoV69atM9iPTnR0tL6ezpYtW1i5ciVdunQhNDQUW1tbEhMTWb16Nb/99htr1qzB29v7Tk+DSb///jvPP/88paWl9O3bl6ZNm5KamsrmzZvZsWMH33//Pe3atQPKkvZHH32U8+fP06NHD/r164eiKFy+fJlt27YRERGBj49PtcRp0RQhagh/f38lPDxcURRFefXVV5U2bdooycnJ+u0TJkxQQkNDlYKCAmXevHmKv7+/smbNGoM+xo4dqwQEBCgbNmwwKM/OzlYeeOABJSgoSElPT9eX5+TkKBkZGUaxJCcnKz169FAiIyNNxunv76+8+uqrSmlpqb48MTFRadOmjTJw4MAKH3Pfvn0Vf39/5amnnlIKCwv15VeuXFE6duyodOzYUVGr1frygwcPKv7+/krv3r2VtLQ0fXlJSYkyefJkxd/fX1m4cOEd7eNm3nvvPcXf31/ZsmWL0basrCxFo9Hov/7oo48Uf39/ZfLkyUpxcbFB3eLiYoPzfuXKFSU3N9eoz/j4eKV9+/bKxIkTDcovXLig+Pv7K6+88opB+SuvvKL4+/srX3/9tUF5UVGRMmHCBCUgIEA5fvy4vnz9+vWKv7+/MmrUKKWoqEhffvXqVaV///6Kv7+/Mnbs2JudEgPnzp0zKisuLlbGjx+vtG3bVklJSTHYNnbsWMXf318ZMGCAkpWVZRDvqFGjFH9/fyU6Olpf/txzzyn+/v5KfHy80X5uvI4nTJig+Pv7K1988YVB+V9//aW0adNG6dy5s5KXl6cvX7Nmjcn/Uzc7B7rzfeHCBX1Zed8bnT179ij+/v7K6NGjlezsbINtuhjeffddk21vNGDAAKVdu3bK1atXDcqLi4uVTp06Kd26dVNKSkr05SkpKUbXoqIoyq5du5TWrVsrb7zxhkH53r17FX9/f+XTTz81KO/bt6/St29fkzF9+umnir+/v7J37159WVZWlhIWFqZ07txZSUxMNKifkJCgtG/fXhk+fLi+bNu2beWeh+LiYpP/V8StyZCWqJFGjRqFRqPRT16+dOkSf/zxB0OHDsXR0dFkmxMnTrB//34GDBjA4MGDDba5urry/PPPU1xcTExMjL7cxcUFDw8Po74aNGhAZGQkZ86c4fLly0bbHR0dmT17NtbW1vqyli1bEhoayunTp8nPz6/U8f773//GwcFB/7Wnpyf9+/cnNzeXs2fP6st1Q2bPPPMMXl5e+nIbGxteeeUVrKys9POcbncfFXF9Pzp169bFyqrsR4pGo2H58uU4ODjw1ltvGd1xsrOzMzjvnp6eJn+jb926NV26dGHfvn1Gw2U3unr1KuvXrycwMJCnn37aYJu9vT0vvfQSiqLw888/68t1wywvvPAC9vb2+nI3NzeeffbZm+7PlKZNmxqV2dnZ8dhjj1FaWsqePXtMtnvmmWeoW7euQbwvvvgigMlh0utj1bn+fKakpLB7924aNWrEU089ZVAvNDSUwYMHk5WVxZYtWyp2YFVo2bJlAPz3v//F1dXVYNuDDz5ImzZtDL5HNzNixAhKSkrYuHGjQfn27dvJzs5m6NChBg84eHt7G12LAD179qRly5Ymh72rwtq1a8nJyWHatGm0bNnSYJu/vz8PP/wwx48fNxq+NfX/zM7O7pZ3v4RpMqQlaqSQkBD8/f2Jiori2WefZfXq1Wi1WoPhqBvp5tzk5eWZXDcjMzMTwGi8/K+//uL777/n77//JiMjw+iDNTU1lUaNGhmU+fr6mvyh06BBA6BsaKVOnToVONKypMvX1/emfekcP34cgK5duxrVb9asGQ0aNODixYvk5ubi4uJyW/swde5GjBhBkyZNGDRoEN9//z1Tp04lIiKC7t27ExoaavRBf+bMGXJzcwkJCanwEMGOHTtYuXIlR48e5erVq0aT1q9evUr9+vXLbX/kyBE0Gg0qlcrkMej6u/77f/z4caysrOjYsaNR/c6dO1co7utdvnyZb775hj179pCcnExRUZHB9tTUVJPtTO2rY8eOWFtbEx8fry8bOnQomzdvZtSoUQwcOJCuXbsSGhqq/z5ef1y6PnRDjNfr2rUr69ev5/jx4wwfPryyh3lH/v77b2xtbdm0aRObNm0y2l5SUkJmZiZXr17F3d39pn0NHz6c+fPnEx0dzWOPPaYvX7t2LYDR0LKiKKxfv57o6GhOnDhBTk4OGo1Gv93UuaoKf//9N1D2S5mpa1M3n+306dO0bNmSzp074+3tzddff82xY8fo3bs3oaGhtGnTxuCXLFE5kvCIGmvUqFG88847/P7770RFRdGuXTvatm1bbv2srCwAYmNjiY2NLbdeQUGB/t9btmxh2rRp2Nvb0717d5o2bYqjoyNWVlbs37+f/fv3m5w4euNvpjq63yav/yF6K5XpKzc3F8Dg7s71vLy8uHz5Mjk5OQYJT2X2YWrya+fOnWnSpAnBwcH8+OOPfPnll8TExOgn4zZr1oznnnuOIUOGAP8kUBVNdpYuXcqcOXOoW7cu3bt3p2HDhjg6OqJSqdi6dSsnTpy45SRi3ff/yJEjHDlypNx61999y83NpW7duiY/6Mo7x+W5cOECI0eOJCcnh7CwMHr27ImzszPW1tZcunSJ6Ojoco+hXr16RmU2Nja4u7uTkZGhLxswYABfffUV3333HVFRUaxatQqAdu3aMXPmTHr06KE/rpsdg65cV+9uysrKorS01OR1dr2CgoJbJjwNGjSgW7duxMbGcvr0aVq0aEFGRga7du2iTZs2RvO+3nvvPZYuXYqXlxc9e/bE29tbfxclOjqaS5cu3dnBlUN3bf700083raf72eTs7MxPP/3Ep59+yvbt2/V3ntzd3RkzZgzPPPNMtSVnlkwSHlFjDRs2jA8//JA333yT1NRUpk6detP6ug/41157jfHjx1doH/Pnz8fW1pY1a9YYPfn1xhtvsH///tsLvprojvHKlSsmh0/S09MN6t2OhISEm27v0KGD/omwo0ePsmvXLn744QdmzpyJh4cH3bt31ydY5d3RuJ7uw8/Ly4uoqCijuzi6345vRXfMTzzxBLNnz65wm+zsbEpKSow+QHTnsqIWL15MVlYW7733Hg8++KDBtg0bNugn0Zpy5coVo7uIpaWlXL161ehOYp8+fejTpw8FBQUcPnyYHTt2sGLFCiZPnszatWtp2bKlwXViiu7YKjI0olKpTC4RAVT4iarrOTs7oyhKlf3fGj58OLGxsURHR/Ovf/2Ln3/+mdLSUqM7VxkZGSxbtgx/f39WrFhhdOzlPcVnikqlKneI1dQ50X0/1q1bV6HJ91CWzM2ZMwdFUTh16hR79+7lxx9/ZMGCBWi12nInwIvyyRweUWO5uroSERFBSkoKTk5ORvNybhQSEgKUPfFTUefOnaNly5ZGyY5Wq+Wvv/6qfNDVTPeki6lHXs+dO0dKSgpNmjQp945OVbKzsyM0NJTp06fz2muvAbBt2zYAmjdvjqurKwkJCbdMeq5evUpOTg4dOnQwSnby8/M5duxYheIJDg7GysqqUt//tm3blvu9ruwHsu7JwQEDBlS6L1Pb//rrLzQajcHTTddzcnKiW7duzJ49m8mTJ1NSUsLvv/8OoL8T+tdff5lMVnTXj+6poJupW7euyUe2NRoNJ06cMCrXDbmUd5ezffv2ZGdnk5iYeMt9V8SAAQNwdnZm/fr1aLVaoqOjsbGxYejQoQb1Lly4gFarpUePHkbJTkpKiv7R+oqoW7euyeFvwOQj/7qfTbfzM0WlUtGqVSvGjRvH4sWLgX/+n4nKkYRH1GgzZsxgwYIFLFq06Ja/jQYFBREWFsaWLVvKXak5ISHBYIigcePGJCUlGXwoK4rCZ599Vun1X+6Ghx56CICFCxfq5yRB2YfL+++/j1arZeTIkdW2/4MHDxrNSwH051Q3PGBtbc2YMWMoKirizTffNBrKUavV+vg9PT1xdHTk2LFjBsNNJSUlvPvuu1y9erVCsXl6ejJ06FCOHj3KggULTH7gnj9/3mANE92dmE8++YTi4mJ9eVZWFgsXLqzQfnUaN24MGCcvu3btKvd61Fm4cCHZ2dn6r4uLi5k3bx7wz/cc4M8//zSZwNx4/hs0aECPHj24dOkSS5cuNah7+PBhNmzYQN26dU0+zn2joKAgLl++bDShd+HChSaHgFxdXVGpVCQnJ5vs74knngDg9ddfN5kMFxQUVPiuHpQd88CBA0lNTWXJkiWcOHGCXr164enpaVBP9/3RJZI6+fn5/Pvf/y73LpYpQUFBlJaWGq0tFBUVxcGDB43qP/jgg7i6uvL5558TFxdntF2r1Rr8EpOYmGjy7pyuzNRkZnFrMqQlarRGjRoZ3eq/mY8++ojHH3+c1157jWXLlhESEoKLiwspKSmcPHmSkydPsmrVKv0PwyeeeII333yTESNGMGDAAGxsbDh48CCnT5+mb9++/Pbbb9V1aLclNDSUp556ikWLFjFkyBAiIiJwdHRk165dnDx5ko4dOzJx4sRq2/+iRYvYu3cvYWFhNGnSBCcnJ06dOsXvv/9O3bp1GT16tL7u1KlTOXz4ML/99hsRERH06dOHOnXqkJycTGxsLC+//DIPPvggVlZWjBs3jq+//pqhQ4fSv39/SkpK2LdvH9nZ2fqntCrijTfe4Ny5c3z66aesX7+e0NBQ6tWrR1paGqdPn+bIkSPMmzdPv4bJkCFD+OWXX9i+fTtDhgyhf//+lJaWsmnTJoKCgm76TrcbjRkzhqioKKZPn05ERAT169cnMTGRXbt2MXDgQH755Zdy2zZv3pzBgwcbrMNz/vx5+vTpw7Bhw/T13nnnHVJTUwkNDaVx48bY2tpy7Ngx9u7dS+PGjQ3ugr711ls8+uijzJ07l9jYWAIDA/Xr8FhZWTFnzpwKDWlNnDiR3bt38+yzzzJo0CDq1q3LoUOHuHjxIp07dzZK8OrUqUNISAgHDhxg5syZNGvWDCsrK/r160fr1q3p1q0bM2fOZN68eURERNCrVy+aNGlCQUEBly9f5s8//yQ0NJRvv/22wud++PDhrF69Wp8kmloHy8vLi8GDB7Nx40aGDx9Ojx49yM3N5Y8//sDOzo42bdoYTBC/mXHjxhEVFcV//vMf9uzZQ8OGDYmPj+fvv/82+XPD3d2dTz/9lKlTpzJq1Ci6detGy5YtUalUpKSkcOjQIbKysvRzz2JjY/nggw9o3749fn5+eHp6kpKSwrZt27CysqrW/+OWTBIeYVEaNGjAmjVr+OGHH9i8eTM///wzGo2GevXq0bJlS8aOHYu/v7++/iOPPIKdnR1Lly5l7dq12NvbExYWxnvvvcfmzZtrXMID8NJLL9G2bVt++OEH1q5dS2lpKU2bNmXGjBlMmDDB5GO3VWXMmDHUrVuXw4cP639T9vb2ZsyYMTz55JP636KhbMhr0aJFrFy5krVr17J27VoURaF+/frcf//9Bk9GTZ8+HQ8PD1avXs2qVatwcXGhe/fuzJgxo1JvqnZ2dmbZsmX89NNPbNiwgc2bN1NcXEy9evXw9fVl9uzZdO/eXV9fpVIxf/58vv76a6Kjo/nhhx+oX78+Dz30EFOnTiUoKKjC+27dujXff/89n3zyCTt37qS0tJTWrVvz+eef4+LictOEZ/78+SxYsICff/6ZtLQ0vL29ef7555k0aZJ+0UGAyZMns3XrVo4ePcqePXtQqVQ0atSIKVOm8Pjjjxs82u7j48OaNWv44osv+P3339m/fz916tQhPDycKVOmEBwcXKHj6tatGwsWLGDBggVs3LgRJycnunfvzscff1zu92bu3Lm899577N69m40bN6IoCg0aNNDPX5k0aRKhoaEsW7aMv/76i+3bt+Ps7Iy3tzejRo3ST36vqLCwMHx9fTl37px+MUNT3n33XXx8fPjll1/48ccf8fDwoF+/fkybNs1gReZbadmyJYsXL+bjjz/mt99+w9ramrCwMFauXMmWLVtM/tzo1q0b69ev57vvvmP37t0cOHAAW1tb6tevT9euXYmIiNDXDQ8PJzk5mT///JNt27aRl5dH/fr16dGjB0888QShoaGVOj+ijEpRKrmuvBBCCCFELSNzeIQQQghh8SThEUIIIYTFk4RHCCGEEBZPEh4hhBBCWDxJeIQQQghh8SThEUIIIYTFk4RHCCGEEBZPEh4hhBBCWDxZadmMcnJy2L9/Pw0bNqzW1XGFEEIIS6NWq0lOTqZz584VemGyJDxmtH//fqZOnWruMIQQQohaa8GCBRV6Ea4kPGbUsGFDoOyb5evra+ZohBBCiNrj3LlzTJ06Vf9ZeiuS8JiRbhjL19eXVq1amTkaIYQQovap6JQQmbQshBBCCIsnCY8QQgghLJ4kPEIIIYSweJLwCCGEEMLiScIjhBBCCIsnCY8QQgghLJ4kPEIIIYSweJLwCCGEEMLiScIjhBBCCIsnKy0LcZ19ZzJQa7R31IedtRVdmntWUURCCCGqgiQ8QlxHrdFyIbPwjvrw8XCsomiEEEJUFRnSEkIIIYTFk4RHCCGEEBavRic8arWaDz74gJ49exIcHMzDDz9MbGxshdqmpqYyffp0wsLCCA0N5ZlnnuHChQsGdYqKinj11VcZMmQIHTt2pEOHDjzwwAMsXbqUkpISoz5zcnJ4/fXX6dq1K+3bt2fcuHEcO3asSo5VCCGEENWnRs/hmTVrFjExMYwfPx4/Pz+io6OZNGkSS5cuJSwsrNx2+fn5jB8/ntzcXCZPnoytrS1Llixh7NixrF27Fnd3d6As4Tl16hS9evWicePGWFlZcejQId577z3i4uL46KOP9H1qtVomTZpEQkICEydOxN3dneXLlzNu3DiioqLw8/Or7tMhhBBCiNtUYxOeuLg4Nm7cyMsvv8zEiRMBGD58OEOGDOHDDz9k5cqV5bZdvnw5SUlJrF69muDgYADCw8MZOnQoixcv5sUXXwTAzc2Nn376yaDto48+iouLCz/88AOzZs3Cy8sLgE2bNnHo0CHmz59PZGQkAAMHDiQiIoLPPvvMIDkSQgghRM1SY4e0Nm3ahLW1NaNHj9aX2dvbM3LkSA4dOkRycnK5bWNiYggKCtInOwAtWrSgW7du/Prrr7fcd+PGjYGyIazr+6xXrx4DBgzQl3l4eDBw4EC2bduGWq2u1PEJIYQQ4u6psQlPfHw8fn5+ODs7G5Trkpj4+HiT7bRaLQkJCQQGBhptCwoK4vz58+Tl5RmUq9VqMjMzSU5OZsuWLXz33Xc0btwYX19fg3jatm2LlZWVUZ+FhYWcPXv2to5TCCGEENWvxiY86enp+uGk6+nK0tLSTLbLyspCrVZXqu2WLVvo1q0bffr04bnnnsPb25uFCxdiY/PPiF958dSvX/+m8QghhBDC/GrsHJ6ioiLs7OyMyu3t7fXbTSkuLga4aVtdHZ0uXbqwePFicnJy2LNnDwkJCRQWGi4+V148urIb+xRCCCFEzVFjEx4HBweT82J0iYWDg4PJdrqk5mZtdXV06tWrR7169QCIjIzkyy+/5Mknn2Tz5s36uzrlxaMru7FPIYQQQtQcNXZIy8vLi/T0dKNyXZluKOlGbm5u2NnZ3VZbnYiICAoKCti2bdst49ENZd2qTyGEEEKYT41NeFq3bk1SUpLRBOPDhw8D0KZNG5PtrKys8Pf35+jRo0bb4uLi8PHxMZoIfSPdnaDc3FyDeI4fP45Wa/hiybi4OBwdHWnWrNmtD0oIIYQQZlFjE57IyEg0Gg2rVq3Sl6nVaqKioggJCaFhw4YAXL58mdOnTxu0jYiI4MiRIxw5ckRfdubMGfbu3atfQwcgMzMTRVGM9r169WoAgye9IiMjuXLlCps3bzZov2nTJvr27Wtyfo8QQgghaoYaO4cnJCSEyMhI5s2bR0ZGBr6+vkRHR3Pp0iXeffddfb1XXnmF/fv3k5CQoC8bM2YMq1evZvLkyUyYMAEbGxuWLFmCp6cnEyZM0Ndbv349K1eu5L777sPHx4f8/Hx2795NbGwsffv2pVu3bvq6ERERtG/fntmzZ3Pq1Cnc3d1ZsWIFGo2G559//u6cFCGEEELclhqb8ADMnTuXTz75hPXr15OdnU1AQABffvklnTp1umk7Z2dnli1bxpw5c1i4cCFarZYuXbowe/ZsPDw89PU6duzIoUOH2LhxI1euXMHGxoZmzZoxe/Zsxo4da9CntbU1X3/9NXPnzmXZsmUUFxcTFBTEe++9R/Pmzavl+IUQQghRNVSKqTEdcVckJiYyZMgQNmzYQKtWrcwdjgB2JaZzIbPw1hVvwsfDkfBWxms2CSGEqDqV/QytsXN4hBBCCCGqiiQ8QgghhLB4kvAIIYQQwuJJwiOEEEIIiycJjxBCCCEsniQ8QgghhLB4kvAIIYQQwuJJwiOEEEIIiycJjxBCCCEsniQ8QgghhLB4kvAIIYQQwuJJwiOEEEIIiycJjxBCCCEsniQ8QgghhLB4kvAIIYQQwuJJwiOEEEIIiycJjxBCCCEsniQ8QgghhLB4kvAIIYQQwuJJwiOEEEIIiycJjxBCCCEsniQ8QgghhLB4kvAIIYQQwuJJwiOEEEIIiycJjxBCCCEsniQ8QgghhLB4kvAIIYQQwuJJwiOEEEIIi2dj7gBuRq1WM3/+fNatW0dOTg4BAQHMmDGDHj163LJtamoqc+bMITY2Fq1WS5cuXXj11Vfx8fHR10lOTmbNmjXs2LGDc+fOYWVlhb+/P8888wzdu3c36C8qKorZs2eb3Nfu3bvx8vK6s4MVQgghRLWp0QnPrFmziImJYfz48fj5+REdHc2kSZNYunQpYWFh5bbLz89n/Pjx5ObmMnnyZGxtbVmyZAljx45l7dq1uLu7A7Bt2za++eYb7rvvPkaMGEFpaSnr1q3jySefZM6cOTz00ENGfU+bNo0mTZoYlLm6ulbtgQshhBCiStXYhCcuLo6NGzfy8ssvM3HiRACGDx/OkCFD+PDDD1m5cmW5bZcvX05SUhKrV68mODgYgPDwcIYOHcrixYt58cUXAejSpQu//fYbHh4e+raPPvoow4YN49NPPzWZ8PTq1YugoKCqPFQhhBBCVLMaO4dn06ZNWFtbM3r0aH2Zvb09I0eO5NChQyQnJ5fbNiYmhqCgIH2yA9CiRQu6devGr7/+qi9r1aqVQbIDYGdnR+/evUlJSSEvL89k/3l5eWg0mts9NCGEEELcZTU24YmPj8fPzw9nZ2eDcl0SEx8fb7KdVqslISGBwMBAo21BQUGcP3++3ERGJz09HUdHRxwdHY22jR8/no4dOxISEsKUKVNISkqq4BEJIYQQwlxq7JBWenq6yYnAurK0tDST7bKyslCr1bdse2MipXPu3Dm2bNlCZGQk1tbW+nIHBwcefPBBunTpgrOzM0ePHmXJkiU88sgjREdH07Bhw0ofoxBCCCHujhqb8BQVFWFnZ2dUbm9vr99uSnFxMcBN2+rq3KiwsJDp06fj4ODAzJkzDbYNGjSIQYMG6b++77776NmzJ2PHjmXhwoW8/fbbFTgqIYQQQphDjU14HBwcUKvVRuW6ZMXBwcFkO11Sc7O2ujrX02g0vPDCC5w6dYpvvvkGb2/vW8YYFhZGSEgIe/bsuWVdIYQQQphPjZ3D4+XlRXp6ulG5rqx+/fom27m5uWFnZ1fptv/+97/ZsWMH//vf/+jWrVuF42zQoAHZ2dkVri+EEEKIu6/GJjytW7cmKSnJaILx4cOHAWjTpo3JdrrFA48ePWq0LS4uDh8fH6P5O++//75+YcEhQ4ZUKs4LFy7o1/URQgghRM1UYxOeyMhINBoNq1at0pep1WqioqIICQnRTxK+fPkyp0+fNmgbERHBkSNHOHLkiL7szJkz7N27l8jISIO6ixYt4rvvvmPKlCk8/vjj5caTmZlpVLZz506OHTtGeHj4bR2jEEIIIe6OGjuHJyQkhMjISObNm0dGRga+vr5ER0dz6dIl3n33XX29V155hf3795OQkKAvGzNmDKtXr2by5MlMmDABGxsblixZgqenJxMmTNDX27JlCx988AF+fn40b96cdevWGcTQo0cP6tWrB8AjjzxCmzZtCAwMxMXFhePHj7NmzRoaNmzIlClTqvlsCCGEEOJO1NiEB2Du3Ll88sknrF+/nuzsbAICAvjyyy/p1KnTTds5OzuzbNky5syZw8KFC/Xv0po9e7bBQoMnTpwAICkpiZdfftmon++//16f8AwcOJCdO3cSGxtLUVERXl5ePPzwwzz33HP6OkIIIYSomVSKoijmDuJelZiYyJAhQ9iwYQOtWrUydzgC2JWYzoXMwjvqw8fDkfBW8jJZIYSoTpX9DK2xc3iEEEIIIaqKJDxCCCGEsHiS8AghhBDC4knCI4QQQgiLJwmPEEIIISyeJDxCCCGEsHiS8AghhBDC4knCI0QVc3OyNXcIQgghblCjV1oWojaysVKx70wGao32tvuws7aiS3PPKoxKCCHubZLwCFEN1BrtHa3Y7OPhWIXRCCGEkCEtIYQQQlg8SXiEEEIIYfEk4RFCCCGExZOERwghhBAWTxIeIYQQQlg8SXiEEEIIYfEk4RFCCCGExZOERwghhBAWTxIeIYQQQlg8SXiEEEIIYfEk4RFCCCGExZOERwghhBAWTxIeIYQQQlg8SXiEEEIIYfEk4RFCCCGExZOERwghhBAWTxIeIYQQQlg8SXiEEEIIYfFqdMKjVqv54IMP6NmzJ8HBwTz88MPExsZWqG1qairTp08nLCyM0NBQnnnmGS5cuGBQJzk5mc8//5yRI0fSqVMnunTpwrhx4/jjjz9M9pmTk8Prr79O165dad++PePGjePYsWN3fJxCCCGEqF41OuGZNWsWS5YsYejQobz22mtYW1szadIkDhw4cNN2+fn5jB8/nj///JPJkyczbdo04uPjGTt2LFevXtXX27ZtG9988w2+vr7MmDGDZ599lvz8fJ588knWrFlj0KdWq2XSpEls2LCBsWPH8tJLL5GZmcm4ceNISkqqjsMXQgghRBWxMXcA5YmLi2Pjxo28/PLLTJw4EYDhw4czZMgQPvzwQ1auXFlu2+XLl5OUlMTq1asJDg4GIDw8nKFDh7J48WJefPFFALp06cJvv/2Gh4eHvu2jjz7KsGHD+PTTT3nooYf05Zs2beLQoUPMnz+fyMhIAAYOHEhERASfffYZH330UZWfAyGEEEJUjRp7h2fTpk1YW1szevRofZm9vT0jR47k0KFDJCcnl9s2JiaGoKAgfbID0KJFC7p168avv/6qL2vVqpVBsgNgZ2dH7969SUlJIS8vz6DPevXqMWDAAH2Zh4cHAwcOZNu2bajV6js6XiGEEEJUnxqb8MTHx+Pn54ezs7NBuS6JiY+PN9lOq9WSkJBAYGCg0bagoCDOnz9vkMiYkp6ejqOjI46OjgbxtG3bFisrw1MWFBREYWEhZ8+erdBxCSGEEOLuq7EJT3p6Ol5eXkblurK0tDST7bKyslCr1bfVFuDcuXNs2bKFAQMGYG1tfct46tevf8s+hRBCCGFeNTbhKSoqws7Ozqjc3t5ev92U4uJigJu21dW5UWFhIdOnT8fBwYGZM2dWKB5dWXl9CiGEEML8auykZQcHB5PzYnSJhYODg8l2uqTmZm11da6n0Wh44YUXOHXqFN988w3e3t4VikdXZqpPIYQQQtQMVXKH56mnnuLnn38u967L7fDy8iI9Pd2oXFemG0q6kZubG3Z2dpVu++9//5sdO3bwv//9j27dulU4Ht1QVnnxCCGEEML8qiThuXDhAi+99BLdu3fnlVde4Y8//kBRlDvqs3Xr1iQlJRlNMD58+DAAbdq0MdnOysoKf39/jh49arQtLi4OHx8fo4nQ77//PlFRUcyePZshQ4aUG8/x48fRarVGfTo6OtKsWbMKH5sQQggh7q4qSXhiYmL46aefePDBB4mNjWXixIn06tWL999/v9ynqW4lMjISjUbDqlWr9GVqtZqoqChCQkJo2LAhAJcvX+b06dMGbSMiIjhy5AhHjhzRl505c4a9e/fq19DRWbRoEd999x1Tpkzh8ccfv2k8V65cYfPmzfqyzMxMNm3aRN++fU3O7xFCCCFEzVBlc3iCg4MJDg7m1VdfJTY2lvXr17Nq1SqWLFlCixYtGDZsGEOHDqVBgwYV6i8kJITIyEjmzZtHRkYGvr6+REdHc+nSJd599119vVdeeYX9+/eTkJCgLxszZgyrV69m8uTJTJgwARsbG5YsWYKnpycTJkzQ19uyZQsffPABfn5+NG/enHXr1hnE0KNHD+rVqweUJVHt27dn9uzZnDp1Cnd3d1asWIFGo+H555+/k1MnhBBCiGpW5ZOWraysCA8PJzw8nJycHN544w02bdrERx99xMcff0znzp154okn6NOnzy37mjt3Lp988gnr168nOzubgIAAvvzySzp16nTTds7Ozixbtow5c+awcOFCtFotXbp0Yfbs2QYLDZ44cQKApKQkXn75ZaN+vv/+e33CY21tzddff83cuXNZtmwZxcXFBAUF8d5779G8efNKnCEhhBBC3G0q5U4n25hw4MAB1q9fT0xMDNnZ2bRq1Yrhw4djY2PDmjVrSExMZMqUKUyfPr2qd12rJCYmMmTIEDZs2ECrVq3MHY4AdiWmcyGz8I766NC0Llfy1HfUj4+HI+GtjNd9EkIIUaayn6FVdofn1KlTrF+/ng0bNpCcnIynpycjRoxg2LBhBhOMH3/8cV5//XWWL19+zyc8QgghhLg7qiThGTZsGCdPnsTOzo7+/fvz5ptvEh4ebvQaBp0uXbqwevXqqti1EEIIIcQtVUnC4+rqyttvv83AgQONHvk2pX///mzbtq0qdi2EEEIIcUtVkvC8//77eHh4lLv6cVFREZmZmTRq1AgAR0dHGjduXBW7FkIIIYS4pSpZh6d///5s2bKl3O3bt2+nf//+VbErIYQQQohKq5KE51YPepWUlJQ7n0cIIYQQorrd9pBWXl4eOTk5+q+zsrK4fPmyUb2cnBx++eUXvLzkEVshhBBCmMdtJzxLlixhwYIFAKhUKubMmcOcOXNM1lUUhRkzZtzuroQQQggh7shtJzw9evTAyckJRVH44IMPGDx4MO3atTOoo1KpcHR0pF27dgQFBd1xsEIIIYQQt+O2E54OHTrQoUMHAAoLC7n//vsJCAiossCEEEIIIapKlTyW/txzz1VFN0IIIYQQ1eK2Ep7PP/8clUrFM888g5WVFZ9//vkt26hUKqZOnXo7uxNCCCGEuCN3lPA8/fTT2NnZScIjhBBCiBrtthKeEydO3PRrIYQQQoiaRFYDFEIIIYTFq5JJy6YUFhayceNG1Go1vXv3lndnCSGEEMJsqiThefXVV4mLi2PDhg0AqNVqRo0aRWJiIgAuLi4sXbqUtm3bVsXuhBBCCCEqpUqGtPbt28f999+v/3rDhg0kJiby4YcfsmHDBurVq1ehic1CCCGEENWhShKeK1euGAxZbd26lcDAQIYMGULLli0ZNWoUcXFxVbErIYQQQohKq5KEx9HRkdzcXABKS0vZv38/PXv21G+vU6eOfrsQQgghxN1WJXN42rVrx08//USXLl3Yvn07+fn59OvXT7/9/PnzeHp6VsWuhBBCCCEqrUoSnhkzZvDUU0/x0EMPoSgKERERBAcH67dv2bKF0NDQqtiVEEIIIUSlVUnCExQUxK+//srBgwdxdXWlc+fO+m05OTmMGTPGoEwIIYQQ4m6qsnV4PDw8uO+++4zKXV1defzxx6tqN0IIIYQQlValCw/m5eVx+fJlcnJyUBTFaHunTp2qcndCCCGEEBVSJQnP1atX+e9//8vmzZvRaDRG2xVFQaVSER8fXxW7E0IIIYSolCpJeF5//XV+++03xo0bR1hYGK6urlXRrRBCCCFElaiShCc2NpbHH3+cl19+uSq601Or1cyfP59169aRk5NDQEAAM2bMoEePHrdsm5qaypw5c4iNjUWr1dKlSxdeffVVfHx8DOotX76cvXv3EhcXR3JyMiNGjOB///ufUX9RUVHMnj3b5L52796Nl5fX7R2kEEIIIapdlSQ8Dg4O1fJy0FmzZhETE8P48ePx8/MjOjqaSZMmsXTpUsLCwsptl5+fz/jx48nNzWXy5MnY2tqyZMkSxo4dy9q1a3F3d9fXXbRoEfn5+QQFBZGenn7LmKZNm0aTJk0MyuSOlhBCCFGzVUnC88ADD7B161Yee+yxqugOgLi4ODZu3MjLL7/MxIkTARg+fDhDhgzhww8/ZOXKleW2Xb58OUlJSaxevVq/HlB4eDhDhw5l8eLFvPjii/q6y5Yto1GjRqhUKjp06HDLuHr16kVQUNAdHp0QQggh7qYqebVEREQE2dnZTJw4kc2bNxMXF8exY8eM/lTGpk2bsLa2ZvTo0foye3t7Ro4cyaFDh0hOTi63bUxMDEFBQQaLH7Zo0YJu3brx66+/GtRt3LgxKpWqUrHl5eWZnJwthBBCiJqpSu7wjBkzRv/vP/74w2j77TylFR8fj5+fH87OzgbluiQmPj6ehg0bGrXTarUkJCTw0EMPGW0LCgpi9+7d5OXlGfVbUePHj6egoABbW1t69uzJrFmz8PPzu62+hBBCCHF3VEnC895771VFNwbS09NNTgTWlaWlpZlsl5WVhVqtvmXbyiY8Dg4OPPjgg3Tp0gVnZ2eOHj3KkiVLeOSRR4iOjjaZfAkhhBCiZqiShGfEiBFV0Y2BoqIi7OzsjMrt7e31200pLi4GuGlbXZ3KGDRoEIMGDdJ/fd9999GzZ0/Gjh3LwoULefvttyvdpxBCCCHujiqZw3O9tLQ0Tpw4QUFBwR314+DggFqtNirXJSsODg4m2+mSmpu11dW5U2FhYYSEhLBnz54q6U8IIYQQ1aPKEp6tW7cSGRlJ7969GTFiBIcPHwYgMzOT4cOHs2XLlkr15+XlZfIxcV1Z/fr1TbZzc3PDzs7uttrejgYNGpCdnV1l/YnaqUSjJe5iFtGHLvLWz8eZ80s8UQcvsv9sJtmFJeYOTwgh7nlVMqS1fft2nn/+edq3b8+QIUP4/PPP9ds8PDzw9vYmKiqK+++/v8J9tm7dmn379hlNMNYlUm3atDHZzsrKCn9/f44ePWq0LS4uDh8fn9uesGzKhQsXDNb1EfeWEo2W7SfS2Hsmg+JSrdH2A+euYq1SEerrRm//+njUMR5qFUIIUf2q5A7PggULCAsLY8WKFSbX4mnfvn2l36MVGRmJRqNh1apV+jK1Wk1UVBQhISH6ScKXL1/m9OnTBm0jIiI4cuQIR44c0ZedOXOGvXv3EhkZWak4dDIzM43Kdu7cybFjxwgPD7+tPkXtdi4jn0+3JbLzZLo+2bG3scLP04mWXnWwtyn776VRFP5Musr8bSc5eO6qOUMWQoh7VpXc4UlMTGTWrFnlbq9Xrx4ZGRmV6jMkJITIyEjmzZtHRkYGvr6+REdHc+nSJd599119vVdeeYX9+/eTkJCgLxszZgyrV69m8uTJTJgwARsbG5YsWYKnpycTJkww2M/27ds5ceIEACUlJSQkJPDFF18A0K9fP1q3bg3AI488Qps2bQgMDMTFxYXjx4+zZs0aGjZsyJQpUyp1bKL2i7uYxeoDF9EoCgBN3B3p1cqL1g1c6NTMnSt5as5lFHAxs4CdJ9OJT8mlRKPwfwcvcuZKPiM6NMbaqnLrPwkhhLh9VZLwODo6UlhYWO72Cxcu4ObmVul+586dyyeffML69evJzs4mICCAL7/8kk6dOt20nbOzM8uWLWPOnDksXLhQ/y6t2bNn4+HhYVB38+bNREdH678+fvw4x48fB8rm5+gSnoEDB7Jz505iY2MpKirCy8uLhx9+mOeee4569epV+thE7bX/bCbr/r6EAlhbqRjQ1pseLethdcMCllYqFU096zCuWx0S03L56cBF8otLOXj+KkUlGh7p7IONVZU/NyCEEMIElaJc+xX1DkybNo2zZ88SHR1Nbm4u3bp1Y/HixXTr1o309HSGDh1K3759q2W9ntosMTGRIUOGsGHDBlq1amXucASwKzGdC5nlJ+9HLmWzYv95AOysrRjb1ZeW9Q3nhHVoWpcreWqjfnKLSli29xwXr5aVt2noypjOTU3e6fHxcCS8lbyQVgghylPZz9Aq+fVyxowZpKSkMHLkSFatWoVKpWL37t18/PHHDB06FEVRmDp1alXsSgizuXS1kP/76wJQNldnYs9mRsnOzbg42DKhRzN83B0BiE/OYUPc5WqJVQghhKEqSXiaN2/O8uXLcXNzY/78+SiKwrfffstXX32Fv78/y5cvN3rDuBC1SdndmSRKNAoq4NHOTfHxcKp0Pw621jzZoxmN3cqSnn1nM9l7pnLz24QQQlRelczhAWjVqhVLliwhOzubc+fOoSgKPj4+RnNmhKhtFEUh+tAlcopKARgU1BB/b5fb7s/B1pqxXX354rdT5BaXsiHuMg1cHfCrV6eqQhZCCHGDO0541Go169atIzY2lvPnz5Ofn0+dOnXw9fUlPDycIUOGmHzNgxC1xcHzVzmRkgtAUOO6dG/hecd91nW0ZWxXX77ZdYZSrcJPf11gWr9WONha33HfQgghjN1RwpOQkMCzzz7L5cuXURQFFxcXnJycyMzM5Pjx42zatIkvv/yShQsX0qJFi6qKWYi75mqBmg1xyQC4ONgwrH0jVKqqeZzcx8OJyMAGbIhLJqughJ8PX+bhMJ8q6VsIIYSh257Dk5+fzzPPPENGRgYvvPACO3fu5M8//zT4e8aMGaSlpTFlypQ7freWEOawMS5Zv6jgQ6FNcLKrslFgALo296TVtYnPhy5kceSSvKZECCGqw20nPFFRUSQnJ/PVV18xadIkvL29DbZ7e3szefJkFi5cyMWLFw3WuhGiNjiZmsvx5BwAOvi43dG8nfJYqVQ81LEJTnZlQ1k/H75MoVpT5fsRQoh73W0nPDt27KBHjx506dLlpvW6detG9+7d2b59++3uSoi7rlSj5efDZY+M29tYERnYoNr25epgy+Cgslel5BWXEnMspdr2JYQQ96rbTnhOnjxJ586dK1S3a9eunDx58nZ3JcRdt+dMBhn5agD6t/HGxcG2WvfX3seNFl5lT2ntT8rkZGpute5PCCHuNbed8GRnZ+PlVbGVYOvVq0d2tsxNELVDUYmGHQnpAHg529Ot+Z0/lXUrKpWK4e0bY3Nt1eXv/ziHRnvHi6ALIYS45rYTHrVajY1NxSZwWltbU1JScru7EuKu2pWYTmFJ2TyaAe2879pLPj2d7entX/ZLxLnMAv2qzkIIIe7cHT1ycunSJY4dO3bLehcvXryT3Qhx12QXlhB7qmzl4ybujrRt6HpX9x/eyosD566SXVjCBzEnGRTUsNqH04QQ4l5wRwnP/PnzmT9//i3rKYpSZWuXCFGdfj58GbWm7DH0AW0b3PXr1s7Gioh2DfjpwAWu5BWz4LfTzBrY+q7GIIQQlui2Ex5587mwNFfyivntRNncneZedSr1YtCqFNKkLgfPZXIqPZ/FsWd5socf3q4OZolFCCEsxW0nPCNGjKjKOIQwu293n9Xf3ekXUN9scahUKh7p3JR3NsZTXKrl8+2n+O/wQLPFI4QQlqBK3pYuRG2XXVDCsj3nAGjq4UQzM7/IM6CBC72uTWBesf88FzJlpXIhhLgTkvAIASz5I4m84rK3ofcNqF8j5py9NCAAgFKtwidbE80cjRBC1G6S8Ih7XlGJhu/3JAHg6+mEv7d55u7cKKhJXSLbla3wHH3oIqfSZDFCIYS4XZLwiHve+sOX9asqDw5qWCPu7ujMHOCPSgVaBeZtkdXKhRDidknCI+5piqLw3e6zAHi72tOpmbuZIzLUytuFEe0bA/DLkRSOytvUhRDitkjCI+5pe89kciKlbKhoXFdfbKxq3n+JGff561858eHmBDNHI4QQtVPN++kuxF20OLbs7o69jRWPdm5q5mhMa+rpxOhOPgDsSEjn7wtZ5g1ICCFqIUl4xD3rfEYBW+JTARjevjGezvZmjqh8U/u2xNa67C7P59tPmTkaIYSofSThEfespXuSUK69kPzJnn5mjeVWGrk58lBoEwC2xqcSn5xj5oiEEKJ2kYRH3JPyikv56c+yt5F3b+FJ6wZ39yWht+OZPi3Qvbh9wW9yl0cIISpDEh5xT/q/AxfIvbbQ4JM9mpk5morx9azDsGtPbG08kszp9DwzRySEELWHJDzinqPVKiy97jUS/Vqb771ZlfVsnxYAKAp88dtpM0cjhBC1hyQ84p7zx+kMzl7JB+Dx7n5YW9WchQZvpZW3CwMDy1ZfXvv3JXnHlhBCVFCNTnjUajUffPABPXv2JDg4mIcffpjY2NgKtU1NTWX69OmEhYURGhrKM888w4ULF4zqLV++nGnTptGnTx8CAgKYNWtWuX3m5OTw+uuv07VrV9q3b8+4ceM4duzYbR+fMI8V+88DZY+ij7w2Ebg2mdq3JQAarcKXO+UujxBCVESNTnhmzZrFkiVLGDp0KK+99hrW1tZMmjSJAwcO3LRdfn4+48eP588//2Ty5MlMmzaN+Ph4xo4dy9WrVw3qLlq0iH379tGyZUtsbGzK7VOr1TJp0iQ2bNjA2LFjeemll8jMzGTcuHEkJSVVxeGKu+BKXjGbj6cAMDi4IXWdbM0cUeUFNq5L34CyN6mvPnCRlOwiM0ckhBA1X41NeOLi4ti4cSMvvvgir7zyCqNHj2bp0qU0atSIDz/88KZtly9fTlJSEl9++SVPP/00TzzxBN9++y3p6eksXrzYoO6yZcvYu3cvixYtws7Ortw+N23axKFDh3jvvfd47rnneOyxx1i2bBnW1tZ89tlnVXLMovqt+esiJZqyZ9Fr6kKDFfFcv1YAqDVavv79jJmjEUKImq/GJjybNm3C2tqa0aNH68vs7e0ZOXIkhw4dIjk5udy2MTExBAUFERwcrC9r0aIF3bp149dffzWo27hx4wq9LDImJoZ69eoxYMAAfZmHhwcDBw5k27ZtqNXqyhyeMANFUfTDWS3rOxPmW7Pem1UZHX3d6d7CE4Dl+8+RkVds5oiEEKJmq7EJT3x8PH5+fjg7OxuU65KY+Ph4k+20Wi0JCQkEBgYabQsKCuL8+fPk5VX+cd74+Hjatm2L1Q3vWgoKCqKwsJCzZ89Wuk9xd+05k0FSRtkk30c7N61Rb0W/Hc/1K5vLU1Si5dvdcv0JIcTN1NiEJz09HS8vL6NyXVlaWprJdllZWajV6ttqezvx1K9f/7b7FHfXiv1lk9btbKx4sENjM0dz57o19yS0qRsA3+85R3ZBiXkDEkKIGqzGJjxFRUUm59TY29vrt5tSXFx2a/9mbXV1qiIeXdnt9Cnunoy8YmKOlk1WHhTYAPc65c/Xqi1UKhXPX5vLk1dcypI/kswbkBBC1GA1NuFxcHAwOS9Gl1g4ODiYbKdLam7WVlenKuLRld1On+LuiTp4CbVGC9Tuyco36hPgRWDjstdifBd7ltwiucsjhBCm1NiEx8vLi/T0dKNyXZluKOlGbm5u2NnZ3Vbb24lHN5R1O32Ku+P6ycrNverQuZmHmSOqOiqViuf6lt3lyS4sYdnec2aOSAghaqYam/C0bt2apKQkownGhw8fBqBNmzYm21lZWeHv78/Ro0eNtsXFxeHj42M0Ebqi8Rw/fhytVmvUp6OjI82a1Y73Md2L9p3N5My1lZXHWMBk5RsNaOtNgLcLAIt2naVAXWrmiIQQouapsQlPZGQkGo2GVatW6cvUajVRUVGEhITQsGFDAC5fvszp04arzUZERHDkyBGOHDmiLztz5gx79+4lMjLytuO5cuUKmzdv1pdlZmayadMm+vbte9M1fIR56e7u2Flb8WAtXFn5VqysVEy99sRWZr6a5fvOmzkiIYSoecpfWtjMQkJCiIyMZN68eWRkZODr60t0dDSXLl3i3Xff1dd75ZVX2L9/PwkJCfqyMWPGsHr1aiZPnsyECROwsbFhyZIleHp6MmHCBIP9bN++nRMnTgBQUlJCQkICX3zxBQD9+vWjdevWQFkS1b59e2bPns2pU6dwd3dnxYoVaDQann/++eo+HeI2Xc1X8+u1ycoRgQ3wsIDJyqYMDmrIJ1tPciY9n69+P8PYrr442FqbOywhhKgxamzCAzB37lw++eQT1q9fT3Z2NgEBAXz55Zd06tTppu2cnZ1ZtmwZc+bMYeHChWi1Wrp06cLs2bPx8DCcv7F582aio6P1Xx8/fpzjx48D0KBBA33CY21tzddff83cuXNZtmwZxcXFBAUF8d5779G8efMqPnJRVaIOXUJdqpus7GPmaKqPtZWKqX1aMnP1YdJzi/npwAXGd/Mzd1hCCFFjqBRFUcwdxL0qMTGRIUOGsGHDBlq1amXucCyOoijc//HvnErLw8/Tid/+1eeW83d2JaZzIbPwjvbboWldruSp76gfHw9HwlsZr/t0M6UaLf0+2sn5zAIa1XVgx0t9sbOpsaPWQghxRyr7GSo/DYXFOnDuKqfSyia9W8LKyrdiY23Fs31aAHA5u4g1By+aOSIhhKg5JOERFmvFtcm7ttYqHupoeZOVTXkwtAmN3RwB+GLHKUo02lu0EEKIe4MkPMIiZReUsPFI2QtmB7RtQD3ne2NhSDsbK6b0LptTdiGzkHV/XzZzREIIUTNIwiMs0tq/L1F8bbLyIxY8WdmUh8N8qO9SluB98dspNFqZpieEEJLwCItz/crKPh6O9GhRz8wR3V0OttZM7l02l+fMlXz9nS4hhLiXScIjLM7fF7I4kZILwCOdmmJlZdmTlU0Z07kpntfWHPp0W6Lc5RFC3PMk4REWZ+X+C0DZ2jQP3yOTlW/kaGfN5GtzeU6l5bH20CUzRySEEOYlCY+wKLlFJfwcVzZRt1/r+tR3dTBzROYzvpsf3q5lc3k+3npSvwCjEELciyThERZl/eHLFKg1gGWvrFwRDrbWPN+vbDGui1cLWfmnvGNLCHHvkoRHWBTdcFbDug709q9v5mjMb3QnH5p6OAHw6bZT8iZ1IcQ9SxIeYTGOXsrmyKVsAEaF+WB9D05WvpGttRUv3u8PwJW8Ypb+cc7MEQkhhHlIwiMshu5RdJUKRnW6t4ezrjc0pBEB3i4AfLnzNNmFJWaOSAgh7j5JeIRFKFCX6lcV7u3vpX+9gih7Wm3mgLK7PNmFJXzz+xkzRySEEHefJDzCImyISyavuGx+yqOdm5o5mprn/rbetPdxA+Db3WdJyS4yb0BCCHGXScIjLMLKa8NZXi729Gstk5VvpFKpeCWyNQCFJRrmbjph5oiEEOLukoRH1HoJKbkcPJ8FwMMdm2BrLZe1Kd1aeBLZrgEAUYcucfD8VTNHJIQQd498MohaTzdZGcoewxble3VQG+xsyv7bv/XzcbTyygkhxD1CEh5RqxWoS1lz8CIAPVp64utZx8wR1WxNPZ14OrwZAIcvZBEtr5wQQtwjJOERtdraQ5fJLSqbrDyuq595g6klnu3TkvouZa+ceH/TCf1kbyGEsGSS8IhaS1EUvt+TBECjug7c10YmK1dEHXsb/QTmtNxivvjtlJkjEkKI6icJj6i19p/N5ERKLgCPdfXFRiYrV9iIDo0JufaY+qJdZ0m6km/egIQQoprJJ4Sotb7fW/aaBDtrK5msXElWVireHNoWALVGy7/XHkVRZAKzEMJyScIjaqWU7CJijqYAMCS4IfWc7c0cUe0T2tRdv0jj7lNXZAKzEMKiScIjaqXl+89Teu2R6vHd/cwbTC02a2BrvK5NYH5nYzyZ+WozRySEENVDEh5R66hLtSzfV7b2TkiTuvpXJojKq+toy3+GtgMgM1/NG+uOmjkiIYSoHpLwiFrn16PJXMkrBmB8Nz/zBmMBBgU14P623kDZO8k2xF02c0RCCFH1JOERtc73e8omK3vUsWNwcEMzR1P7qVQq5owIwt3JFoDX1x4lLVdeLiqEsCw1OuFRq9V88MEH9OzZk+DgYB5++GFiY2Mr1DY1NZXp06cTFhZGaGgozzzzDBcuXDBZd/Xq1QwcOJCgoCAGDBjAsmXLjOp89tlnBAQEGP0JCgq6o2MUlXPo/FX+Olf2DqhRYT442FqbOSLL4OVizzvDy67lqwUlvLQ6Tl47IYSwKDbmDuBmZs2aRUxMDOPHj8fPz4/o6GgmTZrE0qVLCQsLK7ddfn4+48ePJzc3l8mTJ2Nra8uSJUsYO3Ysa9euxd3dXV935cqVvPnmm0RERPDkk09y4MAB3nnnHQoLC5k0aZJR3//5z39wcnLSf21tLR+4d9NXO88AYGut4vHuvmaOxrIMDm7I5uONWPf3ZXaeTOfb3Wd5uldzc4clhBBVosYmPHFxcWzcuJGXX36ZiRMnAjB8+HCGDBnChx9+yMqVK8ttu3z5cpKSkli9ejXBwcEAhIeHM3ToUBYvXsyLL74IQFFRER9//DF9+vTh008/BWDUqFFotVoWLlzI6NGjqVu3rkHfEREReHh4VMchi1s4k55HzPGyR9GHtW9Mw7qOZo7I8rwzPJCD569yIbOQuTEn6NzMQ79AoRBC1GY1dkhr06ZNWFtbM3r0aH2Zvb09I0eO5NChQyQnJ5fbNiYmhqCgIH2yA9CiRQu6devGr7/+qi/bt28fWVlZjBkzxqD9Y489RkFBATt27DDZf15enizSZgbf7DqD7rRPljsP1cLFwZbPHg3FxkpFiUbh2R8PyqPqQgiLUGMTnvj4ePz8/HB2djYo1yUx8fHxJttptVoSEhIIDAw02hYUFMT58+fJy8sD4Pjx4wBGddu1a4eVlZXJffTv35+OHTsSGhrKv/71L65cuVL5gxOVlpZbxJq/yhbGu69NfVp5u5g5IsvV3sdN/66tS1mFTFtxiFKN1sxRCSHEnamxQ1rp6el4eXkZlevK0tLSTLbLyspCrVbfsq2zszPp6elYW1vj6elpUM/Ozg43NzeDfbi6ujJ27Fjat2+PnZ0dBw4cYPny5Rw5coQ1a9YYJWaiai2OTUJ97UN3Su8WZo7G8j0V3ozDF7PYEJfM7lNXmBuTwKuD2pg7LCGEuG01NuEpKirCzs7OqNze3l6/3ZTi4rL1WW7WVlenqKgIW1tbk/3Y29sb7OPxxx832B4REUFwcDD/+te/WL58uckJzqJq5BaV8MO192Z19HUnzE/mUFU3lUrF3JHBJKbmkZCay9e/n8HPsw5jujQ1d2hCCHFbauyQloODA2q18dwBXbLi4OBgsp0uqblZW10dBwcHSkpKTPZTXFxc7j50hg4dipeXF3/88cdN64k7s3L/BXKLSgG5u3M3OdnZ8M34MDzrlP3y8Pq6o+w8mW7mqIQQ4vbU2ITHy8uL9HTjH666svr165ts5+bmhp2dXYXaenl5odFoyMjIMKinVqvJysoqdx/Xa9CgAdnZ2besJ26PulTLt7vPAtCyvjP9W9/6e2IJ3JxM33m825p6OvHN42HY21ih0So888NfHDx/1dxhCSFEpdXYIa3WrVuzb98+8vLyDObHHD58GIA2bUzPJ7CyssLf35+jR43fCRQXF4ePj4++P10fR48epXfv3vp6R48eRavV0rp165vGqCgKly5dom3btpU7OFFhaw5eJCWnbGhxUq/mWFmpzBzR3WFjpWLfmQz9vKXbZWdtRZfmnreueBOhTd35eHR7pi4/SIFawxPf7WfV5G60aeh6R/0KIcTdVGPv8ERGRqLRaFi1apW+TK1WExUVRUhICA0blr1S4PLly5w+fdqgbUREBEeOHOHIkSP6sjNnzrB3714iIyP1ZV27dsXNzY0VK1YYtF+xYgWOjo706dNHX5aZmWkU4/Lly8nMzCQ8PPyOjlWYVlyq4fPtpwBo7ObI8PaNzRzR3aXWaLmQWXhHf+40YdIZFNSQOSPKVmLOKSpl3Lf7OJGSUyV9CyHE3VBj7/CEhIQQGRnJvHnzyMjIwNfXl+joaC5dusS7776rr/fKK6+wf/9+EhIS9GVjxoxh9erVTJ48mQkTJmBjY8OSJUvw9PRkwoQJ+noODg5MmzaNt99+m2nTphEeHs6BAwdYv349L7zwAm5ubvq6ffv2ZdCgQfj7+2NnZ8fBgwfZuHEjbdq0MVgrSFSdnw5c5FJWIQDP9WuJnU2Nzc/vCY92bkp+cSnvbIznSp6aR77eyw8TuxDYuO6tGwshhJnV2IQHYO7cuXzyySesX7+e7OxsAgIC+PLLL+nUqdNN2zk7O7Ns2TLmzJnDwoUL0Wq1dOnShdmzZxutkvzYY49ha2vLd999x/bt22nYsCGzZ882eipr6NChHDp0iJiYGNRqNY0aNeKpp55iypQpODrKir9VrahEw4Jrd3d8PBwZ2bGJmSMSAE+FN0dR4N1f4skqKOHRb/by1biOdG9Rz9yhCSHETakUWTLYbBITExkyZAgbNmygVatW5g6nRvl291n+u6FsYci5I4MZFeZzV/a7KzGdC5mFd9RHh6Z1uZKnvqN+qqIPKEsWw1sZr0l1p77fk8Qb644BZe81+/DhEIbdY0OOQgjzquxnqIwRiBonu7CEz7cnAtCsXh0e7CAfpDXN+G5+fDK6PbbWZa+gmL7ybz6IOYFG3rAuhKihJOERNc6XO09ztaBsfaSXIwKwsZbLtCYa3qExS5/sjItD2cj4gt9OM2HJn1yVd28JIWog+SQRNUpydiHfXVt3p0NTNyIDG5g5InEz3VvWY93UHrSsX7bUw86T6Qycv4s9pzNu0VIIIe4uSXhEjTJ3UwLFpWWPUr86qA0q1b2x7k5t1tzLmehnuzPwWnKaklPEmEV7+c/6Y+QWmV7JXAgh7jZJeESNcSApk+hDZW9Ej2zXgE7yzqw7drdWbHZxsOWLx0J5d0QgDrZWKAos+SOJ++f9TsyxlLsSgxBC3EyNfixd3Ds0WkX/1I+9jRWvDZY3c1eFu7lis0ql4rEuvnRp5smr0UfYfzaTlJwiJi/7i/vbevPGkLZczrrzxRCrYvVoIcS9RxIeUSMs33+e48llK/c+06cFPh5OZo7IcuhWbL4TPh4VX2uqZX1nVj7dlf/76yLv/hJPdmEJW46nsiMhjd7+XoT5eeDqcPt3nioTixBC6MiQljC7lOwi5v56AoAm7o7yRnQLYGWlYlQnH7bN7M2Ia8sKlGgUtsan8dHmBH49mkx+camZoxRC3Esk4RFmpSgK/157lNxrH37/HR6Ig621maMSVaWesz0fj27P+ud60Mu/bAHEEo3CrsQrzI05wdpDl0i79nJYIYSoTpLwCLPaeCSZrfGpAAxv34i+AfXNHJGoDsFN3Ph+QmdeG9QaP8+y4coSjcL+pEw+2ZbI4tizJKTkopWF34UQ1UTm8AizSc0p4vW1RwHwqGPHG0PbmTkiUd1aN3Tl6fDmnLmST+ypKySk5KIAiWl5JKblUdfRlg5N3ejY1B1PZ3tzhyuEsCCS8Aiz0GoV/rX6sH5F5beHtcOjjp2ZoxJ3g0qlooWXMy28nLmSV8wfpzM4eO4qao2W7MISdiSksyMhHT9PJ4KbuNGukSsudzDJWQghQBIeYSbfxZ5lV+IVAB4KbcKQ4EZmjkiYQz1nex4IacSAtt7EXczmr3OZXLha9kRZUkYBSRkF/Hz4Ms3q1SGwcV3aNXIF5CktIUTlScIj7rq/zmXy/qayp7J8PBz5zwNtzRyRMDcHW2s6N/OgczMP0nKK+Ov8VeIuZpNdWIICnLmSz5kr+fx8+DJBTepyJj2f/m3q08Rdli8QQlSMJDzirkrLLeKZHw5SolGwtVYx/5EOMlwhDNR3dWBgYEMi2jXgQmYBRy9lc+RSNjlFpShA3MVs4i5m8+b6Y/h4ONLBx50OTd1o7lUHqwq+ikQWL7z33OkCnHLN1H6S8Ii7prhUw9QfD5KWWwzAG0PbEdrU3cxRiZrKSqXC17MOvp51GBjUkAuZBRy5lE1Cai4ZeWVvZL+QWciFzELWH76Ms70NrRu40LqBKy3rO2NnU/5DqLJ44b3nThfglGum9pOER9wVZZOU4/gz6SoAIzs2YWyXpmaOStQW1yc///Zx5a9zWexISOdESi6Xsso+xPKKSzlw7ioHzl3FxkqFX706+Nd3ppW3C/Vd7OVFtELc4yThEXfF+zEn+PnwZQDCfN15Z3igfACJ26JSlSUz/a2s6N/Gm5zCEk6k5HIiJYdTaXmUahVKtQqn0vI4lZYHR1Oo62hLq2vJT0svZ3MfghDCDCThEdXus22JfLXzDADNverwzfgwWU1ZVBlXR1v9hGd1qZbT6XmcTM3lZGquftmD7MIS/d0fFWXv+xoS3IjeAV4ENa6LtZUk30JYOkl4RLVa8NspPtpyEoD6LvYseaIz7rLejqgmdjZWtGnoSpuGriiKQkaempNpuSSm5nHmSh4lGkW/0OHHW0/y8daTuDnZEt7Ki/BW9ejewlOe/BLCQknCI6qFVqswNyaBL3eeBsrWW1n+dFeaesqHibg7VCoV9VzsqediT/cW9SjRaDmXUcDJ1FySMvK5eG29n6yCEn4+fFk/5NrUw4nuLTzpdu1PfRcHcx6GEKKKSMIjqlxxqYbZa44QdegSUJbsrHi6Cy3ry9wJYT621la0rO9My/rO+Hg40rK+M7tOXmHnyXR2JaaTU1T2AtvzmQWczyxg5Z8XAPD3dqZ7i3p0be5JJz955YUQtZUkPKJKJWcX8swPB/n7QhYAzerVYemTneXOjqhxGtZ1ZFQnH0Z18kGjVTh2OZs/Tmfwx+kM/jybSWGJBoCTqXmcTM1jyR9JQNk8tE6+HnT0c6eTnwd+nk4yAV+IWkASHlFlfjuRxr9WHyYjv2yNlA5N3Vg0Pkx+I7YAbk6WtTjkjcdjbaUiuIkbwU3cmNK7BepSLYcvZvHHqQz+OH2FQ+ez9IvWnUnP50x6PqsOlN0BqudsR0dfd8J8PQj1daNtw7o42smk/OpSotGSma/mSl4xGXlqrhaoKVRrKCzRUKDWUFSi0X+tVUClAhWQkl1EYYkGO2srbG2ssLO2wt7Gijr2NtSxt8HZ3oY69tbYWVtJAmuhJOERdyynqIQ5G+P1QwAAj3VpyhtD22JvIz/4LYGNleqOV6pt4Fpz5sJU9HhCfd0I9XWjuFTDqbR8Eq89/ZWYlkdRSVnbK3lqYo6lEnMsFSj7gG3s5kizenX0f5p6ON10IURLXMX3dq+X/OJS0nKLScstprC4lKSMApKzi8gqUJNTVEJesaYaov2HrbWKOvY21HW0xcPJDvc6drg72ZFXXEJzL2cauDrIU321lCQ84rZptQprDl7k/U0JXMkrWz3Z2d6Gtx5ox0Mdm5g5OlHV7nSl2nrONevpvMoej6uDLR19Pejo64FWUUjJLkKjaDl0Pov45FyyC8segVcUuHi1kItXC/UvyLVSQX0XBxq7OeJd1wFvV3u8XR1wsbdBpVJZ5Cq+5Z1fraKQU1hCRr6aq/lqMvLVZF73RzeUeLtUlM3XsrVWoVKpUChLcDVahRKNlhKNFq1SfvsSjUJWQQlZBSWcyygw2DbnlxPY2VjRvF4d/ZpOrbzL5oX5eda5aVIrzE8SHlFpiqIQcyyV+dsSiU/O0Zd3b+HJ3JHB8livsHhWKhWN3Bzp0LQu3VvU40JmIVkFai5eLeRS1rU/Vwv1H95aBVJyikjJKTLox9HWGm9XB1p41eFUWh5+9erg51mHJu6O2FrX3g/PvOJSLmQWcPxyLpn5xWQW/JPQXC0oQXOzjOMGzvY2uDnZUtfRFmf90JON/t9OdtbY2VhhZ2OFrbUVNlYqoyGpDk3rciVPzYXMQhRFQaMolJQqFJVoyFeXkldcSn6xhvzisn/nFZdytaAsIcu99g43HXWp9tpCl7kG+7C2UuHr6USr+s74e7vQytuFAG8XmtWTRKimkIRHVFiBupToQ5dY+kcSJ1Pz9OXerva8OqgND4Q0krFvcc9yc7LDzcmOwMZ1gbJfDK4WlOiTn0tZBVzOKjK4g1FYoiEpI5+kjHy2nUjTl1tbqWji7lj2Og0PJxq6OdCwrgMN6zrSsK4D3q4OZlu8U6tVyCosIS23iIuZhVy8WqC/o3Uxq+zfWdcWfKwIKxXUdbTFs4497nXs8Kxjp/+7T0A9Ckvu7M7ijVQqFTYqFTZ24Ghnfct1wUo1WrIKSrC2BldHO86m55OYlsvptDwuZ/+TwGq0in5+l254E/jnNSfeZYlQ2R9nfD3r1Oqktjaq0QmPWq1m/vz5rFu3jpycHAICApgxYwY9evS4ZdvU1FTmzJlDbGwsWq2WLl268Oqrr+Lj42NUd/Xq1Xz33XdcvHiRhg0bMm7cOMaNG3dHfVqK4lINe89ksv7vy2w6mky++p8f1nUdbXk6vBlP9mhGHfsafSkJcdepVCo86tjhUceOoOuSoLziUlJziknNKdL/Sc8r1s8JgrIPz3MZBUZDKtfT9e3uZIu7U9k8E7c6Zf92trfBwdYaB1srHGyssbe1wsHWGltrKxRFQatg8LdGUSgovv5uRyl51+54XC1QcyVPzZXcYq7kFZOZr6a0EndooGxBSM9r8Xo42eHh/M+/3Zzsyp0TU8fehsISdaX2VdVsrK2o52KPj4cj4a28DLblFZdyOi2PxGuvMTmVVja/63xmAcq1U3T9a05+OZKib2tnbUVzr7KhMf/6zvg3cKFVfWeauN98vpe4fTX6U2rWrFnExMQwfvx4/Pz8iI6OZtKkSSxdupSwsLBy2+Xn5zN+/Hhyc3OZPHkytra2LFmyhLFjx7J27Vrc3f95Q/fKlSt58803iYiI4Mknn+TAgQO88847FBYWMmnSpNvqszYr0Wg5fjmHP5My+TMpk9hTGeQVlxrUaezmyLhuvjzWpSkuDpb19I4Q1UmlUuHiYIuLg63BulRN3B1o3cCVpIwCkq7k6+/6nL1SwMWrBeQWlRr1pRsiqgnsbKxo4u5IE3ena387kldUilYpS8zq2Flb5N1fZ3sbQnzcCPFxMygvVGs4lXbtFSfXVvpOuO5Ft1A2x8nU0JiVChq5OeLr6URTjzr4eTrp/+3r6SS/XN6BGnvm4uLi2LhxIy+//DITJ04EYPjw4QwZMoQPP/yQlStXltt2+fLlJCUlsXr1aoKDgwEIDw9n6NChLF68mBdffBGAoqIiPv74Y/r06cOnn34KwKhRo9BqtSxcuJDRo0dTt27dSvVZW2i1Cmm5xZxOL/vNQ/f+ocMXsk1OGrSztqJ/m/o8GNqEvgFe2MitWCGqjEqlor6rA/VdHejczMNoe15xKSnZRSRnF5KcXXTt32VPLmXmq8kqKCmbc1KgpkRTubsvN+NkZ42bo23ZitXO9tRztrv2tz1eLvY0vpbc1Ktjj9UNd2l2JaZX6VBUbeJoZ01Qk7oENalrUJ5XXPpPIpSSy8m0PBJTc0m+bmhMe92k91gyjPp2c7KlgWvZsGYDVwe865b9Xd/FHvc6trhdu9tX19FWnia7QY1NeDZt2oS1tTWjR4/Wl9nb2zNy5EjmzZtHcnIyDRs2NNk2JiaGoKAgfWIC0KJFC7p168avv/6qT0727dtHVlYWY8aMMWj/2GOP8fPPP7Njxw6GDRtWqT7N7UJm2dL5OUUl5BaVkltUSk5hCTlFpVzNV5Oiu4WeW3zL29L1Xezp2tyT+9p60yfAC1e5myOEWTjb2+hXib4ZRVHIV2soKC6lqERLUWnZujRFJVqKSzWUaLSoVCqsVCqsVKDi2t8qFU521gbr0dSxszFKYsSdcba3ob2PG+1vuCOUXVjCqbRcTqflcy4zn3MZZat9n8so0D/9p6N7guzGO0M3Ul2bG+XuZIeb0z9DnU521jjZXfvb3honW2uc7G2oY2eDo51u4rcVdjaqa0+7lT3x9s+/y762ttJdRypUKvTXlLWJSeM1RY1NeOLj4/Hz88PZ2fA/uC7hiI+PN5nwaLVaEhISeOihh4y2BQUFsXv3bvLy8nB2dub48eMABAYGGtRr164dVlZWxMfHM2zYsEr1WRlqddnt6HPnzlWqXXkSU3N5bsUh/djxrVx/STraWtHE3Qn/Bi60a+hKYOO6eLvaX7tw80m9kE9qeR1ZkLSLWVjdwVozAJfOZZJdWHJH/VRFHzWtH4mlfGkFViSSdUd9mKICHK/9AcDUPGfl2p8iUIogl7I/d6qm/F+qqn6q63uk4wp0cIMObtbQ3AVwASC3qJTk7EIuZ5Xd3UvLLSYjr2w+1ZU8NdmFJeX+zM/Ohuxqi/jmypIgsKIsIXKwteap8OZEBjaosn3oPjt1n6W3UmMTnvT0dLy8vIzKdWVpaWlG2wCysrJQq9W3bOvs7Ex6ejrW1tZ4ehou+GVnZ4ebm5t+H5XpszKSk5MBmDp1aqXa3czt3oPRAOeu/dlSZdEIIYSoTrXlvnsR8PlG+Lwa+k5OTqZdu3a3rFdjE56ioiLs7IwfF7S3t9dvN6W4uGwBvJu11dUpKirC1tb05WJvb6/fR2X6rIzOnTuzYMECGjZsaLJvIYQQQpimVqtJTk6mc+fOFapfYxMeBwcHk7epdImFg4PpZep1CcjN2urqODg4UFJier2I4uJi/T4q02dluLq6ct9991W6nRBCCCGo0J0dnRr7qI2Xlxfp6elG5bqy+vXrm2zn5uaGnZ1dhdp6eXmh0WjIyDCcCa9Wq8nKytLXq0yfQgghhKh5amzC07p1a5KSksjLyzMoP3z4MABt2rQx2c7Kygp/f3+OHj1qtC0uLg4fHx/9XBtdHzfWPXr0KFqtltatW1e6TyGEEELUPDU24YmMjESj0bBq1Sp9mVqtJioqipCQEP0TWpcvX+b06dMGbSMiIjhy5AhHjhzRl505c4a9e/cSGRmpL+vatStubm6sWLHCoP2KFStwdHSkT58+le5TCCGEEDWPSlEq+hDz3Td9+nS2bt3K448/jq+vL9HR0Rw5coQlS5bQqVMnAMaNG8f+/ftJSEjQt8vLy2PEiBHk5+czYcIEbGxsWLJkCRqNhnXr1uHh8c/CXj/++CNvv/02ERERhIeHc+DAAdauXcsLL7zAlClTbqtPIYQQQtQsNTrhKS4u5pNPPuHnn38mOzubgIAApk+fTnh4uL6OqYQHICUlxei9V7Nnz8bX19doPz/99JPBu7Qee+wxHn/8caPFkyrTpxBCCCFqjhqd8AghhBBCVIUaO4dHCCGEEKKqSMIjhBBCCIsnCY8QQgghLJ4kPEIIIYSweJLw3EOioqIICAgw+cfUKtLbtm1jxIgRBAUF0adPHz799FNKS0vNEHnVU6vVfPDBB/Ts2ZPg4GAefvhhYmNjzR1Wtdq3b1+53/+///7boO7Bgwd59NFHCQkJoUePHrzzzjvk5+ebJ/A7lJ+fz6effsrEiRPp3LkzAQEBREVFmax7+vRpJk6cSIcOHejcuTMvvfQSmZmZRvW0Wi3ffPMN/fr1IygoiKFDh7Jhw4bqPpQ7VtFzMWvWLJPXiak1x2rruYiLi+Ptt99m8ODBtG/fnj59+jB9+nTOnj1rVNfSr4uKnovafl3U2Hdpieozbdo0mjRpYlDm6upq8PXOnTuZOnUqnTt35vXXX+fkyZMsXLiQjIwM3nrrrbsZbrWYNWsWMTExjB8/Hj8/P6Kjo5k0aRJLly4lLCzM3OFVq3HjxhEUFGRQ1rRpU/2/4+PjeeKJJ2jRogWzZs0iJSWF7777jqSkJBYtWnS3w71jV69eZcGCBTRq1IiAgAD2799vsl5KSgqPPfYYLi4uvPDCCxQUFPDdd99x8uRJVq9ebfCC348//pivv/6aUaNGERQUxLZt25g5cyYqlYrBgwffrUOrtIqeCyh7WfI777xjUObi4mJUr7aei0WLFnHw4EEiIyP1v/T9+OOPPPjgg6xatQp/f3/g3rguKnouoJZfF4q4Z6xZs0bx9/dX4uLibll30KBBygMPPKCUlJToy+bNm6cEBAQop06dqs4wq93hw4cVf39/ZdGiRfqyoqIi5b777lNGjx5txsiq1969exV/f3/l119/vWm9p556SunRo4eSm5urL/vpp58Uf39/ZdeuXdUdZpUrLi5W0tLSFEVRlLi4OMXf319Zs2aNUb0333xTCQ4OVi5duqQvi42NVfz9/ZWVK1fqy1JSUpR27dopb731lr5Mq9UqY8aMUXr16qWUlpZW49HcmYqei1deeUVp3779Lfurzefir7/+UoqLiw3Kzp49qwQGBiozZ87Ul90L10VFz0Vtvy5kSOselZeXh0ajMbnt1KlTnDp1ilGjRmFj889NwDFjxqAoCjExMXcrzGqxadMmrK2tGT16tL7M3t6ekSNHcujQIZKTk80Y3d2Rl5dncngyLy+PP/74gwceeMDg/XDDhg3DycmJX3/99W6GWSXs7Ozw8vK6Zb3NmzfTp08fGjVqpC/r3r07fn5+Bse9detWSkpKGDNmjL5MpVLx6KOPkpKSwqFDh6r2AKpQRc+FjkajMXqf4fVq87kIDQ01uDsD4OfnR6tWrThz5oy+7F64Lip6LnRq63UhCc89aPz48XTs2JGQkBCmTJlCUlKSwfbjx48DGA17eHt706BBA+Lj4+9WqNUiPj4ePz8/oxe+BgcH67dbstmzZ9OxY0eCg4MZN26cwfvhEhISKC0tJTAw0KCNnZ0dbdq0sdhzk5qaSkZGhtFxQ9l1cf1xx8fH4+TkRIsWLYzq6bZbgsLCQjp27EjHjh3p3Lkzb731ltE8Lks7F4qicOXKFdzd3YF7+7q48Vzo1ObrQubw3EMcHBx48MEH6dKlC87Ozhw9epQlS5bwyCOPEB0drX8hq24Cs6nfBL28vEhLS7urcVe19PT0co8NqPXHVx5bW1siIiLo1asX7u7unD59mm+//ZbHHnuMlStX0rZtW/33vn79+kbtvby8+Ouvv+522HeF7nte3nWRlZWFWq3Gzs6O9PR0PD09jV49Y0nXj5eXF0899RRt27ZFURR27drF8uXLOXHiBMuWLdPf+bW0c7F+/XpSU1OZNm0acG9fFzeeC6j914UkPLWUVqulpKSkQnXt7OxQqVQMGjSIQYMG6cvvu+8+evbsydixY1m4cCFvv/02AEVFRfp2N7K3t7/prczaoKioqNxj0223RKGhoYSGhuq/7t+/PxERETzwwAN89NFHfPvtt7f83lvquSkuLgbKP27457q5F66fmTNnGnw9ePBg/Pz8+Pjjj4mJidFPOrWkc3H69GnefvttOnTowIgRI4B797owdS6g9l8XMqRVS/35558EBwdX6I+pMVidsLAwQkJC2LNnj77MwcEBKHt0+0bFxcX67bWVg4NDucem236v8PX1pX///uzbtw+NRmPx3/vy6H4QV+S6uFevnyeeeAIrKyv++OMPfZmlnIv09HQmT56Mi4sL8+fPx9raGrg3r4vyzkV5atN1IXd4aqnmzZvz3nvvVaiuqeGJ6zVo0MBgvQXdbcf09HT9MJdOenq6fhy2tvLy8iI1NdWo/GbDOZasQYMGlJSUUFhYeNNbzunp6RZ7bnTHZWo9qvT0dNzc3PS/sXp5ebFv3z4URTG4ZW/p14+DgwNubm5kZ2fryyzhXOTm5vL000+Tm5vLjz/+iLe3t37bvXZd3OxclKc2XReS8NRSXl5ePPjgg1XS14ULFwwmprVp0waAI0eOGCQ3qamppKSkMGrUqCrZr7m0bt2affv2kZeXZzBx+fDhw8A/x3+vuHjxIvb29jg5OeHv74+NjQ1Hjx41GP5Uq9XEx8czcOBAM0Zafby9vfHw8ODo0aNG2+Li4mjdurX+6zZt2rB69WpOnz5Ny5Yt9eWWfv3k5eVx9epVPDw89GW1/VwUFxfrH9xYvHixwTHAvXVd3OpclKc2XRcypHUPMbUy6M6dOzl27Bjh4eH6slatWtG8eXN++ukng0fXV6xYgUqlMrmqZm0SGRmJRqNh1apV+jK1Wk1UVBQhISFGd7Ushanv/4kTJ9i+fTs9evTAysoKFxcXunXrxvr16w3maq1bt46CgoJa/72/mQEDBrBjxw6DZQn27NlDUlKSwXH3798fW1tbli9fri9TFIWVK1fi7e1Nhw4d7mrcVa24uNjkPL0vvvgCRVEMflbU5nOh0WiYMWMGf//9N/Pnzy831nvhuqjIubCE60Lu8NxDHnnkEdq0aUNgYCAuLi4cP36cNWvW0LBhQ6ZMmWJQ9+WXX+aZZ55hwoQJDB48mJMnT/Ljjz/y8MMPGz1qWNuEhIQQGRnJvHnzyMjIwNfXl+joaC5dusS7775r7vCqzYwZM3BwcKBDhw54enpy6tQpfvrpJxwcHPjXv/6lr/fCCy/wyCOPMG7cOEaNGkVKSgqLFy+mZ8+e9OrVy4xHcPt++OEHcnJy9EN1v/32GykpKUDZytMuLi5MmTKFTZs2MX78eMaPH09BQQHffvst/v7+PPTQQ/q+GjRowPjx4/n2228pLS0lKCiIrVu3cuDAAT788MNbznkwt1udi+zsbEaMGMHgwYNp3rw5ALt372bnzp2Eh4fTv39/fV+1+Vz873//Y/v27fTt25esrCzWrVtnsH3YsGEA98R1UZFzkZ6eXuuvC5WiKIpZ9izuuo8//pidO3dy8eJFioqK8PLyonfv3jz33HPUq1fPqP7WrVv5/PPPOX36NB4eHowYMYKpU6dia2trhuirVnFxMZ988gk///wz2dnZBAQEMH36dIPfUizN999/z88//8z58+fJy8vD3d2dbt268dxzz+Hr62tQV/eD6fjx49SpU4eBAwfy4osvGq1dVFv069ePS5cumdy2bds2/atWEhMT+d///sdff/2Fra0tvXv3ZtasWUb/P3TvCVq1ahVpaWn4+fkxadIkHnjggWo/ljt1q3Ph6urKf//7Xw4fPkxaWhoajQZfX1+GDh3KhAkTjP7/19ZzMW7cuJu+WiMhIUH/b0u/LipyLnJycmr9dSEJjxBCCCEsnszhEUIIIYTFk4RHCCGEEBZPEh4hhBBCWDxJeIQQQghh8SThEUIIIYTFk4RHCCGEEBZPEh4hhBBCWDxJeIQQQghh8SThEUIIIYTFk4RHCCGEEBZPEh4hhBBCWDxJeIQQQghh8SThEUIIIYTFk4RHCCGEEBZPEh4hhBBCWLz/B1G93KDsvf3gAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 600x360 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fc = read_dataset('/uftp/users/ashchetsova/hackathon/ID_vs_population_n_cells.tsv')\n",
    "fc_old = read_dataset('/uftp/users/ashchetsova/hackathon/data_90_day.tsv')\n",
    "fc = fc.reindex(fc_old.index)\n",
    "\n",
    "anno = read_dataset('/uftp/users/ashchetsova/hackathon/total_annotation.tsv')\n",
    "anno.index = anno.Sample_ID\n",
    "anno = anno[['ID','cGVHD_development_time', 'cGVHD_development', 'Days','Day_after_transplant']]\n",
    "anno = anno[~anno.index.duplicated()]\n",
    "durations = pd.concat([anno[anno.cGVHD_development_time.isna()].ID.map(lambda x:anno[anno.ID==x].Day_after_transplant.max()),anno[~anno.cGVHD_development_time.isna()].cGVHD_development_time\n",
    "                      ])\n",
    "events = fc_old.GVHD_status\n",
    "durations = durations.reindex(events.index)\n",
    "\n",
    "fig,ax=plt.subplots(figsize=(5,3))\n",
    "sns.distplot(fc.T.mean(),ax=ax)\n",
    "plt.title('Mean non-scaled absolute values')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1ec2eb1f-abfa-46f1-a947-512522632f6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Mean log-scaled absolute values')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 600x360 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fc = np.log2(fc+1)\n",
    "fig,ax=plt.subplots(figsize=(5,3))\n",
    "sns.distplot(fc.T.mean(),ax=ax)\n",
    "plt.title('Mean log-scaled absolute values')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "62933442-2d3a-46ff-a58a-3da787dd3663",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CD8+ \tlen\t 52 mean\t 87.37587412587412 std\t 11.376954448937838\n",
      "CD4+ \tlen\t 52 mean\t 92.7972027972028 std\t 7.8164979942816055\n",
      "Treg \tlen\t 19 mean\t 59.950558213716114 std\t 14.845026929214987\n",
      "Treg_Th22 \tlen\t 1 mean\t 47.871212121212125 std\t 18.5618116253735\n",
      "Treg_Th2 \tlen\t 2 mean\t 49.14772727272727 std\t 15.685673109850622\n",
      "Treg_Th17.1 \tlen\t 1 mean\t 12.5 std\t 7.767288409140556\n",
      "Treg_Th17 \tlen\t 2 mean\t 37.484848484848484 std\t 10.664812630244773\n",
      "Treg_Th1 \tlen\t 3 mean\t 31.335858585858592 std\t 9.868991948117198\n",
      "Th22 \tlen\t 7 mean\t 31.945887445887447 std\t 8.732138626539442\n",
      "Th2 \tlen\t 14 mean\t 42.83008658008657 std\t 7.405058574276365\n",
      "Th17.1 \tlen\t 7 mean\t 27.023809523809522 std\t 9.514884096067108\n",
      "Th17 \tlen\t 14 mean\t 40.05844155844156 std\t 6.672619244418973\n",
      "Th1 \tlen\t 21 mean\t 43.7435064935065 std\t 6.787488098542187\n",
      "Tfh \tlen\t 1 mean\t 101.00757575757575 std\t 19.046197364110085\n"
     ]
    }
   ],
   "source": [
    "parent_to_daughter = {i.split(' ')[0]:i for i in fc.columns}\n",
    "fc_ranked = fc.T.rank().T\n",
    "for key in parent_to_daughter.keys():\n",
    "    cols = fc.columns[fc.columns.str.contains(key)]\n",
    "    print(key,'\\tlen\\t',len(cols),'mean\\t',fc_ranked[cols].T.mean().mean(),'std\\t',fc_ranked[cols].T.mean().std())\n",
    "    parent_to_daughter[key] = fc_ranked[cols].T.mean()\n",
    "parent_to_daughter = pd.DataFrame(parent_to_daughter)\n",
    "parent_to_daughter['CD4_to_CD8'] = parent_to_daughter['CD4+']/parent_to_daughter['CD8+']\n",
    "parent_to_daughter['Th'] = parent_to_daughter[['Th17',\t'Th1','Th22','Th17.1','Tfh']].T.mean()\n",
    "parent_to_daughter['Treg_to_Th'] = parent_to_daughter['Treg']/parent_to_daughter['Th']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "088d4671-2989-41ed-bf75-e834ecd579bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# it's important to set Random seed to make results reproducible. choose your lucky number\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "# if you want to limit the number of parallel threads, \n",
    "# which is especially important when working on biomics and galactomics, set the number here. \n",
    "# the default is using all threads availiable (for quber)\n",
    "n_jobs = -1\n",
    "#n_jobs = 0 #default for the server's system settings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efe0975c-b0c2-4bb8-9f1f-2bec0e7948d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "357d7e47-ea61-4b90-a7e9-a13480014b3e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c847be4c-f4ef-41d5-bc94-54709a89bb05",
   "metadata": {},
   "source": [
    "# Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a9f06109-0f78-45f2-ae61-5069e513bb56",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pycaret.classification import *\n",
    "exp = ClassificationExperiment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6cc1ed20-c209-4920-9935-4fbeb2ba8e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.concat([fc[['CD4+ NV maturity and HLA-DR+ expression', 'CD4+ NV maturity and PD-1+TIGIT+ expression', 'CD4+ NV maturity and PD1+ expression', 'CD4+ None maturity and None expression', 'CD4+ CM maturity and CD226+ expression', 'CD8+ EM maturity and None expression', 'CD4+ CM maturity and PD-1+TIGIT- expression', 'CD4+ NV maturity and PD-1+TIGIT- expression', 'CD4+ CM maturity and None expression', 'CD4+ NV maturity and TIGIT+ expression', 'Th22 CM maturity and None expression', 'CD8+ EMTM maturity and PD-1+TIGIT+ expression']], events],axis=1)\n",
    "data.columns = data.columns.to_list()[:-1]+['GVHD_Development']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4072b844-8b21-4561-9be8-65b3c5e6730b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_3cb4c\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_3cb4c_level0_col0\" class=\"col_heading level0 col0\" >Description</th>\n",
       "      <th id=\"T_3cb4c_level0_col1\" class=\"col_heading level0 col1\" >Value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_3cb4c_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_3cb4c_row0_col0\" class=\"data row0 col0\" >Session id</td>\n",
       "      <td id=\"T_3cb4c_row0_col1\" class=\"data row0 col1\" >42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_3cb4c_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_3cb4c_row1_col0\" class=\"data row1 col0\" >Target</td>\n",
       "      <td id=\"T_3cb4c_row1_col1\" class=\"data row1 col1\" >GVHD_Development</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_3cb4c_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_3cb4c_row2_col0\" class=\"data row2 col0\" >Target type</td>\n",
       "      <td id=\"T_3cb4c_row2_col1\" class=\"data row2 col1\" >Binary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_3cb4c_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_3cb4c_row3_col0\" class=\"data row3 col0\" >Original data shape</td>\n",
       "      <td id=\"T_3cb4c_row3_col1\" class=\"data row3 col1\" >(66, 13)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_3cb4c_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "      <td id=\"T_3cb4c_row4_col0\" class=\"data row4 col0\" >Transformed data shape</td>\n",
       "      <td id=\"T_3cb4c_row4_col1\" class=\"data row4 col1\" >(66, 13)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_3cb4c_level0_row5\" class=\"row_heading level0 row5\" >5</th>\n",
       "      <td id=\"T_3cb4c_row5_col0\" class=\"data row5 col0\" >Transformed train set shape</td>\n",
       "      <td id=\"T_3cb4c_row5_col1\" class=\"data row5 col1\" >(49, 13)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_3cb4c_level0_row6\" class=\"row_heading level0 row6\" >6</th>\n",
       "      <td id=\"T_3cb4c_row6_col0\" class=\"data row6 col0\" >Transformed test set shape</td>\n",
       "      <td id=\"T_3cb4c_row6_col1\" class=\"data row6 col1\" >(17, 13)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_3cb4c_level0_row7\" class=\"row_heading level0 row7\" >7</th>\n",
       "      <td id=\"T_3cb4c_row7_col0\" class=\"data row7 col0\" >Numeric features</td>\n",
       "      <td id=\"T_3cb4c_row7_col1\" class=\"data row7 col1\" >12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7f396002f8b0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<pycaret.classification.oop.ClassificationExperiment at 0x7f396e780250>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp.setup(data, target = 'GVHD_Development', session_id = SEED,preprocess=False,log_experiment = False,n_jobs=-1,train_size=0.75, log_plots=True, fold=5,feature_selection=True,feature_selection_method='sequential',remove_multicollinearity = True, multicollinearity_threshold = 0.7,    remove_outliers= True,  outliers_method = 'iforest',    outliers_threshold = 0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "9b0366db-0507-4186-8b79-ce5b76fcc519",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_a78ed th {\n",
       "  text-align: left;\n",
       "}\n",
       "#T_a78ed_row0_col0, #T_a78ed_row0_col2, #T_a78ed_row0_col4, #T_a78ed_row1_col0, #T_a78ed_row1_col1, #T_a78ed_row1_col2, #T_a78ed_row1_col3, #T_a78ed_row1_col5, #T_a78ed_row1_col6, #T_a78ed_row1_col7, #T_a78ed_row2_col0, #T_a78ed_row2_col1, #T_a78ed_row2_col4, #T_a78ed_row2_col5, #T_a78ed_row2_col6, #T_a78ed_row2_col7, #T_a78ed_row3_col0, #T_a78ed_row3_col1, #T_a78ed_row3_col2, #T_a78ed_row3_col3, #T_a78ed_row3_col4, #T_a78ed_row3_col5, #T_a78ed_row3_col6, #T_a78ed_row3_col7, #T_a78ed_row4_col0, #T_a78ed_row4_col1, #T_a78ed_row4_col2, #T_a78ed_row4_col3, #T_a78ed_row4_col4, #T_a78ed_row4_col5, #T_a78ed_row4_col6, #T_a78ed_row4_col7, #T_a78ed_row5_col0, #T_a78ed_row5_col1, #T_a78ed_row5_col2, #T_a78ed_row5_col3, #T_a78ed_row5_col4, #T_a78ed_row5_col5, #T_a78ed_row5_col6, #T_a78ed_row5_col7, #T_a78ed_row6_col0, #T_a78ed_row6_col1, #T_a78ed_row6_col2, #T_a78ed_row6_col3, #T_a78ed_row6_col4, #T_a78ed_row6_col5, #T_a78ed_row6_col6, #T_a78ed_row6_col7, #T_a78ed_row7_col0, #T_a78ed_row7_col1, #T_a78ed_row7_col2, #T_a78ed_row7_col3, #T_a78ed_row7_col4, #T_a78ed_row7_col5, #T_a78ed_row7_col6, #T_a78ed_row7_col7, #T_a78ed_row8_col0, #T_a78ed_row8_col1, #T_a78ed_row8_col2, #T_a78ed_row8_col3, #T_a78ed_row8_col4, #T_a78ed_row8_col5, #T_a78ed_row8_col6, #T_a78ed_row8_col7, #T_a78ed_row9_col0, #T_a78ed_row9_col1, #T_a78ed_row9_col2, #T_a78ed_row9_col3, #T_a78ed_row9_col4, #T_a78ed_row9_col5, #T_a78ed_row9_col6, #T_a78ed_row9_col7, #T_a78ed_row10_col0, #T_a78ed_row10_col1, #T_a78ed_row10_col2, #T_a78ed_row10_col3, #T_a78ed_row10_col4, #T_a78ed_row10_col5, #T_a78ed_row10_col6, #T_a78ed_row10_col7, #T_a78ed_row11_col0, #T_a78ed_row11_col1, #T_a78ed_row11_col2, #T_a78ed_row11_col3, #T_a78ed_row11_col4, #T_a78ed_row11_col5, #T_a78ed_row11_col6, #T_a78ed_row11_col7, #T_a78ed_row12_col0, #T_a78ed_row12_col1, #T_a78ed_row12_col2, #T_a78ed_row12_col3, #T_a78ed_row12_col4, #T_a78ed_row12_col5, #T_a78ed_row12_col6, #T_a78ed_row12_col7, #T_a78ed_row13_col0, #T_a78ed_row13_col1, #T_a78ed_row13_col2, #T_a78ed_row13_col3, #T_a78ed_row13_col4, #T_a78ed_row13_col5, #T_a78ed_row13_col6, #T_a78ed_row13_col7, #T_a78ed_row14_col0, #T_a78ed_row14_col1, #T_a78ed_row14_col2, #T_a78ed_row14_col3, #T_a78ed_row14_col4, #T_a78ed_row14_col5, #T_a78ed_row14_col6, #T_a78ed_row14_col7, #T_a78ed_row15_col0, #T_a78ed_row15_col1, #T_a78ed_row15_col2, #T_a78ed_row15_col3, #T_a78ed_row15_col4, #T_a78ed_row15_col5, #T_a78ed_row15_col6, #T_a78ed_row15_col7 {\n",
       "  text-align: left;\n",
       "}\n",
       "#T_a78ed_row0_col1, #T_a78ed_row0_col3, #T_a78ed_row0_col5, #T_a78ed_row0_col6, #T_a78ed_row0_col7, #T_a78ed_row1_col4, #T_a78ed_row2_col2, #T_a78ed_row2_col3 {\n",
       "  text-align: left;\n",
       "  background-color: yellow;\n",
       "}\n",
       "#T_a78ed_row0_col8, #T_a78ed_row1_col8, #T_a78ed_row2_col8, #T_a78ed_row3_col8, #T_a78ed_row6_col8, #T_a78ed_row7_col8, #T_a78ed_row8_col8, #T_a78ed_row11_col8, #T_a78ed_row12_col8, #T_a78ed_row13_col8, #T_a78ed_row14_col8, #T_a78ed_row15_col8 {\n",
       "  text-align: left;\n",
       "  background-color: lightgrey;\n",
       "}\n",
       "#T_a78ed_row4_col8, #T_a78ed_row5_col8, #T_a78ed_row9_col8, #T_a78ed_row10_col8 {\n",
       "  text-align: left;\n",
       "  background-color: yellow;\n",
       "  background-color: lightgrey;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_a78ed\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_a78ed_level0_col0\" class=\"col_heading level0 col0\" >Model</th>\n",
       "      <th id=\"T_a78ed_level0_col1\" class=\"col_heading level0 col1\" >Accuracy</th>\n",
       "      <th id=\"T_a78ed_level0_col2\" class=\"col_heading level0 col2\" >AUC</th>\n",
       "      <th id=\"T_a78ed_level0_col3\" class=\"col_heading level0 col3\" >Recall</th>\n",
       "      <th id=\"T_a78ed_level0_col4\" class=\"col_heading level0 col4\" >Prec.</th>\n",
       "      <th id=\"T_a78ed_level0_col5\" class=\"col_heading level0 col5\" >F1</th>\n",
       "      <th id=\"T_a78ed_level0_col6\" class=\"col_heading level0 col6\" >Kappa</th>\n",
       "      <th id=\"T_a78ed_level0_col7\" class=\"col_heading level0 col7\" >MCC</th>\n",
       "      <th id=\"T_a78ed_level0_col8\" class=\"col_heading level0 col8\" >TT (Sec)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_a78ed_level0_row0\" class=\"row_heading level0 row0\" >rf</th>\n",
       "      <td id=\"T_a78ed_row0_col0\" class=\"data row0 col0\" >Random Forest Classifier</td>\n",
       "      <td id=\"T_a78ed_row0_col1\" class=\"data row0 col1\" >0.7525</td>\n",
       "      <td id=\"T_a78ed_row0_col2\" class=\"data row0 col2\" >0.8045</td>\n",
       "      <td id=\"T_a78ed_row0_col3\" class=\"data row0 col3\" >0.7063</td>\n",
       "      <td id=\"T_a78ed_row0_col4\" class=\"data row0 col4\" >0.7321</td>\n",
       "      <td id=\"T_a78ed_row0_col5\" class=\"data row0 col5\" >0.7063</td>\n",
       "      <td id=\"T_a78ed_row0_col6\" class=\"data row0 col6\" >0.4948</td>\n",
       "      <td id=\"T_a78ed_row0_col7\" class=\"data row0 col7\" >0.5087</td>\n",
       "      <td id=\"T_a78ed_row0_col8\" class=\"data row0 col8\" >0.1700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a78ed_level0_row1\" class=\"row_heading level0 row1\" >knn</th>\n",
       "      <td id=\"T_a78ed_row1_col0\" class=\"data row1 col0\" >K Neighbors Classifier</td>\n",
       "      <td id=\"T_a78ed_row1_col1\" class=\"data row1 col1\" >0.7145</td>\n",
       "      <td id=\"T_a78ed_row1_col2\" class=\"data row1 col2\" >0.7304</td>\n",
       "      <td id=\"T_a78ed_row1_col3\" class=\"data row1 col3\" >0.6190</td>\n",
       "      <td id=\"T_a78ed_row1_col4\" class=\"data row1 col4\" >0.7500</td>\n",
       "      <td id=\"T_a78ed_row1_col5\" class=\"data row1 col5\" >0.6116</td>\n",
       "      <td id=\"T_a78ed_row1_col6\" class=\"data row1 col6\" >0.3983</td>\n",
       "      <td id=\"T_a78ed_row1_col7\" class=\"data row1 col7\" >0.4457</td>\n",
       "      <td id=\"T_a78ed_row1_col8\" class=\"data row1 col8\" >0.5467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a78ed_level0_row2\" class=\"row_heading level0 row2\" >xgboost</th>\n",
       "      <td id=\"T_a78ed_row2_col0\" class=\"data row2 col0\" >Extreme Gradient Boosting</td>\n",
       "      <td id=\"T_a78ed_row2_col1\" class=\"data row2 col1\" >0.7120</td>\n",
       "      <td id=\"T_a78ed_row2_col2\" class=\"data row2 col2\" >0.8063</td>\n",
       "      <td id=\"T_a78ed_row2_col3\" class=\"data row2 col3\" >0.7063</td>\n",
       "      <td id=\"T_a78ed_row2_col4\" class=\"data row2 col4\" >0.6528</td>\n",
       "      <td id=\"T_a78ed_row2_col5\" class=\"data row2 col5\" >0.6723</td>\n",
       "      <td id=\"T_a78ed_row2_col6\" class=\"data row2 col6\" >0.4176</td>\n",
       "      <td id=\"T_a78ed_row2_col7\" class=\"data row2 col7\" >0.4250</td>\n",
       "      <td id=\"T_a78ed_row2_col8\" class=\"data row2 col8\" >0.2633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a78ed_level0_row3\" class=\"row_heading level0 row3\" >catboost</th>\n",
       "      <td id=\"T_a78ed_row3_col0\" class=\"data row3 col0\" >CatBoost Classifier</td>\n",
       "      <td id=\"T_a78ed_row3_col1\" class=\"data row3 col1\" >0.7120</td>\n",
       "      <td id=\"T_a78ed_row3_col2\" class=\"data row3 col2\" >0.7738</td>\n",
       "      <td id=\"T_a78ed_row3_col3\" class=\"data row3 col3\" >0.6111</td>\n",
       "      <td id=\"T_a78ed_row3_col4\" class=\"data row3 col4\" >0.7083</td>\n",
       "      <td id=\"T_a78ed_row3_col5\" class=\"data row3 col5\" >0.6344</td>\n",
       "      <td id=\"T_a78ed_row3_col6\" class=\"data row3 col6\" >0.4025</td>\n",
       "      <td id=\"T_a78ed_row3_col7\" class=\"data row3 col7\" >0.4257</td>\n",
       "      <td id=\"T_a78ed_row3_col8\" class=\"data row3 col8\" >0.6167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a78ed_level0_row4\" class=\"row_heading level0 row4\" >lda</th>\n",
       "      <td id=\"T_a78ed_row4_col0\" class=\"data row4 col0\" >Linear Discriminant Analysis</td>\n",
       "      <td id=\"T_a78ed_row4_col1\" class=\"data row4 col1\" >0.6532</td>\n",
       "      <td id=\"T_a78ed_row4_col2\" class=\"data row4 col2\" >0.6519</td>\n",
       "      <td id=\"T_a78ed_row4_col3\" class=\"data row4 col3\" >0.6508</td>\n",
       "      <td id=\"T_a78ed_row4_col4\" class=\"data row4 col4\" >0.5741</td>\n",
       "      <td id=\"T_a78ed_row4_col5\" class=\"data row4 col5\" >0.6083</td>\n",
       "      <td id=\"T_a78ed_row4_col6\" class=\"data row4 col6\" >0.2977</td>\n",
       "      <td id=\"T_a78ed_row4_col7\" class=\"data row4 col7\" >0.3008</td>\n",
       "      <td id=\"T_a78ed_row4_col8\" class=\"data row4 col8\" >0.0200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a78ed_level0_row5\" class=\"row_heading level0 row5\" >svm</th>\n",
       "      <td id=\"T_a78ed_row5_col0\" class=\"data row5 col0\" >SVM - Linear Kernel</td>\n",
       "      <td id=\"T_a78ed_row5_col1\" class=\"data row5 col1\" >0.6520</td>\n",
       "      <td id=\"T_a78ed_row5_col2\" class=\"data row5 col2\" >0.0000</td>\n",
       "      <td id=\"T_a78ed_row5_col3\" class=\"data row5 col3\" >0.2857</td>\n",
       "      <td id=\"T_a78ed_row5_col4\" class=\"data row5 col4\" >0.5238</td>\n",
       "      <td id=\"T_a78ed_row5_col5\" class=\"data row5 col5\" >0.3386</td>\n",
       "      <td id=\"T_a78ed_row5_col6\" class=\"data row5 col6\" >0.1860</td>\n",
       "      <td id=\"T_a78ed_row5_col7\" class=\"data row5 col7\" >0.2248</td>\n",
       "      <td id=\"T_a78ed_row5_col8\" class=\"data row5 col8\" >0.0200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a78ed_level0_row6\" class=\"row_heading level0 row6\" >gbc</th>\n",
       "      <td id=\"T_a78ed_row6_col0\" class=\"data row6 col0\" >Gradient Boosting Classifier</td>\n",
       "      <td id=\"T_a78ed_row6_col1\" class=\"data row6 col1\" >0.6311</td>\n",
       "      <td id=\"T_a78ed_row6_col2\" class=\"data row6 col2\" >0.6989</td>\n",
       "      <td id=\"T_a78ed_row6_col3\" class=\"data row6 col3\" >0.5556</td>\n",
       "      <td id=\"T_a78ed_row6_col4\" class=\"data row6 col4\" >0.5833</td>\n",
       "      <td id=\"T_a78ed_row6_col5\" class=\"data row6 col5\" >0.5501</td>\n",
       "      <td id=\"T_a78ed_row6_col6\" class=\"data row6 col6\" >0.2420</td>\n",
       "      <td id=\"T_a78ed_row6_col7\" class=\"data row6 col7\" >0.2551</td>\n",
       "      <td id=\"T_a78ed_row6_col8\" class=\"data row6 col8\" >0.0500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a78ed_level0_row7\" class=\"row_heading level0 row7\" >et</th>\n",
       "      <td id=\"T_a78ed_row7_col0\" class=\"data row7 col0\" >Extra Trees Classifier</td>\n",
       "      <td id=\"T_a78ed_row7_col1\" class=\"data row7 col1\" >0.6311</td>\n",
       "      <td id=\"T_a78ed_row7_col2\" class=\"data row7 col2\" >0.7272</td>\n",
       "      <td id=\"T_a78ed_row7_col3\" class=\"data row7 col3\" >0.5079</td>\n",
       "      <td id=\"T_a78ed_row7_col4\" class=\"data row7 col4\" >0.5833</td>\n",
       "      <td id=\"T_a78ed_row7_col5\" class=\"data row7 col5\" >0.5261</td>\n",
       "      <td id=\"T_a78ed_row7_col6\" class=\"data row7 col6\" >0.2326</td>\n",
       "      <td id=\"T_a78ed_row7_col7\" class=\"data row7 col7\" >0.2457</td>\n",
       "      <td id=\"T_a78ed_row7_col8\" class=\"data row7 col8\" >0.1500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a78ed_level0_row8\" class=\"row_heading level0 row8\" >nb</th>\n",
       "      <td id=\"T_a78ed_row8_col0\" class=\"data row8 col0\" >Naive Bayes</td>\n",
       "      <td id=\"T_a78ed_row8_col1\" class=\"data row8 col1\" >0.6299</td>\n",
       "      <td id=\"T_a78ed_row8_col2\" class=\"data row8 col2\" >0.7325</td>\n",
       "      <td id=\"T_a78ed_row8_col3\" class=\"data row8 col3\" >0.6111</td>\n",
       "      <td id=\"T_a78ed_row8_col4\" class=\"data row8 col4\" >0.5563</td>\n",
       "      <td id=\"T_a78ed_row8_col5\" class=\"data row8 col5\" >0.5554</td>\n",
       "      <td id=\"T_a78ed_row8_col6\" class=\"data row8 col6\" >0.2603</td>\n",
       "      <td id=\"T_a78ed_row8_col7\" class=\"data row8 col7\" >0.2769</td>\n",
       "      <td id=\"T_a78ed_row8_col8\" class=\"data row8 col8\" >0.0233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a78ed_level0_row9\" class=\"row_heading level0 row9\" >dt</th>\n",
       "      <td id=\"T_a78ed_row9_col0\" class=\"data row9 col0\" >Decision Tree Classifier</td>\n",
       "      <td id=\"T_a78ed_row9_col1\" class=\"data row9 col1\" >0.6299</td>\n",
       "      <td id=\"T_a78ed_row9_col2\" class=\"data row9 col2\" >0.6204</td>\n",
       "      <td id=\"T_a78ed_row9_col3\" class=\"data row9 col3\" >0.5556</td>\n",
       "      <td id=\"T_a78ed_row9_col4\" class=\"data row9 col4\" >0.6481</td>\n",
       "      <td id=\"T_a78ed_row9_col5\" class=\"data row9 col5\" >0.5556</td>\n",
       "      <td id=\"T_a78ed_row9_col6\" class=\"data row9 col6\" >0.2484</td>\n",
       "      <td id=\"T_a78ed_row9_col7\" class=\"data row9 col7\" >0.2806</td>\n",
       "      <td id=\"T_a78ed_row9_col8\" class=\"data row9 col8\" >0.0200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a78ed_level0_row10\" class=\"row_heading level0 row10\" >ridge</th>\n",
       "      <td id=\"T_a78ed_row10_col0\" class=\"data row10 col0\" >Ridge Classifier</td>\n",
       "      <td id=\"T_a78ed_row10_col1\" class=\"data row10 col1\" >0.6127</td>\n",
       "      <td id=\"T_a78ed_row10_col2\" class=\"data row10 col2\" >0.0000</td>\n",
       "      <td id=\"T_a78ed_row10_col3\" class=\"data row10 col3\" >0.5476</td>\n",
       "      <td id=\"T_a78ed_row10_col4\" class=\"data row10 col4\" >0.5333</td>\n",
       "      <td id=\"T_a78ed_row10_col5\" class=\"data row10 col5\" >0.5374</td>\n",
       "      <td id=\"T_a78ed_row10_col6\" class=\"data row10 col6\" >0.2011</td>\n",
       "      <td id=\"T_a78ed_row10_col7\" class=\"data row10 col7\" >0.2028</td>\n",
       "      <td id=\"T_a78ed_row10_col8\" class=\"data row10 col8\" >0.0200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a78ed_level0_row11\" class=\"row_heading level0 row11\" >lr</th>\n",
       "      <td id=\"T_a78ed_row11_col0\" class=\"data row11 col0\" >Logistic Regression</td>\n",
       "      <td id=\"T_a78ed_row11_col1\" class=\"data row11 col1\" >0.6115</td>\n",
       "      <td id=\"T_a78ed_row11_col2\" class=\"data row11 col2\" >0.7310</td>\n",
       "      <td id=\"T_a78ed_row11_col3\" class=\"data row11 col3\" >0.5556</td>\n",
       "      <td id=\"T_a78ed_row11_col4\" class=\"data row11 col4\" >0.5238</td>\n",
       "      <td id=\"T_a78ed_row11_col5\" class=\"data row11 col5\" >0.5348</td>\n",
       "      <td id=\"T_a78ed_row11_col6\" class=\"data row11 col6\" >0.2061</td>\n",
       "      <td id=\"T_a78ed_row11_col7\" class=\"data row11 col7\" >0.2091</td>\n",
       "      <td id=\"T_a78ed_row11_col8\" class=\"data row11 col8\" >0.6600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a78ed_level0_row12\" class=\"row_heading level0 row12\" >ada</th>\n",
       "      <td id=\"T_a78ed_row12_col0\" class=\"data row12 col0\" >Ada Boost Classifier</td>\n",
       "      <td id=\"T_a78ed_row12_col1\" class=\"data row12 col1\" >0.6103</td>\n",
       "      <td id=\"T_a78ed_row12_col2\" class=\"data row12 col2\" >0.6458</td>\n",
       "      <td id=\"T_a78ed_row12_col3\" class=\"data row12 col3\" >0.5000</td>\n",
       "      <td id=\"T_a78ed_row12_col4\" class=\"data row12 col4\" >0.5595</td>\n",
       "      <td id=\"T_a78ed_row12_col5\" class=\"data row12 col5\" >0.5134</td>\n",
       "      <td id=\"T_a78ed_row12_col6\" class=\"data row12 col6\" >0.1910</td>\n",
       "      <td id=\"T_a78ed_row12_col7\" class=\"data row12 col7\" >0.2016</td>\n",
       "      <td id=\"T_a78ed_row12_col8\" class=\"data row12 col8\" >0.0500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a78ed_level0_row13\" class=\"row_heading level0 row13\" >lightgbm</th>\n",
       "      <td id=\"T_a78ed_row13_col0\" class=\"data row13 col0\" >Light Gradient Boosting Machine</td>\n",
       "      <td id=\"T_a78ed_row13_col1\" class=\"data row13 col1\" >0.5919</td>\n",
       "      <td id=\"T_a78ed_row13_col2\" class=\"data row13 col2\" >0.5000</td>\n",
       "      <td id=\"T_a78ed_row13_col3\" class=\"data row13 col3\" >0.0000</td>\n",
       "      <td id=\"T_a78ed_row13_col4\" class=\"data row13 col4\" >0.0000</td>\n",
       "      <td id=\"T_a78ed_row13_col5\" class=\"data row13 col5\" >0.0000</td>\n",
       "      <td id=\"T_a78ed_row13_col6\" class=\"data row13 col6\" >0.0000</td>\n",
       "      <td id=\"T_a78ed_row13_col7\" class=\"data row13 col7\" >0.0000</td>\n",
       "      <td id=\"T_a78ed_row13_col8\" class=\"data row13 col8\" >0.0300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a78ed_level0_row14\" class=\"row_heading level0 row14\" >dummy</th>\n",
       "      <td id=\"T_a78ed_row14_col0\" class=\"data row14 col0\" >Dummy Classifier</td>\n",
       "      <td id=\"T_a78ed_row14_col1\" class=\"data row14 col1\" >0.5919</td>\n",
       "      <td id=\"T_a78ed_row14_col2\" class=\"data row14 col2\" >0.5000</td>\n",
       "      <td id=\"T_a78ed_row14_col3\" class=\"data row14 col3\" >0.0000</td>\n",
       "      <td id=\"T_a78ed_row14_col4\" class=\"data row14 col4\" >0.0000</td>\n",
       "      <td id=\"T_a78ed_row14_col5\" class=\"data row14 col5\" >0.0000</td>\n",
       "      <td id=\"T_a78ed_row14_col6\" class=\"data row14 col6\" >0.0000</td>\n",
       "      <td id=\"T_a78ed_row14_col7\" class=\"data row14 col7\" >0.0000</td>\n",
       "      <td id=\"T_a78ed_row14_col8\" class=\"data row14 col8\" >0.0233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a78ed_level0_row15\" class=\"row_heading level0 row15\" >qda</th>\n",
       "      <td id=\"T_a78ed_row15_col0\" class=\"data row15 col0\" >Quadratic Discriminant Analysis</td>\n",
       "      <td id=\"T_a78ed_row15_col1\" class=\"data row15 col1\" >0.5502</td>\n",
       "      <td id=\"T_a78ed_row15_col2\" class=\"data row15 col2\" >0.5177</td>\n",
       "      <td id=\"T_a78ed_row15_col3\" class=\"data row15 col3\" >0.0000</td>\n",
       "      <td id=\"T_a78ed_row15_col4\" class=\"data row15 col4\" >0.0000</td>\n",
       "      <td id=\"T_a78ed_row15_col5\" class=\"data row15 col5\" >0.0000</td>\n",
       "      <td id=\"T_a78ed_row15_col6\" class=\"data row15 col6\" >-0.0809</td>\n",
       "      <td id=\"T_a78ed_row15_col7\" class=\"data row15 col7\" >-0.1426</td>\n",
       "      <td id=\"T_a78ed_row15_col8\" class=\"data row15 col8\" >0.0233</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7f3962e02fa0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "best = exp.compare_models(fold=3,probability_threshold=0.5, n_select=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "0057112d-ed11-4239-b1bb-5a6db8a53647",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = fc.copy()[features]\n",
    "y = events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "a02e840c-6feb-483c-a5bc-2d4807362ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_discovery, X_val, y_discovery, y_val = train_test_split(X, y, test_size=0.6, random_state=SEED, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "ecaf0b57-600c-4085-aa58-0c5b02f4a8b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.011840105056762695,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 5000,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c8d398df59141949fbadc0c336aa050",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_39636/2881230760.py\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# here, I only take the \"train\" chunk of discovery data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mfeature_selector\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBorutaShap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimportance_measure\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'shap'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclassification\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m feature_selector.fit(X=X_discovery, y=y_discovery, n_trials=5000, random_state=SEED, train_or_test='train',\n\u001b[0m\u001b[1;32m      4\u001b[0m                         \u001b[0mnormalize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                      verbose=True)\n",
      "\u001b[0;32m~/venvs/default/lib/python3.9/site-packages/BorutaShap.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, n_trials, random_state, sample, train_or_test, normalize, verbose, stratify)\u001b[0m\n\u001b[1;32m    361\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCheck_if_chose_train_or_test_and_train_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 363\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mX_feature_import\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mShadow_feature_import\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_importance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnormalize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnormalize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    364\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_importance_history\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m                 \u001b[0mhits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcalculate_hits\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venvs/default/lib/python3.9/site-packages/BorutaShap.py\u001b[0m in \u001b[0;36mfeature_importance\u001b[0;34m(self, normalize)\u001b[0m\n\u001b[1;32m    606\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimportance_measure\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'shap'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    607\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 608\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexplain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    609\u001b[0m             \u001b[0mvals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshap_values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    610\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venvs/default/lib/python3.9/site-packages/BorutaShap.py\u001b[0m in \u001b[0;36mexplain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    699\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    700\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 701\u001b[0;31m         \u001b[0mexplainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mshap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTreeExplainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_perturbation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"tree_path_dependent\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    702\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    703\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venvs/default/lib/python3.9/site-packages/shap/explainers/_tree.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, model, data, model_output, feature_perturbation, feature_names, approximate, **deprecated_options)\u001b[0m\n\u001b[1;32m    147\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_perturbation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeature_perturbation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpected_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 149\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTreeEnsemble\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_missing\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    150\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m         \u001b[0;31m#self.model_output = self.model.model_output # this allows the TreeEnsemble to translate model outputs types by how it loads the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venvs/default/lib/python3.9/site-packages/shap/explainers/_tree.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, model, data, data_missing, model_output)\u001b[0m\n\u001b[1;32m    634\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_dtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    635\u001b[0m             \u001b[0mscaling\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1.0\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimators_\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# output is average of trees\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 636\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrees\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mSingleTree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtree_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscaling\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscaling\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_missing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_missing\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimators_\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    637\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobjective\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobjective_name_map\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    638\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtree_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"raw_value\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venvs/default/lib/python3.9/site-packages/shap/explainers/_tree.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    634\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_dtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    635\u001b[0m             \u001b[0mscaling\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1.0\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimators_\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# output is average of trees\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 636\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrees\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mSingleTree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtree_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscaling\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscaling\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_missing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_missing\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimators_\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    637\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobjective\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobjective_name_map\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    638\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtree_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"raw_value\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venvs/default/lib/python3.9/site-packages/shap/explainers/_tree.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, tree, normalize, scaling, data, data_missing)\u001b[0m\n\u001b[1;32m   1146\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren_default\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren_left\u001b[0m \u001b[0;31m# missing values not supported in sklearn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1147\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1148\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mthresholds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mthreshold\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1149\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1150\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# here, I only take the \"train\" chunk of discovery data\n",
    "feature_selector = BorutaShap(importance_measure='shap', classification=False)\n",
    "feature_selector.fit(X=X_discovery, y=y_discovery, n_trials=5000, random_state=SEED, train_or_test='train',\n",
    "                        normalize=True,\n",
    "                     verbose=True)\n",
    "selected_features = [i for i in X.columns if i not in feature_selector.features_to_remove]\n",
    "print(selected_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b1ef2103-71cf-4be4-937b-b6766f8f3499",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['CD4+ NV maturity and HLA-DR+ expression', 'CD4+ NV maturity and PD-1+TIGIT+ expression', 'CD4+ NV maturity and PD1+ expression', 'CD4+ None maturity and None expression', 'CD4+ CM maturity and CD226+ expression', 'CD8+ EM maturity and None expression', 'CD4+ CM maturity and PD-1+TIGIT- expression', 'CD4+ NV maturity and PD-1+TIGIT- expression', 'CD4+ CM maturity and None expression', 'CD4+ NV maturity and TIGIT+ expression', 'Th22 CM maturity and None expression', 'CD8+ EMTM maturity and PD-1+TIGIT+ expression']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "bead6e23-7d71-486d-927f-cc8bf5ed3e6d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:45:42,190]\u001b[0m A new study created in memory with name: no-name-5a47b83f-de33-44fa-b814-5134d0c78480\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:45:42,328]\u001b[0m Trial 0 finished with value: 0.3333333333333333 and parameters: {'booster': 'gbtree', 'learning_rate': 0.10599034612277504, 'n_estimators': 83, 'max_depth': 9, 'min_child_weight': 7.168459369662799, 'subsample': 0.6341211028370475, 'colsample_bytree': 0.9030668756589313, 'colsample_bylevel': 0.5396690060958131, 'gamma': 0.10213732455880647, 'reg_alpha': 0.49475651286937883, 'reg_lambda': 0.9438949666527545}. Best is trial 0 with value: 0.3333333333333333.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:45:42,480]\u001b[0m Trial 1 finished with value: 0.48571428571428577 and parameters: {'booster': 'dart', 'learning_rate': 0.09915004437995333, 'n_estimators': 84, 'max_depth': 7, 'min_child_weight': 1.7114749944945107, 'subsample': 0.5591908730305758, 'colsample_bytree': 0.6550103663444389, 'colsample_bylevel': 0.9503171628180864, 'gamma': 0.30146507648165566, 'reg_alpha': 0.7860353588368614, 'reg_lambda': 0.1976105203393682}. Best is trial 1 with value: 0.48571428571428577.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:45:42,610]\u001b[0m Trial 2 finished with value: 0.3333333333333333 and parameters: {'booster': 'gbtree', 'learning_rate': 0.09178669578386175, 'n_estimators': 86, 'max_depth': 6, 'min_child_weight': 5.918643266564003, 'subsample': 0.6458836645822437, 'colsample_bytree': 0.6441365637835257, 'colsample_bylevel': 0.9398741945308364, 'gamma': 0.19531702605494367, 'reg_alpha': 0.8604729080244621, 'reg_lambda': 0.8637141141846993}. Best is trial 1 with value: 0.48571428571428577.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:45:42,741]\u001b[0m Trial 3 finished with value: 0.3333333333333333 and parameters: {'booster': 'gblinear', 'learning_rate': 0.11568084781042988, 'n_estimators': 88, 'max_depth': 10, 'min_child_weight': 3.1549682999700623, 'subsample': 0.9406667186829119, 'colsample_bytree': 0.9540380089237693, 'colsample_bylevel': 0.5210014262986664, 'gamma': 0.05248380665613139, 'reg_alpha': 0.9088451912338373, 'reg_lambda': 0.3400088709260588}. Best is trial 1 with value: 0.48571428571428577.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:45:42] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:45:42] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:45:42,870]\u001b[0m Trial 4 finished with value: 0.8285714285714286 and parameters: {'booster': 'gblinear', 'learning_rate': 0.0925409036020692, 'n_estimators': 87, 'max_depth': 9, 'min_child_weight': 5.465428413915224, 'subsample': 0.7343348456254776, 'colsample_bytree': 0.8673421418156695, 'colsample_bylevel': 0.7525178421285326, 'gamma': 0.10573044726000236, 'reg_alpha': 0.1467416004135248, 'reg_lambda': 0.9059645133854193}. Best is trial 4 with value: 0.8285714285714286.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:45:43,018]\u001b[0m Trial 5 finished with value: 0.3333333333333333 and parameters: {'booster': 'dart', 'learning_rate': 0.17705053693866626, 'n_estimators': 83, 'max_depth': 9, 'min_child_weight': 3.148589982514505, 'subsample': 0.5543972256743992, 'colsample_bytree': 0.6985720828122157, 'colsample_bylevel': 0.7070279430501365, 'gamma': 0.06673985898251344, 'reg_alpha': 0.0544840944245395, 'reg_lambda': 0.09970010904451354}. Best is trial 4 with value: 0.8285714285714286.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:45:43,174]\u001b[0m Trial 6 finished with value: 0.3333333333333333 and parameters: {'booster': 'dart', 'learning_rate': 0.025125314377506917, 'n_estimators': 88, 'max_depth': 7, 'min_child_weight': 4.7014006435817475, 'subsample': 0.6965570295776176, 'colsample_bytree': 0.8563322493946857, 'colsample_bylevel': 0.8263191991425647, 'gamma': 0.48815540187245454, 'reg_alpha': 0.1816467704882203, 'reg_lambda': 0.8506566402624274}. Best is trial 4 with value: 0.8285714285714286.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:45:43,297]\u001b[0m Trial 7 finished with value: 0.3333333333333333 and parameters: {'booster': 'gblinear', 'learning_rate': 0.1946493750995769, 'n_estimators': 82, 'max_depth': 3, 'min_child_weight': 4.019161053939826, 'subsample': 0.7701054325850178, 'colsample_bytree': 0.779916609714141, 'colsample_bylevel': 0.8625717347887447, 'gamma': 0.3051677411325026, 'reg_alpha': 0.0008551848982100152, 'reg_lambda': 0.87427962506233}. Best is trial 4 with value: 0.8285714285714286.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:45:43] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:45:43,443]\u001b[0m Trial 8 finished with value: 0.3333333333333333 and parameters: {'booster': 'gbtree', 'learning_rate': 0.14968954313704633, 'n_estimators': 97, 'max_depth': 9, 'min_child_weight': 3.0956377096247953, 'subsample': 0.6038273074310128, 'colsample_bytree': 0.6679637626168919, 'colsample_bylevel': 0.5058054544679973, 'gamma': 0.21106961936644614, 'reg_alpha': 0.9694122262712189, 'reg_lambda': 0.5079099784837778}. Best is trial 4 with value: 0.8285714285714286.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:45:43,589]\u001b[0m Trial 9 finished with value: 0.3333333333333333 and parameters: {'booster': 'dart', 'learning_rate': 0.08940380520396841, 'n_estimators': 82, 'max_depth': 7, 'min_child_weight': 8.078317562779734, 'subsample': 0.5935579543408098, 'colsample_bytree': 0.9655371765055923, 'colsample_bylevel': 0.6506459676784058, 'gamma': 0.02445228493096052, 'reg_alpha': 0.3131122848785125, 'reg_lambda': 0.39673833272059766}. Best is trial 4 with value: 0.8285714285714286.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:45:43,747]\u001b[0m Trial 10 finished with value: 0.3333333333333333 and parameters: {'booster': 'gblinear', 'learning_rate': 0.03517109071965723, 'n_estimators': 94, 'max_depth': 5, 'min_child_weight': 9.435074554454165, 'subsample': 0.8258057783797675, 'colsample_bytree': 0.5107088512870475, 'colsample_bylevel': 0.7743106773815338, 'gamma': 0.43905890872246434, 'reg_alpha': 0.40869247446360923, 'reg_lambda': 0.675854766735535}. Best is trial 4 with value: 0.8285714285714286.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:45:43] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:45:43,953]\u001b[0m Trial 11 finished with value: 0.6249999999999999 and parameters: {'booster': 'dart', 'learning_rate': 0.0644080660919534, 'n_estimators': 93, 'max_depth': 8, 'min_child_weight': 1.2588024146268801, 'subsample': 0.8589834570304338, 'colsample_bytree': 0.5521574714403881, 'colsample_bylevel': 0.9935576672215923, 'gamma': 0.32625499525578544, 'reg_alpha': 0.719912200146526, 'reg_lambda': 0.04397149062619149}. Best is trial 4 with value: 0.8285714285714286.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:45:44,109]\u001b[0m Trial 12 finished with value: 0.3333333333333333 and parameters: {'booster': 'gblinear', 'learning_rate': 0.055926634254106, 'n_estimators': 92, 'max_depth': 8, 'min_child_weight': 1.0431794056615784, 'subsample': 0.8648504266207544, 'colsample_bytree': 0.513591631831147, 'colsample_bylevel': 0.9964574646911564, 'gamma': 0.37452100298210644, 'reg_alpha': 0.731395228230727, 'reg_lambda': 0.012933965007984573}. Best is trial 4 with value: 0.8285714285714286.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:45:43] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:45:44] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:45:44,277]\u001b[0m Trial 13 finished with value: 0.3333333333333333 and parameters: {'booster': 'gblinear', 'learning_rate': 0.06501199864037038, 'n_estimators': 100, 'max_depth': 10, 'min_child_weight': 6.36043568273189, 'subsample': 0.9993633259402186, 'colsample_bytree': 0.8214990500114465, 'colsample_bylevel': 0.6362534195798868, 'gamma': 0.1587974461670945, 'reg_alpha': 0.6494058669332065, 'reg_lambda': 0.6445443975781155}. Best is trial 4 with value: 0.8285714285714286.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:45:44,461]\u001b[0m Trial 14 finished with value: 0.3333333333333333 and parameters: {'booster': 'dart', 'learning_rate': 0.13156489869926205, 'n_estimators': 91, 'max_depth': 8, 'min_child_weight': 9.10267274589954, 'subsample': 0.7481299928411725, 'colsample_bytree': 0.5869553075744639, 'colsample_bylevel': 0.8774233005074374, 'gamma': 0.30319180871265283, 'reg_alpha': 0.608900120167606, 'reg_lambda': 0.6380911203797123}. Best is trial 4 with value: 0.8285714285714286.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:45:44,624]\u001b[0m Trial 15 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.0715559596102712, 'n_estimators': 94, 'max_depth': 5, 'min_child_weight': 4.891009166379371, 'subsample': 0.8750588706212519, 'colsample_bytree': 0.7386371120688078, 'colsample_bylevel': 0.7492967578298728, 'gamma': 0.12511841166144674, 'reg_alpha': 0.24493238214913976, 'reg_lambda': 0.290664497662243}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:45:44] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:45:44] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:45:44,787]\u001b[0m Trial 16 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.07451806308030683, 'n_estimators': 95, 'max_depth': 4, 'min_child_weight': 5.01628392786569, 'subsample': 0.7901259965602759, 'colsample_bytree': 0.7493875349360645, 'colsample_bylevel': 0.7315639373773795, 'gamma': 0.133575582035899, 'reg_alpha': 0.21138969504238106, 'reg_lambda': 0.2805227516878779}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:45:44,956]\u001b[0m Trial 17 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.043165590328515274, 'n_estimators': 96, 'max_depth': 4, 'min_child_weight': 4.863254946249084, 'subsample': 0.9261911079812642, 'colsample_bytree': 0.7400890538080412, 'colsample_bylevel': 0.6827655870738949, 'gamma': 0.139551775943927, 'reg_alpha': 0.2913478334089724, 'reg_lambda': 0.2470234203545651}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:45:44] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:45:44] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:45:45,122]\u001b[0m Trial 18 finished with value: 0.6249999999999999 and parameters: {'booster': 'gblinear', 'learning_rate': 0.03891511663134297, 'n_estimators': 98, 'max_depth': 5, 'min_child_weight': 6.7519522829075385, 'subsample': 0.9262063769933457, 'colsample_bytree': 0.7339756188111427, 'colsample_bylevel': 0.6189574747859063, 'gamma': 0.2160714488308301, 'reg_alpha': 0.34717376368896175, 'reg_lambda': 0.20768760061769015}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:45:45,281]\u001b[0m Trial 19 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.01263161829183243, 'n_estimators': 95, 'max_depth': 3, 'min_child_weight': 7.760533973678211, 'subsample': 0.7954081317214045, 'colsample_bytree': 0.7929607324082568, 'colsample_bylevel': 0.8139511216305984, 'gamma': 0.24877483194953587, 'reg_alpha': 0.183951950639541, 'reg_lambda': 0.4001927142000406}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:45:45] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:45:45] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:45:45,443]\u001b[0m Trial 20 finished with value: 0.3333333333333333 and parameters: {'booster': 'gblinear', 'learning_rate': 0.015715584908197387, 'n_estimators': 97, 'max_depth': 3, 'min_child_weight': 7.950736651981039, 'subsample': 0.9844894625165331, 'colsample_bytree': 0.8055140450393541, 'colsample_bylevel': 0.8084572980151433, 'gamma': 0.25168467826693175, 'reg_alpha': 0.5053765369386573, 'reg_lambda': 0.4932127010982528}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:45:45,604]\u001b[0m Trial 21 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.07142916927700757, 'n_estimators': 95, 'max_depth': 4, 'min_child_weight': 4.932370352773759, 'subsample': 0.8952993184279607, 'colsample_bytree': 0.7315524389268422, 'colsample_bylevel': 0.7003709194285533, 'gamma': 0.12708777695506196, 'reg_alpha': 0.25399595198345015, 'reg_lambda': 0.2569391727263047}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:45:45] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:45:45] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:45:45,770]\u001b[0m Trial 22 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.010208657499369627, 'n_estimators': 100, 'max_depth': 4, 'min_child_weight': 3.8564954103447833, 'subsample': 0.920763930194525, 'colsample_bytree': 0.7003487319227282, 'colsample_bylevel': 0.6810286836868747, 'gamma': 0.1794027087013334, 'reg_alpha': 0.09086521861955635, 'reg_lambda': 0.4389945860985387}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:45:45,935]\u001b[0m Trial 23 finished with value: 0.6249999999999999 and parameters: {'booster': 'gblinear', 'learning_rate': 0.0449224146682717, 'n_estimators': 100, 'max_depth': 4, 'min_child_weight': 3.9757804546188193, 'subsample': 0.9482349353960547, 'colsample_bytree': 0.6973605851962804, 'colsample_bylevel': 0.5814826153494996, 'gamma': 0.17752973752162088, 'reg_alpha': 0.05932536261429011, 'reg_lambda': 0.43954589618015427}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:45:45] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:45:45] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:45:46,097]\u001b[0m Trial 24 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.04974391882425853, 'n_estimators': 96, 'max_depth': 3, 'min_child_weight': 7.593573908429926, 'subsample': 0.8226341159067915, 'colsample_bytree': 0.7759204513603253, 'colsample_bylevel': 0.7010371977404779, 'gamma': 0.2391823522140924, 'reg_alpha': 0.2902705554927561, 'reg_lambda': 0.19253768953463582}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:45:46,265]\u001b[0m Trial 25 finished with value: 0.3333333333333333 and parameters: {'booster': 'gbtree', 'learning_rate': 0.02648932741787645, 'n_estimators': 99, 'max_depth': 4, 'min_child_weight': 4.103140677536672, 'subsample': 0.8962028558502638, 'colsample_bytree': 0.603768216610026, 'colsample_bylevel': 0.6801144535165322, 'gamma': 0.24510583152097698, 'reg_alpha': 0.41820694267127156, 'reg_lambda': 0.15733642837965275}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:45:46,419]\u001b[0m Trial 26 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.01153604462323366, 'n_estimators': 90, 'max_depth': 3, 'min_child_weight': 2.058361547740415, 'subsample': 0.8206936225247717, 'colsample_bytree': 0.7034808107291575, 'colsample_bylevel': 0.7986107876129609, 'gamma': 0.0030659726333758797, 'reg_alpha': 0.1253293193021589, 'reg_lambda': 0.5417411886694503}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:45:46] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:45:46] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:45:46,572]\u001b[0m Trial 27 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.010290819537072605, 'n_estimators': 90, 'max_depth': 3, 'min_child_weight': 2.6026834266841066, 'subsample': 0.8203296233927587, 'colsample_bytree': 0.6158724036366257, 'colsample_bylevel': 0.5886578355587175, 'gamma': 0.3677363183280282, 'reg_alpha': 0.11354426477241286, 'reg_lambda': 0.5677109176143518}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:45:46,725]\u001b[0m Trial 28 finished with value: 0.8285714285714286 and parameters: {'booster': 'gblinear', 'learning_rate': 0.02830330574820117, 'n_estimators': 90, 'max_depth': 3, 'min_child_weight': 2.550777104562707, 'subsample': 0.7088763046004729, 'colsample_bytree': 0.6200733755959453, 'colsample_bylevel': 0.5999762081527741, 'gamma': 0.3779448648244212, 'reg_alpha': 0.10332341650863897, 'reg_lambda': 0.7467207349165639}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:45:46] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:45:46,883]\u001b[0m Trial 29 finished with value: 0.3333333333333333 and parameters: {'booster': 'gbtree', 'learning_rate': 0.11565572674531886, 'n_estimators': 91, 'max_depth': 6, 'min_child_weight': 5.710386704010033, 'subsample': 0.9617753806441641, 'colsample_bytree': 0.8459928169291228, 'colsample_bylevel': 0.5605286942299467, 'gamma': 0.08532650166384925, 'reg_alpha': 0.40921075755550207, 'reg_lambda': 0.2950611565468265}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:45:47,042]\u001b[0m Trial 30 finished with value: 0.3333333333333333 and parameters: {'booster': 'gblinear', 'learning_rate': 0.13852156017770037, 'n_estimators': 93, 'max_depth': 5, 'min_child_weight': 8.522561018707016, 'subsample': 0.5006153701003039, 'colsample_bytree': 0.9996723006718554, 'colsample_bylevel': 0.8639255429604404, 'gamma': 0.16962057038260392, 'reg_alpha': 0.0015512733682070956, 'reg_lambda': 0.4034373446369016}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:45:46] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:45:47] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:45:47,207]\u001b[0m Trial 31 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.07210193667699896, 'n_estimators': 95, 'max_depth': 5, 'min_child_weight': 6.916946785329333, 'subsample': 0.8808132899053626, 'colsample_bytree': 0.7845106818955242, 'colsample_bylevel': 0.7410026747576279, 'gamma': 0.12243089630238913, 'reg_alpha': 0.22104091607684057, 'reg_lambda': 0.3684148428053041}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:45:47,361]\u001b[0m Trial 32 finished with value: 0.8285714285714286 and parameters: {'booster': 'gblinear', 'learning_rate': 0.05808384493181326, 'n_estimators': 90, 'max_depth': 3, 'min_child_weight': 1.884619974837244, 'subsample': 0.8269853206215733, 'colsample_bytree': 0.677328653080534, 'colsample_bylevel': 0.7917862029957176, 'gamma': 0.012380736119267086, 'reg_alpha': 0.1484604274845635, 'reg_lambda': 0.5467741618798816}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:45:47] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:45:47] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:45:47,514]\u001b[0m Trial 33 finished with value: 0.3333333333333333 and parameters: {'booster': 'gblinear', 'learning_rate': 0.04836419855343953, 'n_estimators': 89, 'max_depth': 3, 'min_child_weight': 2.001438761201385, 'subsample': 0.8022221644837955, 'colsample_bytree': 0.6415266237192562, 'colsample_bylevel': 0.736282096423291, 'gamma': 0.3651470385719179, 'reg_alpha': 0.3508006367136667, 'reg_lambda': 0.5671994606631159}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:45:47,683]\u001b[0m Trial 34 finished with value: 0.3333333333333333 and parameters: {'booster': 'gbtree', 'learning_rate': 0.07694311171519612, 'n_estimators': 95, 'max_depth': 4, 'min_child_weight': 5.160625205789332, 'subsample': 0.9004014727009513, 'colsample_bytree': 0.7499124296922379, 'colsample_bylevel': 0.7161401157222639, 'gamma': 0.13448550865400027, 'reg_alpha': 0.22551057155168613, 'reg_lambda': 0.28243981148348524}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:45:47,846]\u001b[0m Trial 35 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.07608112386405867, 'n_estimators': 97, 'max_depth': 6, 'min_child_weight': 4.803601488101306, 'subsample': 0.8539299425079362, 'colsample_bytree': 0.7366687219734189, 'colsample_bylevel': 0.6605714713934658, 'gamma': 0.13967540362082254, 'reg_alpha': 0.271747216378777, 'reg_lambda': 0.2639760522952424}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:45:47] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:45:47] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:45:47,994]\u001b[0m Trial 36 finished with value: 0.8285714285714286 and parameters: {'booster': 'gblinear', 'learning_rate': 0.03521881745531536, 'n_estimators': 85, 'max_depth': 5, 'min_child_weight': 2.418461443419294, 'subsample': 0.8412678921589316, 'colsample_bytree': 0.5733700892568271, 'colsample_bylevel': 0.5623499636226025, 'gamma': 0.04651501095578532, 'reg_alpha': 0.12510063454759998, 'reg_lambda': 0.7470619428936492}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:45:48,144]\u001b[0m Trial 37 finished with value: 0.3333333333333333 and parameters: {'booster': 'gblinear', 'learning_rate': 0.02129173812115765, 'n_estimators': 87, 'max_depth': 3, 'min_child_weight': 7.390707452194801, 'subsample': 0.6802141454313126, 'colsample_bytree': 0.90209101964109, 'colsample_bylevel': 0.7749407946976009, 'gamma': 0.09286899367681545, 'reg_alpha': 0.4726311927922776, 'reg_lambda': 0.14774061724258794}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:45:48] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:45:48] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:45:48,309]\u001b[0m Trial 38 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.08386568751715953, 'n_estimators': 98, 'max_depth': 6, 'min_child_weight': 6.18780217305993, 'subsample': 0.7689957437941368, 'colsample_bytree': 0.7785537710472834, 'colsample_bylevel': 0.660983043786398, 'gamma': 0.2578596468830316, 'reg_alpha': 0.2774533550423317, 'reg_lambda': 0.20789128057034884}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:45:48,468]\u001b[0m Trial 39 finished with value: 0.3333333333333333 and parameters: {'booster': 'gbtree', 'learning_rate': 0.10491258983082873, 'n_estimators': 92, 'max_depth': 6, 'min_child_weight': 6.810316700556456, 'subsample': 0.7248842357378663, 'colsample_bytree': 0.8148617123769339, 'colsample_bylevel': 0.8336233973911478, 'gamma': 0.2716051737222982, 'reg_alpha': 0.17894422741972732, 'reg_lambda': 0.3296527899352955}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:45:48,632]\u001b[0m Trial 40 finished with value: 0.3333333333333333 and parameters: {'booster': 'dart', 'learning_rate': 0.01484424116765402, 'n_estimators': 80, 'max_depth': 4, 'min_child_weight': 3.0325048923688334, 'subsample': 0.9092465520151929, 'colsample_bytree': 0.7075947521417905, 'colsample_bylevel': 0.842954881147178, 'gamma': 0.21280510420026524, 'reg_alpha': 0.06908740683521258, 'reg_lambda': 0.4580602195112237}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:45:48,783]\u001b[0m Trial 41 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.0324589981197086, 'n_estimators': 89, 'max_depth': 4, 'min_child_weight': 4.395985197254285, 'subsample': 0.7871246956508383, 'colsample_bytree': 0.6513867837584969, 'colsample_bylevel': 0.7565335650431313, 'gamma': 0.07027310353517148, 'reg_alpha': 0.2068905529925269, 'reg_lambda': 0.5615054041021584}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:45:48] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:45:48] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:45:48,949]\u001b[0m Trial 42 finished with value: 0.6249999999999999 and parameters: {'booster': 'gblinear', 'learning_rate': 0.02972552321537536, 'n_estimators': 99, 'max_depth': 4, 'min_child_weight': 4.2323149270263, 'subsample': 0.7825059848698014, 'colsample_bytree': 0.7718836654502763, 'colsample_bylevel': 0.6882857289563455, 'gamma': 0.20782635964150203, 'reg_alpha': 0.34017689264919526, 'reg_lambda': 0.3497316505462498}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:45:49,108]\u001b[0m Trial 43 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.055618471369450403, 'n_estimators': 94, 'max_depth': 5, 'min_child_weight': 3.5988786266112105, 'subsample': 0.880167400517521, 'colsample_bytree': 0.6658416903139852, 'colsample_bylevel': 0.7655930004563388, 'gamma': 0.06413407692797463, 'reg_alpha': 0.2435762341665589, 'reg_lambda': 0.446498908535495}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:45:48] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:45:49] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:45:49,270]\u001b[0m Trial 44 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.08841138063034684, 'n_estimators': 97, 'max_depth': 6, 'min_child_weight': 6.07376192692249, 'subsample': 0.8497348486241707, 'colsample_bytree': 0.8347997043724139, 'colsample_bylevel': 0.6538929139593141, 'gamma': 0.10870193933830827, 'reg_alpha': 0.27604807989124075, 'reg_lambda': 0.0782505838873872}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:45:49,435]\u001b[0m Trial 45 finished with value: 0.8285714285714286 and parameters: {'booster': 'gblinear', 'learning_rate': 0.08180165990495875, 'n_estimators': 98, 'max_depth': 6, 'min_child_weight': 5.585163574929844, 'subsample': 0.6847851513389531, 'colsample_bytree': 0.8845100366969101, 'colsample_bylevel': 0.6511870803954012, 'gamma': 0.1789760502778648, 'reg_alpha': 0.18611007267786234, 'reg_lambda': 0.3365114777976095}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:45:49] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:45:49,633]\u001b[0m Trial 46 finished with value: 0.3333333333333333 and parameters: {'booster': 'dart', 'learning_rate': 0.09643128741709855, 'n_estimators': 96, 'max_depth': 5, 'min_child_weight': 3.546611245722282, 'subsample': 0.9605367068499555, 'colsample_bytree': 0.8337265863495724, 'colsample_bylevel': 0.8930335730127086, 'gamma': 0.11371473433684386, 'reg_alpha': 0.0695746787858243, 'reg_lambda': 0.08664946807632745}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:45:49,794]\u001b[0m Trial 47 finished with value: 0.3333333333333333 and parameters: {'booster': 'gblinear', 'learning_rate': 0.07011909531875113, 'n_estimators': 94, 'max_depth': 7, 'min_child_weight': 4.653294751455447, 'subsample': 0.8739123998227537, 'colsample_bytree': 0.7455453218295068, 'colsample_bylevel': 0.7374181951462208, 'gamma': 0.1337306731685409, 'reg_alpha': 0.5359781288133829, 'reg_lambda': 0.25211237205394754}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:45:49] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:45:49] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:45:49,947]\u001b[0m Trial 48 finished with value: 0.6249999999999999 and parameters: {'booster': 'gblinear', 'learning_rate': 0.01231065102378871, 'n_estimators': 87, 'max_depth': 3, 'min_child_weight': 2.7277459207602934, 'subsample': 0.8179034839707289, 'colsample_bytree': 0.7105580368852716, 'colsample_bylevel': 0.6257510715044342, 'gamma': 0.4598645863289271, 'reg_alpha': 0.010137460118913644, 'reg_lambda': 0.6265450129373222}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:45:50,113]\u001b[0m Trial 49 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.04567413355547702, 'n_estimators': 96, 'max_depth': 4, 'min_child_weight': 7.420924722920192, 'subsample': 0.8044367314569253, 'colsample_bytree': 0.7914837116440988, 'colsample_bylevel': 0.7186775476306717, 'gamma': 0.23665144936409338, 'reg_alpha': 0.32089907023564823, 'reg_lambda': 0.15696111069911073}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:45:49] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:45:50,312]\u001b[0m Trial 50 finished with value: 0.3333333333333333 and parameters: {'booster': 'dart', 'learning_rate': 0.054760976265812666, 'n_estimators': 98, 'max_depth': 6, 'min_child_weight': 6.243352901838033, 'subsample': 0.7649356750179871, 'colsample_bytree': 0.6479629790059109, 'colsample_bylevel': 0.7697180271004739, 'gamma': 0.05503409525331844, 'reg_alpha': 0.3724701433833942, 'reg_lambda': 0.08646559097285353}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:45:50,474]\u001b[0m Trial 51 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.019567374839156066, 'n_estimators': 95, 'max_depth': 3, 'min_child_weight': 5.216934167208927, 'subsample': 0.7489304863151339, 'colsample_bytree': 0.7623572469249694, 'colsample_bylevel': 0.5243142577851587, 'gamma': 0.27288046674367394, 'reg_alpha': 0.1715437096647076, 'reg_lambda': 0.41992825541045625}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:45:50] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:45:50] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:45:50,634]\u001b[0m Trial 52 finished with value: 0.8285714285714286 and parameters: {'booster': 'gblinear', 'learning_rate': 0.022011889439602687, 'n_estimators': 92, 'max_depth': 3, 'min_child_weight': 8.407999477323731, 'subsample': 0.8387603331521553, 'colsample_bytree': 0.768330653096556, 'colsample_bylevel': 0.5176079072343794, 'gamma': 0.3469148418179565, 'reg_alpha': 0.1472717537779564, 'reg_lambda': 0.50222917860134}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:45:50,794]\u001b[0m Trial 53 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.040960399482312414, 'n_estimators': 93, 'max_depth': 4, 'min_child_weight': 3.4396217173190973, 'subsample': 0.9197176689556149, 'colsample_bytree': 0.6871817572321474, 'colsample_bylevel': 0.6754814238370799, 'gamma': 0.15874714202005627, 'reg_alpha': 0.3082726983513198, 'reg_lambda': 0.20992547123608263}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:45:50] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:45:50] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:45:50,947]\u001b[0m Trial 54 finished with value: 0.6249999999999999 and parameters: {'booster': 'gblinear', 'learning_rate': 0.16859775241414093, 'n_estimators': 88, 'max_depth': 5, 'min_child_weight': 4.412593862422781, 'subsample': 0.7462537950845362, 'colsample_bytree': 0.6615065265993414, 'colsample_bylevel': 0.7933071637879253, 'gamma': 0.0730462475451779, 'reg_alpha': 0.17351121082879573, 'reg_lambda': 0.013119933749047241}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:45:51,107]\u001b[0m Trial 55 finished with value: 0.8285714285714286 and parameters: {'booster': 'gblinear', 'learning_rate': 0.03904393227637523, 'n_estimators': 93, 'max_depth': 4, 'min_child_weight': 2.9442581797906344, 'subsample': 0.9246799748738221, 'colsample_bytree': 0.6868576167990739, 'colsample_bylevel': 0.7027636266779521, 'gamma': 0.40867219186697146, 'reg_alpha': 0.09529078981819322, 'reg_lambda': 0.37321782255403657}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:45:50] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:45:51] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:45:51,264]\u001b[0m Trial 56 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.010733870746930479, 'n_estimators': 91, 'max_depth': 3, 'min_child_weight': 1.6181280155644988, 'subsample': 0.8156337382595037, 'colsample_bytree': 0.7217009389828168, 'colsample_bylevel': 0.8155277270007275, 'gamma': 0.2773861200965091, 'reg_alpha': 0.13189085505052642, 'reg_lambda': 0.6075092465548511}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:45:51,426]\u001b[0m Trial 57 finished with value: 0.8285714285714286 and parameters: {'booster': 'gbtree', 'learning_rate': 0.018112849384739296, 'n_estimators': 91, 'max_depth': 3, 'min_child_weight': 1.4980429430666387, 'subsample': 0.8032558551422256, 'colsample_bytree': 0.7234705546443985, 'colsample_bylevel': 0.9201171679214369, 'gamma': 0.001944205132277901, 'reg_alpha': 0.12033032444976283, 'reg_lambda': 0.6831250331587029}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:45:51,596]\u001b[0m Trial 58 finished with value: 0.3333333333333333 and parameters: {'booster': 'gblinear', 'learning_rate': 0.11463990095076416, 'n_estimators': 99, 'max_depth': 8, 'min_child_weight': 1.3183051598559903, 'subsample': 0.8570289425067221, 'colsample_bytree': 0.6266195861858744, 'colsample_bylevel': 0.6081686365754235, 'gamma': 0.3130744100902329, 'reg_alpha': 0.038025353491414296, 'reg_lambda': 0.4704208994958267}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:45:51] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:45:51] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:45:51,750]\u001b[0m Trial 59 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.06406979365172957, 'n_estimators': 89, 'max_depth': 5, 'min_child_weight': 3.4767644903601376, 'subsample': 0.8824119946930307, 'colsample_bytree': 0.5841003631248998, 'colsample_bylevel': 0.846397495861868, 'gamma': 0.03301753098489354, 'reg_alpha': 0.21391488899047642, 'reg_lambda': 0.5818801236747215}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:45:51,915]\u001b[0m Trial 60 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.08734933983014523, 'n_estimators': 97, 'max_depth': 5, 'min_child_weight': 3.5809987373681107, 'subsample': 0.8751774362681387, 'colsample_bytree': 0.5577613940737796, 'colsample_bylevel': 0.750231782176472, 'gamma': 0.0320735068675593, 'reg_alpha': 0.24526034817420542, 'reg_lambda': 0.6879906258974959}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:45:51] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:45:51] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:45:52,081]\u001b[0m Trial 61 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.09101376078675398, 'n_estimators': 97, 'max_depth': 5, 'min_child_weight': 6.958717749784215, 'subsample': 0.8838946510229755, 'colsample_bytree': 0.543274645156695, 'colsample_bylevel': 0.5898177570475777, 'gamma': 0.030910569273914418, 'reg_alpha': 0.2483982124563396, 'reg_lambda': 0.7254105394501345}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:45:52,246]\u001b[0m Trial 62 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.08617832526100555, 'n_estimators': 97, 'max_depth': 7, 'min_child_weight': 6.180772655143658, 'subsample': 0.8511415380282673, 'colsample_bytree': 0.532528666920523, 'colsample_bylevel': 0.6498998318907039, 'gamma': 0.03444945555723203, 'reg_alpha': 0.264577411883658, 'reg_lambda': 0.7844288264441479}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:45:52] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:45:52] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:45:52,415]\u001b[0m Trial 63 finished with value: 0.6249999999999999 and parameters: {'booster': 'gblinear', 'learning_rate': 0.10464276723537025, 'n_estimators': 100, 'max_depth': 3, 'min_child_weight': 7.8497076655554405, 'subsample': 0.8318426299699293, 'colsample_bytree': 0.5215948909313204, 'colsample_bylevel': 0.5932750375731113, 'gamma': 0.4997089946523574, 'reg_alpha': 0.09691961624844467, 'reg_lambda': 0.5329305194187914}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:45:52,570]\u001b[0m Trial 64 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.03306305153103914, 'n_estimators': 89, 'max_depth': 7, 'min_child_weight': 6.32571925518863, 'subsample': 0.7770660508282389, 'colsample_bytree': 0.6089185772255389, 'colsample_bylevel': 0.7241135566581278, 'gamma': 0.011022174492633178, 'reg_alpha': 0.28824865435138425, 'reg_lambda': 0.8133492080967445}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:45:52] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:45:52] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:45:52,738]\u001b[0m Trial 65 finished with value: 0.3333333333333333 and parameters: {'booster': 'gblinear', 'learning_rate': 0.05165817233665648, 'n_estimators': 99, 'max_depth': 7, 'min_child_weight': 6.486352084281986, 'subsample': 0.7713169461850427, 'colsample_bytree': 0.7264403987868453, 'colsample_bylevel': 0.6648367327854907, 'gamma': 0.22958087911934194, 'reg_alpha': 0.45104413076298705, 'reg_lambda': 0.9358312648108869}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:45:52,889]\u001b[0m Trial 66 finished with value: 0.3333333333333333 and parameters: {'booster': 'gblinear', 'learning_rate': 0.07908662489444755, 'n_estimators': 86, 'max_depth': 6, 'min_child_weight': 5.951850336824044, 'subsample': 0.7267913582321803, 'colsample_bytree': 0.7921626994406294, 'colsample_bylevel': 0.6563387780474125, 'gamma': 0.10419081595480356, 'reg_alpha': 0.3810795747708523, 'reg_lambda': 0.8258162260373991}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:45:52] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:45:53,085]\u001b[0m Trial 67 finished with value: 0.3333333333333333 and parameters: {'booster': 'dart', 'learning_rate': 0.062012682174366215, 'n_estimators': 96, 'max_depth': 4, 'min_child_weight': 5.29841248897001, 'subsample': 0.7553510982992407, 'colsample_bytree': 0.7674259688807191, 'colsample_bylevel': 0.7175249865512346, 'gamma': 0.2781267746770505, 'reg_alpha': 0.3272748512008442, 'reg_lambda': 0.12758029693863}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:45:53,249]\u001b[0m Trial 68 finished with value: 0.8285714285714286 and parameters: {'booster': 'gblinear', 'learning_rate': 0.025971225578169395, 'n_estimators': 95, 'max_depth': 4, 'min_child_weight': 9.073344753516821, 'subsample': 0.9428798803359403, 'colsample_bytree': 0.860354334692519, 'colsample_bylevel': 0.697889342488511, 'gamma': 0.15217852591959224, 'reg_alpha': 0.17695464957681545, 'reg_lambda': 0.2118975986629479}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:45:53] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:45:53,415]\u001b[0m Trial 69 finished with value: 0.3333333333333333 and parameters: {'booster': 'gbtree', 'learning_rate': 0.0719864141252208, 'n_estimators': 95, 'max_depth': 4, 'min_child_weight': 7.147347380362679, 'subsample': 0.9126287442165215, 'colsample_bytree': 0.8302548139119409, 'colsample_bylevel': 0.676052766707929, 'gamma': 0.18512437686629524, 'reg_alpha': 0.9484111706540422, 'reg_lambda': 0.18266390029312102}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:45:53,575]\u001b[0m Trial 70 finished with value: 0.6249999999999999 and parameters: {'booster': 'gblinear', 'learning_rate': 0.01320758039662395, 'n_estimators': 92, 'max_depth': 9, 'min_child_weight': 4.719068312467166, 'subsample': 0.9904428795554157, 'colsample_bytree': 0.7188930938089791, 'colsample_bylevel': 0.6199194226447281, 'gamma': 0.14382079188990887, 'reg_alpha': 0.026417378130284122, 'reg_lambda': 0.2387426163245447}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:45:53] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:45:53] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:45:53,732]\u001b[0m Trial 71 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.056267791761347136, 'n_estimators': 90, 'max_depth': 6, 'min_child_weight': 2.122838924069791, 'subsample': 0.8486907288500505, 'colsample_bytree': 0.6350269906971023, 'colsample_bylevel': 0.7569114483778938, 'gamma': 0.07490659415414792, 'reg_alpha': 0.21134361864787457, 'reg_lambda': 0.5174015980284674}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:45:53,895]\u001b[0m Trial 72 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.0668179510007058, 'n_estimators': 88, 'max_depth': 5, 'min_child_weight': 4.4302861570178536, 'subsample': 0.8212856568090434, 'colsample_bytree': 0.6661840121279766, 'colsample_bylevel': 0.7670259393902013, 'gamma': 0.05926981472731845, 'reg_alpha': 0.20308943872330112, 'reg_lambda': 0.573567702329921}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:45:53] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:45:53] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:45:54,055]\u001b[0m Trial 73 finished with value: 0.3333333333333333 and parameters: {'booster': 'gblinear', 'learning_rate': 0.06652453737478874, 'n_estimators': 90, 'max_depth': 6, 'min_child_weight': 2.2997619567249856, 'subsample': 0.7885380570785803, 'colsample_bytree': 0.6392264696422829, 'colsample_bylevel': 0.794561327736952, 'gamma': 0.08206911177660252, 'reg_alpha': 0.8059894491249322, 'reg_lambda': 0.5275750796214379}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:45:54,212]\u001b[0m Trial 74 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.010615310687460838, 'n_estimators': 91, 'max_depth': 3, 'min_child_weight': 1.524223367880718, 'subsample': 0.895508606754856, 'colsample_bytree': 0.5817191962914513, 'colsample_bylevel': 0.8544364938665967, 'gamma': 0.20157345773429575, 'reg_alpha': 0.14362800595690586, 'reg_lambda': 0.5879443074782822}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:45:54] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:45:54] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:45:54,368]\u001b[0m Trial 75 finished with value: 0.6249999999999999 and parameters: {'booster': 'gblinear', 'learning_rate': 0.09727166743112997, 'n_estimators': 89, 'max_depth': 5, 'min_child_weight': 3.9414180782221786, 'subsample': 0.8984559428924869, 'colsample_bytree': 0.5716307768521252, 'colsample_bylevel': 0.8499476046511176, 'gamma': 0.19160588172426432, 'reg_alpha': 0.08913305698331292, 'reg_lambda': 0.674619933703271}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:45:54,535]\u001b[0m Trial 76 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.08910876065710138, 'n_estimators': 96, 'max_depth': 5, 'min_child_weight': 3.7666273084791997, 'subsample': 0.8808902725768575, 'colsample_bytree': 0.5423349288551753, 'colsample_bylevel': 0.5440652900091135, 'gamma': 0.02975254074670861, 'reg_alpha': 0.2423254796791282, 'reg_lambda': 0.7168997888475891}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:45:54] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:45:54] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:45:54,700]\u001b[0m Trial 77 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.08211017446976493, 'n_estimators': 98, 'max_depth': 6, 'min_child_weight': 5.87864664977131, 'subsample': 0.8664329876375539, 'colsample_bytree': 0.8026504027998822, 'colsample_bylevel': 0.6347082000794468, 'gamma': 0.11421884628911369, 'reg_alpha': 0.28306161320101103, 'reg_lambda': 0.37058801342235015}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:45:54,866]\u001b[0m Trial 78 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.10910988067810568, 'n_estimators': 94, 'max_depth': 6, 'min_child_weight': 4.4303110210785155, 'subsample': 0.8639261854817842, 'colsample_bytree': 0.8778865783274225, 'colsample_bylevel': 0.5566056972833623, 'gamma': 0.10096548829154679, 'reg_alpha': 0.2619456103390354, 'reg_lambda': 0.04412460947537749}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:45:54] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:45:54] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:45:55,029]\u001b[0m Trial 79 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.13461017727709265, 'n_estimators': 93, 'max_depth': 8, 'min_child_weight': 7.1106733552535974, 'subsample': 0.8424367684355106, 'colsample_bytree': 0.525702812615293, 'colsample_bylevel': 0.5685257360700668, 'gamma': 0.012338297751526643, 'reg_alpha': 0.30412568301644016, 'reg_lambda': 0.8077555078595091}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:45:55,193]\u001b[0m Trial 80 finished with value: 0.6249999999999999 and parameters: {'booster': 'gblinear', 'learning_rate': 0.12433423639575764, 'n_estimators': 97, 'max_depth': 8, 'min_child_weight': 6.723086937952722, 'subsample': 0.8462605312123687, 'colsample_bytree': 0.5003441166366793, 'colsample_bylevel': 0.5774742435738053, 'gamma': 0.009520381600340529, 'reg_alpha': 0.36126070456437853, 'reg_lambda': 0.7921013992593702}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:45:55] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:45:55] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:45:55,358]\u001b[0m Trial 81 finished with value: 0.8285714285714286 and parameters: {'booster': 'gblinear', 'learning_rate': 0.03298822387356483, 'n_estimators': 97, 'max_depth': 7, 'min_child_weight': 7.521047019411874, 'subsample': 0.8186174718418472, 'colsample_bytree': 0.5974761223841125, 'colsample_bylevel': 0.8136312328161447, 'gamma': 0.3430497282600289, 'reg_alpha': 0.13095780817727748, 'reg_lambda': 0.6020223054129279}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:45:55,525]\u001b[0m Trial 82 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.09352697728897122, 'n_estimators': 98, 'max_depth': 7, 'min_child_weight': 1.0138616594995487, 'subsample': 0.7758386285535886, 'colsample_bytree': 0.5458314571131285, 'colsample_bylevel': 0.6383108927384483, 'gamma': 0.0397309175078016, 'reg_alpha': 0.23175328686772068, 'reg_lambda': 0.8959574463801676}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:45:55] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:45:55] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:45:55,690]\u001b[0m Trial 83 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.047579820627947994, 'n_estimators': 96, 'max_depth': 4, 'min_child_weight': 5.344148582896149, 'subsample': 0.7136996589168116, 'colsample_bytree': 0.7892425402575629, 'colsample_bylevel': 0.7408893467392849, 'gamma': 0.16534799183700266, 'reg_alpha': 0.27638938555635384, 'reg_lambda': 0.42317798649913696}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:45:55,855]\u001b[0m Trial 84 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.06003752827798118, 'n_estimators': 98, 'max_depth': 6, 'min_child_weight': 5.093595062117848, 'subsample': 0.802045561912451, 'colsample_bytree': 0.8098541043523122, 'colsample_bylevel': 0.6401066648867322, 'gamma': 0.12042412298972655, 'reg_alpha': 0.22829249296490867, 'reg_lambda': 0.3027219572261327}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:45:55] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:45:55] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:45:56,024]\u001b[0m Trial 85 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.02263153760500425, 'n_estimators': 100, 'max_depth': 3, 'min_child_weight': 3.283182977669544, 'subsample': 0.8854104889081056, 'colsample_bytree': 0.5615842163135117, 'colsample_bylevel': 0.593583484270817, 'gamma': 0.02483969296717612, 'reg_alpha': 0.15910609942405465, 'reg_lambda': 0.7403948516746945}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:45:56,196]\u001b[0m Trial 86 finished with value: 0.3333333333333333 and parameters: {'booster': 'gbtree', 'learning_rate': 0.020020038229716056, 'n_estimators': 100, 'max_depth': 3, 'min_child_weight': 2.723997971920828, 'subsample': 0.9371579745130334, 'colsample_bytree': 0.5355020054767933, 'colsample_bylevel': 0.5370535593723578, 'gamma': 0.03898034759032347, 'reg_alpha': 0.17164475755216907, 'reg_lambda': 0.7643899170607514}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:45:56,377]\u001b[0m Trial 87 finished with value: 0.3333333333333333 and parameters: {'booster': 'dart', 'learning_rate': 0.06898726020240407, 'n_estimators': 88, 'max_depth': 5, 'min_child_weight': 2.26694517327477, 'subsample': 0.83066666599186, 'colsample_bytree': 0.6722658024333691, 'colsample_bylevel': 0.8309137768534987, 'gamma': 0.05586890612363042, 'reg_alpha': 0.21504693534347671, 'reg_lambda': 0.48642729614036423}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:45:56,529]\u001b[0m Trial 88 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.0633257529567638, 'n_estimators': 87, 'max_depth': 5, 'min_child_weight': 2.9178620179170784, 'subsample': 0.822920067153766, 'colsample_bytree': 0.758209278875351, 'colsample_bylevel': 0.7704583206341182, 'gamma': 0.06034443429949182, 'reg_alpha': 0.19231227538903706, 'reg_lambda': 0.5635416671909852}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:45:56] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:45:56] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:45:56,691]\u001b[0m Trial 89 finished with value: 0.3333333333333333 and parameters: {'booster': 'gblinear', 'learning_rate': 0.042005600263546945, 'n_estimators': 94, 'max_depth': 4, 'min_child_weight': 5.594761663500445, 'subsample': 0.7464322292328112, 'colsample_bytree': 0.6785414029821389, 'colsample_bylevel': 0.687471541440699, 'gamma': 0.27929027976792375, 'reg_alpha': 0.4322420041632629, 'reg_lambda': 0.12728779078152086}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:45:56,847]\u001b[0m Trial 90 finished with value: 0.6249999999999999 and parameters: {'booster': 'gblinear', 'learning_rate': 0.08303627641876875, 'n_estimators': 90, 'max_depth': 6, 'min_child_weight': 1.778638688775115, 'subsample': 0.85189992180856, 'colsample_bytree': 0.6271794828527659, 'colsample_bylevel': 0.781458769177566, 'gamma': 0.07623397205198443, 'reg_alpha': 0.14784956744840128, 'reg_lambda': 0.5135639068247931}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:45:56] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:45:56] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:45:57,004]\u001b[0m Trial 91 finished with value: 0.6249999999999999 and parameters: {'booster': 'gblinear', 'learning_rate': 0.035825302833742, 'n_estimators': 91, 'max_depth': 3, 'min_child_weight': 7.602308469556981, 'subsample': 0.8088125805287841, 'colsample_bytree': 0.6146696027491967, 'colsample_bylevel': 0.7148374118230613, 'gamma': 0.23068840048492908, 'reg_alpha': 0.04724314537619717, 'reg_lambda': 0.44644201620161295}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:45:57,161]\u001b[0m Trial 92 finished with value: 0.8285714285714286 and parameters: {'booster': 'gblinear', 'learning_rate': 0.028847683894537308, 'n_estimators': 89, 'max_depth': 3, 'min_child_weight': 8.132413682031586, 'subsample': 0.7949582490268795, 'colsample_bytree': 0.7064573037101833, 'colsample_bylevel': 0.7267991607060247, 'gamma': 0.29275383204708094, 'reg_alpha': 0.11795837054794406, 'reg_lambda': 0.4283805699236417}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:45:57] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:45:57] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:45:57,329]\u001b[0m Trial 93 finished with value: 0.3333333333333333 and parameters: {'booster': 'gblinear', 'learning_rate': 0.1581333074358678, 'n_estimators': 96, 'max_depth': 10, 'min_child_weight': 3.87593459229969, 'subsample': 0.8666981477024946, 'colsample_bytree': 0.8997502626237711, 'colsample_bylevel': 0.5489500979527263, 'gamma': 0.22628937245152386, 'reg_alpha': 0.07367526913219974, 'reg_lambda': 0.048762371291388926}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:45:57,495]\u001b[0m Trial 94 finished with value: 0.3333333333333333 and parameters: {'booster': 'gblinear', 'learning_rate': 0.07678972191810056, 'n_estimators': 98, 'max_depth': 6, 'min_child_weight': 5.842412095393304, 'subsample': 0.8702005978130867, 'colsample_bytree': 0.5647627890759814, 'colsample_bylevel': 0.5712136706168996, 'gamma': 0.2579318435753976, 'reg_alpha': 0.38532150421781775, 'reg_lambda': 0.6881618125813789}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:45:57] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:45:57] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:45:57,669]\u001b[0m Trial 95 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.10944053210602414, 'n_estimators': 99, 'max_depth': 6, 'min_child_weight': 3.7328769012300063, 'subsample': 0.8626897597112246, 'colsample_bytree': 0.878921265487028, 'colsample_bylevel': 0.5420282902327251, 'gamma': 0.11570984568442061, 'reg_alpha': 0.26201327096481214, 'reg_lambda': 0.7137381735933593}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:45:57,830]\u001b[0m Trial 96 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.13785587362019286, 'n_estimators': 93, 'max_depth': 6, 'min_child_weight': 6.572012877333249, 'subsample': 0.9088607621032189, 'colsample_bytree': 0.9721778361709811, 'colsample_bylevel': 0.9621539431390459, 'gamma': 0.08937243201802728, 'reg_alpha': 0.303015460136925, 'reg_lambda': 0.9943932879636759}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:45:57] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:45:57] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:45:57,983]\u001b[0m Trial 97 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.07365753682093014, 'n_estimators': 88, 'max_depth': 7, 'min_child_weight': 6.490986488750201, 'subsample': 0.9577150219899684, 'colsample_bytree': 0.6615542305350136, 'colsample_bylevel': 0.9843017290551215, 'gamma': 0.09052356287917744, 'reg_alpha': 0.2933067812485851, 'reg_lambda': 0.9990977636057685}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:45:58,134]\u001b[0m Trial 98 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.07433614577906601, 'n_estimators': 86, 'max_depth': 7, 'min_child_weight': 6.325411389073857, 'subsample': 0.811381209602385, 'colsample_bytree': 0.7397441114668484, 'colsample_bylevel': 0.7591388274377681, 'gamma': 0.26039781689718766, 'reg_alpha': 0.19374949423424795, 'reg_lambda': 0.875017086114872}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:45:58] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:45:58,324]\u001b[0m Trial 99 finished with value: 0.3333333333333333 and parameters: {'booster': 'dart', 'learning_rate': 0.04507103167279404, 'n_estimators': 94, 'max_depth': 4, 'min_child_weight': 3.2427238518615993, 'subsample': 0.9563789312797053, 'colsample_bytree': 0.6526813271699669, 'colsample_bylevel': 0.9069177838677214, 'gamma': 0.3248322033202317, 'reg_alpha': 0.3283885630932675, 'reg_lambda': 0.10811553253693648}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:45:58,472]\u001b[0m Trial 100 finished with value: 0.8285714285714286 and parameters: {'booster': 'gblinear', 'learning_rate': 0.023406291116624955, 'n_estimators': 83, 'max_depth': 3, 'min_child_weight': 5.084870503269484, 'subsample': 0.7051587133531265, 'colsample_bytree': 0.7902850387539175, 'colsample_bylevel': 0.74650982226143, 'gamma': 0.23926441355908462, 'reg_alpha': 0.15970449411445437, 'reg_lambda': 0.3935179614665692}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:45:58] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:45:58] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:45:58,635]\u001b[0m Trial 101 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.05083050775328513, 'n_estimators': 95, 'max_depth': 3, 'min_child_weight': 3.3889119165422605, 'subsample': 0.8935679830950998, 'colsample_bytree': 0.6896465554103574, 'colsample_bylevel': 0.5086237908043462, 'gamma': 0.15213803266911943, 'reg_alpha': 0.23724861364499905, 'reg_lambda': 0.29086261603653984}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:45:58,800]\u001b[0m Trial 102 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.05855092606706949, 'n_estimators': 98, 'max_depth': 6, 'min_child_weight': 1.0555874676311112, 'subsample': 0.7316300459485808, 'colsample_bytree': 0.8040827502515547, 'colsample_bylevel': 0.7410848482131982, 'gamma': 0.16864179488577505, 'reg_alpha': 0.23268683269889484, 'reg_lambda': 0.30949975672338637}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:45:58] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:45:58] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:45:58,962]\u001b[0m Trial 103 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.05123289776054436, 'n_estimators': 95, 'max_depth': 5, 'min_child_weight': 3.4920727963539098, 'subsample': 0.8832476142580603, 'colsample_bytree': 0.6963286797564413, 'colsample_bylevel': 0.5115985055017798, 'gamma': 0.046738121159904215, 'reg_alpha': 0.3193893882060303, 'reg_lambda': 0.6191741881334216}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:45:59,127]\u001b[0m Trial 104 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.03868043032806773, 'n_estimators': 97, 'max_depth': 5, 'min_child_weight': 4.141061509229483, 'subsample': 0.6342017512494886, 'colsample_bytree': 0.8344041699326028, 'colsample_bylevel': 0.8749751275348286, 'gamma': 0.290760841597181, 'reg_alpha': 0.2528735454958099, 'reg_lambda': 0.1609828485560904}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:45:58] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:45:59] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:45:59,284]\u001b[0m Trial 105 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.017850767211654066, 'n_estimators': 90, 'max_depth': 7, 'min_child_weight': 2.1519338480192918, 'subsample': 0.5988125009653599, 'colsample_bytree': 0.6028194512521141, 'colsample_bylevel': 0.5289028301716613, 'gamma': 0.0030443666094042204, 'reg_alpha': 0.20232533756113213, 'reg_lambda': 0.476111803780238}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:45:59,445]\u001b[0m Trial 106 finished with value: 0.6666666666666666 and parameters: {'booster': 'gbtree', 'learning_rate': 0.015565611973616455, 'n_estimators': 90, 'max_depth': 7, 'min_child_weight': 1.4786058138260152, 'subsample': 0.5711990591892456, 'colsample_bytree': 0.5877792599672683, 'colsample_bylevel': 0.785373015891038, 'gamma': 0.0004981569681976042, 'reg_alpha': 0.20116059260095392, 'reg_lambda': 0.6557541808381042}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:45:59,605]\u001b[0m Trial 107 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.10080501108655796, 'n_estimators': 92, 'max_depth': 6, 'min_child_weight': 4.373941972546343, 'subsample': 0.9324683595001122, 'colsample_bytree': 0.9332294901067715, 'colsample_bylevel': 0.6657789342903311, 'gamma': 0.09903150011714663, 'reg_alpha': 0.2717780795016127, 'reg_lambda': 0.3803191275503681}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:45:59] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:45:59] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:45:59,762]\u001b[0m Trial 108 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.08577185541165148, 'n_estimators': 91, 'max_depth': 5, 'min_child_weight': 1.5259246987276278, 'subsample': 0.9125045320010037, 'colsample_bytree': 0.5494643767649974, 'colsample_bylevel': 0.6052146897004814, 'gamma': 0.022667820855631356, 'reg_alpha': 0.24704124600838323, 'reg_lambda': 0.7110026837594341}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:45:59,923]\u001b[0m Trial 109 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.09351302331998579, 'n_estimators': 94, 'max_depth': 8, 'min_child_weight': 4.595021413037598, 'subsample': 0.8611236514268555, 'colsample_bytree': 0.5188508534050158, 'colsample_bylevel': 0.6066341575180078, 'gamma': 0.021508977379418585, 'reg_alpha': 0.28373179045377656, 'reg_lambda': 0.8917072126975227}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:45:59] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:45:59] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:46:00,080]\u001b[0m Trial 110 finished with value: 0.3333333333333333 and parameters: {'booster': 'gblinear', 'learning_rate': 0.06032038749517153, 'n_estimators': 91, 'max_depth': 8, 'min_child_weight': 5.362601962201303, 'subsample': 0.7770045409357007, 'colsample_bytree': 0.5129937778160227, 'colsample_bylevel': 0.6376553182737952, 'gamma': 0.01912203967717229, 'reg_alpha': 0.6375797946367561, 'reg_lambda': 0.8803761937574466}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:46:00,247]\u001b[0m Trial 111 finished with value: 0.8285714285714286 and parameters: {'booster': 'gblinear', 'learning_rate': 0.18867714705103128, 'n_estimators': 99, 'max_depth': 6, 'min_child_weight': 7.020202994689181, 'subsample': 0.662058211481313, 'colsample_bytree': 0.5307463632269627, 'colsample_bylevel': 0.6631797203561873, 'gamma': 0.03796936269699238, 'reg_alpha': 0.3473503001230125, 'reg_lambda': 0.8392339908296768}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:46:00] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:46:00] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:46:00,406]\u001b[0m Trial 112 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.010571441150548073, 'n_estimators': 92, 'max_depth': 8, 'min_child_weight': 1.2506992731099889, 'subsample': 0.837494804201063, 'colsample_bytree': 0.5446185327349979, 'colsample_bylevel': 0.565859498907596, 'gamma': 0.4261679818020213, 'reg_alpha': 0.13081316124835948, 'reg_lambda': 0.5943939882121375}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:46:00,561]\u001b[0m Trial 113 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.08736771252724634, 'n_estimators': 89, 'max_depth': 5, 'min_child_weight': 1.9431677617699894, 'subsample': 0.8936994694076236, 'colsample_bytree': 0.5793184473128525, 'colsample_bylevel': 0.8440096834878752, 'gamma': 0.03295212100797435, 'reg_alpha': 0.2206968488570187, 'reg_lambda': 0.7780420768671411}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:46:00] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:46:00] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:46:00,722]\u001b[0m Trial 114 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.010350724964879435, 'n_estimators': 92, 'max_depth': 5, 'min_child_weight': 1.67621013142157, 'subsample': 0.8791603304798962, 'colsample_bytree': 0.5409542729982864, 'colsample_bylevel': 0.8031756041001626, 'gamma': 0.029857849965281638, 'reg_alpha': 0.13556850287962116, 'reg_lambda': 0.6138011191824067}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:46:00,885]\u001b[0m Trial 115 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.12372858960780854, 'n_estimators': 96, 'max_depth': 5, 'min_child_weight': 5.736331971446163, 'subsample': 0.8744151751245737, 'colsample_bytree': 0.7814736990496834, 'colsample_bylevel': 0.5570810089074503, 'gamma': 0.14295337602497615, 'reg_alpha': 0.273529827054721, 'reg_lambda': 0.3201880620613907}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:46:00] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:46:00] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:46:01,049]\u001b[0m Trial 116 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.12388102279135602, 'n_estimators': 96, 'max_depth': 6, 'min_child_weight': 4.937328800856982, 'subsample': 0.7164688569179402, 'colsample_bytree': 0.780177094133668, 'colsample_bylevel': 0.6329451553308013, 'gamma': 0.12204394195954898, 'reg_alpha': 0.2736932782831004, 'reg_lambda': 0.3521401117373305}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:46:01,209]\u001b[0m Trial 117 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.14165382723214734, 'n_estimators': 93, 'max_depth': 6, 'min_child_weight': 6.078436671111833, 'subsample': 0.76044812958833, 'colsample_bytree': 0.814748525757856, 'colsample_bylevel': 0.9765839434534174, 'gamma': 0.08931716917641913, 'reg_alpha': 0.32120189482977685, 'reg_lambda': 0.9848075766286679}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:46:01] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:46:01] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:46:01,370]\u001b[0m Trial 118 finished with value: 0.3333333333333333 and parameters: {'booster': 'gblinear', 'learning_rate': 0.15430931610482537, 'n_estimators': 93, 'max_depth': 4, 'min_child_weight': 6.0610339835929485, 'subsample': 0.7586206340966266, 'colsample_bytree': 0.9853262972116112, 'colsample_bylevel': 0.9750455485702825, 'gamma': 0.0890485433633652, 'reg_alpha': 0.39664953598341657, 'reg_lambda': 0.9936794196022022}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:46:01,522]\u001b[0m Trial 119 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.07432137705882402, 'n_estimators': 86, 'max_depth': 7, 'min_child_weight': 6.465687230913235, 'subsample': 0.9744987587906406, 'colsample_bytree': 0.7600231182520227, 'colsample_bylevel': 0.953933000882769, 'gamma': 0.06951237452245349, 'reg_alpha': 0.30300224099175976, 'reg_lambda': 0.9292175135700822}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:46:01] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:46:01] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:46:01,677]\u001b[0m Trial 120 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.09398826487103133, 'n_estimators': 88, 'max_depth': 7, 'min_child_weight': 5.441989510256177, 'subsample': 0.8373092471853183, 'colsample_bytree': 0.5896693077357598, 'colsample_bylevel': 0.8645707943789042, 'gamma': 0.04371163883000715, 'reg_alpha': 0.21542594251681343, 'reg_lambda': 0.8072676997745041}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:46:01,841]\u001b[0m Trial 121 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.09073129675847044, 'n_estimators': 97, 'max_depth': 7, 'min_child_weight': 6.6991739072686025, 'subsample': 0.8500817906516328, 'colsample_bytree': 0.5609443530221465, 'colsample_bylevel': 0.582670541533871, 'gamma': 0.0673493518846436, 'reg_alpha': 0.29773968139824664, 'reg_lambda': 0.9213312906562181}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:46:01] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:46:01] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:46:02,008]\u001b[0m Trial 122 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.11035566824778294, 'n_estimators': 98, 'max_depth': 5, 'min_child_weight': 4.5870503104145675, 'subsample': 0.858318846662355, 'colsample_bytree': 0.5278906955468603, 'colsample_bylevel': 0.6422034292562406, 'gamma': 0.05296693426529042, 'reg_alpha': 0.24013875616264838, 'reg_lambda': 0.7295498244233836}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:46:02,174]\u001b[0m Trial 123 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.08034040581791782, 'n_estimators': 98, 'max_depth': 7, 'min_child_weight': 7.212426565985157, 'subsample': 0.8861117566814934, 'colsample_bytree': 0.5010544495804402, 'colsample_bylevel': 0.5906617287131691, 'gamma': 0.01644284543606256, 'reg_alpha': 0.2547736499875887, 'reg_lambda': 0.8554037245713098}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:46:02] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:46:02] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:46:02,343]\u001b[0m Trial 124 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.10038638068436528, 'n_estimators': 98, 'max_depth': 8, 'min_child_weight': 6.9458105993823365, 'subsample': 0.780337262708617, 'colsample_bytree': 0.85000581444168, 'colsample_bylevel': 0.6162866872512288, 'gamma': 0.011352874710876128, 'reg_alpha': 0.1600696708093773, 'reg_lambda': 0.8165505960975857}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:46:02,516]\u001b[0m Trial 125 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.05385851591926884, 'n_estimators': 100, 'max_depth': 6, 'min_child_weight': 2.920553993367485, 'subsample': 0.8482930049092214, 'colsample_bytree': 0.5551982234494827, 'colsample_bylevel': 0.5522898604718919, 'gamma': 0.2048656295453556, 'reg_alpha': 0.18089237271337164, 'reg_lambda': 0.752631294457798}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:46:02] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:46:02] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:46:02,667]\u001b[0m Trial 126 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.06675098113146205, 'n_estimators': 85, 'max_depth': 9, 'min_child_weight': 3.1477254567427106, 'subsample': 0.8234868441530435, 'colsample_bytree': 0.8031314152406364, 'colsample_bylevel': 0.7743146226831957, 'gamma': 0.05948831940652491, 'reg_alpha': 0.2264734438789552, 'reg_lambda': 0.5466143464843104}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:46:02,819]\u001b[0m Trial 127 finished with value: 0.3333333333333333 and parameters: {'booster': 'gblinear', 'learning_rate': 0.08381900312687927, 'n_estimators': 85, 'max_depth': 9, 'min_child_weight': 6.510778420482622, 'subsample': 0.8084676587494779, 'colsample_bytree': 0.8230299612076487, 'colsample_bylevel': 0.5034350001014382, 'gamma': 0.25741433039494577, 'reg_alpha': 0.546407183090239, 'reg_lambda': 0.9470462794137541}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:46:02] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:46:03,006]\u001b[0m Trial 128 finished with value: 0.3333333333333333 and parameters: {'booster': 'dart', 'learning_rate': 0.10077565075386824, 'n_estimators': 91, 'max_depth': 6, 'min_child_weight': 4.365698679375238, 'subsample': 0.9322282151248301, 'colsample_bytree': 0.8718570434804934, 'colsample_bylevel': 0.5247050379133731, 'gamma': 0.10316587664160946, 'reg_alpha': 0.28742092671090375, 'reg_lambda': 0.059847858343178864}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:46:03,174]\u001b[0m Trial 129 finished with value: 0.3333333333333333 and parameters: {'booster': 'gbtree', 'learning_rate': 0.0970694694165657, 'n_estimators': 94, 'max_depth': 6, 'min_child_weight': 4.8350900982357725, 'subsample': 0.9081960493308296, 'colsample_bytree': 0.840049154390478, 'colsample_bylevel': 0.6097710404501518, 'gamma': 0.10870333594986778, 'reg_alpha': 0.27245600927721, 'reg_lambda': 0.0071226224386130785}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:46:03,342]\u001b[0m Trial 130 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.04531814912009839, 'n_estimators': 97, 'max_depth': 5, 'min_child_weight': 3.5998239948248476, 'subsample': 0.5812401214289973, 'colsample_bytree': 0.605959633611523, 'colsample_bylevel': 0.873241596689535, 'gamma': 0.2900921036632651, 'reg_alpha': 0.32820280002516905, 'reg_lambda': 0.16073337850440295}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:46:03] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:46:03] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:46:03,512]\u001b[0m Trial 131 finished with value: 0.3333333333333333 and parameters: {'booster': 'gblinear', 'learning_rate': 0.04656404011402661, 'n_estimators': 97, 'max_depth': 5, 'min_child_weight': 7.2990855401398935, 'subsample': 0.5054791027252521, 'colsample_bytree': 0.6065209443115684, 'colsample_bylevel': 0.7265863957421884, 'gamma': 0.02769297143564908, 'reg_alpha': 0.3564340195441137, 'reg_lambda': 0.6532545975422531}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:46:03,674]\u001b[0m Trial 132 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.11459064504882967, 'n_estimators': 94, 'max_depth': 4, 'min_child_weight': 5.235564994288412, 'subsample': 0.9167252154207407, 'colsample_bytree': 0.9300624528219398, 'colsample_bylevel': 0.676770966467467, 'gamma': 0.09875051452228886, 'reg_alpha': 0.2685745925544566, 'reg_lambda': 0.42633802047724}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:46:03] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:46:03] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:46:03,838]\u001b[0m Trial 133 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.08623838109364895, 'n_estimators': 92, 'max_depth': 4, 'min_child_weight': 3.806064190774853, 'subsample': 0.9188879836756473, 'colsample_bytree': 0.9502684155653008, 'colsample_bylevel': 0.6533863925404467, 'gamma': 0.046830348569087035, 'reg_alpha': 0.2508137277783526, 'reg_lambda': 0.7011735951931721}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:46:04,010]\u001b[0m Trial 134 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.08696385536800798, 'n_estimators': 92, 'max_depth': 7, 'min_child_weight': 4.067207261843525, 'subsample': 0.9495460402571314, 'colsample_bytree': 0.9483834412999734, 'colsample_bylevel': 0.6513756277817602, 'gamma': 0.02173954636368826, 'reg_alpha': 0.3003677695091522, 'reg_lambda': 0.6909622727472388}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:46:03] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:46:04] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:46:04,171]\u001b[0m Trial 135 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.0861662008333403, 'n_estimators': 92, 'max_depth': 8, 'min_child_weight': 3.666110599071328, 'subsample': 0.948894493890294, 'colsample_bytree': 0.9466463886389253, 'colsample_bylevel': 0.6061165025940548, 'gamma': 0.40955645427677945, 'reg_alpha': 0.30029089067114245, 'reg_lambda': 0.702789330888757}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:46:04,332]\u001b[0m Trial 136 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.09235099091291707, 'n_estimators': 92, 'max_depth': 8, 'min_child_weight': 4.0390017273879755, 'subsample': 0.9460868406270891, 'colsample_bytree': 0.9295836536828822, 'colsample_bylevel': 0.6711468188154036, 'gamma': 0.4424765332257507, 'reg_alpha': 0.19360015594773944, 'reg_lambda': 0.7875397944929196}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:46:04] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:46:04] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:46:04,488]\u001b[0m Trial 137 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.05193024503855272, 'n_estimators': 87, 'max_depth': 6, 'min_child_weight': 3.4488555878004443, 'subsample': 0.7332245494469666, 'colsample_bytree': 0.6915554127065029, 'colsample_bylevel': 0.7616092070550591, 'gamma': 0.07928898675890989, 'reg_alpha': 0.19363951865479612, 'reg_lambda': 0.9710129664292746}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:46:04,644]\u001b[0m Trial 138 finished with value: 0.3333333333333333 and parameters: {'booster': 'gblinear', 'learning_rate': 0.05084735278849835, 'n_estimators': 87, 'max_depth': 6, 'min_child_weight': 6.362612030412667, 'subsample': 0.7341181993428487, 'colsample_bytree': 0.6843511008284877, 'colsample_bylevel': 0.5207231697922147, 'gamma': 0.2712129470017885, 'reg_alpha': 0.33657091918240617, 'reg_lambda': 0.9702736739846439}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:46:04] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:46:04] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:46:04,801]\u001b[0m Trial 139 finished with value: 0.8285714285714286 and parameters: {'booster': 'gblinear', 'learning_rate': 0.026539709838298536, 'n_estimators': 89, 'max_depth': 5, 'min_child_weight': 1.9381481439022705, 'subsample': 0.8992846667057801, 'colsample_bytree': 0.5787253258791242, 'colsample_bylevel': 0.8050552952483574, 'gamma': 0.03425573249499359, 'reg_alpha': 0.14304099030279985, 'reg_lambda': 0.6116810200089388}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:46:04,966]\u001b[0m Trial 140 finished with value: 0.8285714285714286 and parameters: {'booster': 'gblinear', 'learning_rate': 0.0311461019975241, 'n_estimators': 89, 'max_depth': 5, 'min_child_weight': 1.247900587552355, 'subsample': 0.8735271636531756, 'colsample_bytree': 0.5428910789601675, 'colsample_bylevel': 0.809579139906201, 'gamma': 0.15359400759132794, 'reg_alpha': 0.1115912376518641, 'reg_lambda': 0.7721080745234645}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:46:04] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:46:05] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:46:05,132]\u001b[0m Trial 141 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.018012192972317487, 'n_estimators': 95, 'max_depth': 3, 'min_child_weight': 3.400084281505558, 'subsample': 0.8900656806657032, 'colsample_bytree': 0.6987909524632915, 'colsample_bylevel': 0.5112349393886528, 'gamma': 0.1682219356336558, 'reg_alpha': 0.2343336690453937, 'reg_lambda': 0.4829944337250816}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:46:05,297]\u001b[0m Trial 142 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.038185972369628965, 'n_estimators': 95, 'max_depth': 4, 'min_child_weight': 4.124627725243286, 'subsample': 0.612196608078779, 'colsample_bytree': 0.7204533811906978, 'colsample_bylevel': 0.5312687664753535, 'gamma': 0.2931712764424798, 'reg_alpha': 0.23269472645046946, 'reg_lambda': 0.16773159576817007}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:46:05] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:46:05] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:46:05,468]\u001b[0m Trial 143 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.05883904706418678, 'n_estimators': 99, 'max_depth': 6, 'min_child_weight': 1.1509554892608524, 'subsample': 0.8641980544059927, 'colsample_bytree': 0.8000962606703168, 'colsample_bylevel': 0.9569122755284679, 'gamma': 0.12244940176740221, 'reg_alpha': 0.2239907508615275, 'reg_lambda': 0.896648081666495}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:46:05,622]\u001b[0m Trial 144 finished with value: 0.8285714285714286 and parameters: {'booster': 'gblinear', 'learning_rate': 0.13961349463624717, 'n_estimators': 86, 'max_depth': 7, 'min_child_weight': 2.93710555772442, 'subsample': 0.7937645142632166, 'colsample_bytree': 0.7551908904235283, 'colsample_bylevel': 0.9931081198809356, 'gamma': 0.16637342098596358, 'reg_alpha': 0.20668027774041117, 'reg_lambda': 0.30737422633118017}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:46:05] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:46:05] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:46:05,779]\u001b[0m Trial 145 finished with value: 0.8285714285714286 and parameters: {'booster': 'gblinear', 'learning_rate': 0.06878058285655729, 'n_estimators': 88, 'max_depth': 4, 'min_child_weight': 2.4073848248644794, 'subsample': 0.5308586673255391, 'colsample_bytree': 0.6620704297477872, 'colsample_bylevel': 0.5124275678435435, 'gamma': 0.06651701619156244, 'reg_alpha': 0.19879205422001237, 'reg_lambda': 0.21983476352564824}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:46:05,943]\u001b[0m Trial 146 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.04173676508938863, 'n_estimators': 95, 'max_depth': 5, 'min_child_weight': 4.636506794293247, 'subsample': 0.6358471783224529, 'colsample_bytree': 0.7337356541074138, 'colsample_bylevel': 0.6934989059817075, 'gamma': 0.26377623005665624, 'reg_alpha': 0.31837206485400216, 'reg_lambda': 0.2731109005921665}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:46:05] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:46:05] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:46:06,093]\u001b[0m Trial 147 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.09091357386794742, 'n_estimators': 82, 'max_depth': 7, 'min_child_weight': 6.147200942237903, 'subsample': 0.8346072524375275, 'colsample_bytree': 0.5884318816610243, 'colsample_bylevel': 0.859411535295483, 'gamma': 0.04594951562978467, 'reg_alpha': 0.3119629645339742, 'reg_lambda': 0.9222959468087746}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:46:06,239]\u001b[0m Trial 148 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.12658389676488696, 'n_estimators': 81, 'max_depth': 6, 'min_child_weight': 2.7998612584861, 'subsample': 0.7844178329092028, 'colsample_bytree': 0.8188075668260707, 'colsample_bylevel': 0.6284359078441498, 'gamma': 0.12744493580779412, 'reg_alpha': 0.17150690496159746, 'reg_lambda': 0.8669833327758331}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:46:06] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:46:06] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:46:06,411]\u001b[0m Trial 149 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.04795493044245199, 'n_estimators': 99, 'max_depth': 3, 'min_child_weight': 3.3132688343862142, 'subsample': 0.6785770189891094, 'colsample_bytree': 0.857649751453057, 'colsample_bylevel': 0.9216518331497234, 'gamma': 0.1881309406172551, 'reg_alpha': 0.2604214367622904, 'reg_lambda': 0.2965984176239902}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:46:06,581]\u001b[0m Trial 150 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.06276378201698918, 'n_estimators': 99, 'max_depth': 7, 'min_child_weight': 1.0233254516978643, 'subsample': 0.9685538514129611, 'colsample_bytree': 0.915718493912697, 'colsample_bylevel': 0.5367672107344481, 'gamma': 0.0916383149685442, 'reg_alpha': 0.15813758931481667, 'reg_lambda': 0.9526962310346878}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:46:06] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:46:06] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:46:06,743]\u001b[0m Trial 151 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.012973539094510711, 'n_estimators': 93, 'max_depth': 5, 'min_child_weight': 1.6685684740138478, 'subsample': 0.8785045181563881, 'colsample_bytree': 0.5708118048402464, 'colsample_bylevel': 0.8415313547419868, 'gamma': 0.028681668820017645, 'reg_alpha': 0.12122979529834438, 'reg_lambda': 0.5848963355939691}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:46:06,910]\u001b[0m Trial 152 finished with value: 0.6249999999999999 and parameters: {'booster': 'gblinear', 'learning_rate': 0.1239909185867182, 'n_estimators': 96, 'max_depth': 5, 'min_child_weight': 1.8191198407496445, 'subsample': 0.8768186233686818, 'colsample_bytree': 0.5701744796424437, 'colsample_bylevel': 0.8352990130125414, 'gamma': 0.03159612391364, 'reg_alpha': 0.09842490121364561, 'reg_lambda': 0.5787734298133429}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:46:06] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:46:06] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:46:07,076]\u001b[0m Trial 153 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.015525531177057485, 'n_estimators': 96, 'max_depth': 5, 'min_child_weight': 5.861002455736401, 'subsample': 0.7591690501192221, 'colsample_bytree': 0.5378601559763515, 'colsample_bylevel': 0.5616856448421601, 'gamma': 0.033336021745437615, 'reg_alpha': 0.12916714458676828, 'reg_lambda': 0.7288894777671355}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:46:07,232]\u001b[0m Trial 154 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.05808746031698802, 'n_estimators': 86, 'max_depth': 8, 'min_child_weight': 3.8078602905937586, 'subsample': 0.6134709921034223, 'colsample_bytree': 0.9711071574729222, 'colsample_bylevel': 0.5430414107221458, 'gamma': 0.11880602578568797, 'reg_alpha': 0.24605484472812766, 'reg_lambda': 0.6409818856402294}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:46:07] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:46:07] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:46:07,392]\u001b[0m Trial 155 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.07326986749344647, 'n_estimators': 87, 'max_depth': 6, 'min_child_weight': 1.2132403032399066, 'subsample': 0.9354287637440181, 'colsample_bytree': 0.8904124361895445, 'colsample_bylevel': 0.5297266802737594, 'gamma': 0.11235038996791952, 'reg_alpha': 0.28408520374931356, 'reg_lambda': 0.3797727513163591}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:46:07,561]\u001b[0m Trial 156 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.07985817148419029, 'n_estimators': 98, 'max_depth': 6, 'min_child_weight': 1.462876462303256, 'subsample': 0.9066822397173462, 'colsample_bytree': 0.9631822557181008, 'colsample_bylevel': 0.9887511153363454, 'gamma': 0.13225270691609808, 'reg_alpha': 0.2509772710317542, 'reg_lambda': 0.35659763693039426}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:46:07] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:46:07,761]\u001b[0m Trial 157 finished with value: 0.8285714285714286 and parameters: {'booster': 'dart', 'learning_rate': 0.10145710508613559, 'n_estimators': 94, 'max_depth': 8, 'min_child_weight': 2.1283093692234893, 'subsample': 0.8913327276893199, 'colsample_bytree': 0.8724104779869959, 'colsample_bylevel': 0.5720840199143347, 'gamma': 0.0036887632936320157, 'reg_alpha': 0.2630951660961998, 'reg_lambda': 0.6623522723013096}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:46:07,925]\u001b[0m Trial 158 finished with value: 0.3333333333333333 and parameters: {'booster': 'gbtree', 'learning_rate': 0.12016965588708312, 'n_estimators': 91, 'max_depth': 5, 'min_child_weight': 4.931560971868182, 'subsample': 0.8670261289008147, 'colsample_bytree': 0.5537726165069725, 'colsample_bylevel': 0.5854678757416631, 'gamma': 0.020408617668345293, 'reg_alpha': 0.1727234711914482, 'reg_lambda': 0.6300050405368429}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:46:08,092]\u001b[0m Trial 159 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.11503209087949098, 'n_estimators': 97, 'max_depth': 7, 'min_child_weight': 5.176062909259388, 'subsample': 0.8515855191683503, 'colsample_bytree': 0.6343166425689413, 'colsample_bylevel': 0.6444713837931296, 'gamma': 0.0542723095131651, 'reg_alpha': 0.2782298927757305, 'reg_lambda': 0.4137329334601756}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:46:07] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:46:08] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:46:08,253]\u001b[0m Trial 160 finished with value: 0.3333333333333333 and parameters: {'booster': 'gblinear', 'learning_rate': 0.135496836626344, 'n_estimators': 93, 'max_depth': 8, 'min_child_weight': 1.4440351057982002, 'subsample': 0.5939638807173402, 'colsample_bytree': 0.508023084179076, 'colsample_bylevel': 0.823624961635322, 'gamma': 0.008966992202914706, 'reg_alpha': 0.3689894473698061, 'reg_lambda': 0.8385228898336418}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:46:08,420]\u001b[0m Trial 161 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.1089980710894153, 'n_estimators': 97, 'max_depth': 7, 'min_child_weight': 5.54213080743123, 'subsample': 0.9230736461939714, 'colsample_bytree': 0.9470589592630578, 'colsample_bylevel': 0.6453408901949533, 'gamma': 0.048953745875533285, 'reg_alpha': 0.2896129157406466, 'reg_lambda': 0.42266022012058535}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:46:08] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:46:08] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:46:08,591]\u001b[0m Trial 162 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.10908905362504151, 'n_estimators': 98, 'max_depth': 7, 'min_child_weight': 5.654247843276375, 'subsample': 0.846056370559056, 'colsample_bytree': 0.7834878604471129, 'colsample_bylevel': 0.9380958880697384, 'gamma': 0.06986731503640832, 'reg_alpha': 0.20972772558473435, 'reg_lambda': 0.815414633434891}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:46:08,760]\u001b[0m Trial 163 finished with value: 0.3333333333333333 and parameters: {'booster': 'gblinear', 'learning_rate': 0.012083525313717073, 'n_estimators': 96, 'max_depth': 7, 'min_child_weight': 5.9823754117773245, 'subsample': 0.7428580240801973, 'colsample_bytree': 0.7735315805821158, 'colsample_bylevel': 0.9370250886133012, 'gamma': 0.04051323292979593, 'reg_alpha': 0.7083977565406965, 'reg_lambda': 0.7713507256357712}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:46:08] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:46:08] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:46:08,920]\u001b[0m Trial 164 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.09418903711492627, 'n_estimators': 92, 'max_depth': 8, 'min_child_weight': 3.927304643775085, 'subsample': 0.9471067762660582, 'colsample_bytree': 0.9341946218619764, 'colsample_bylevel': 0.671951412412573, 'gamma': 0.4334289314638485, 'reg_alpha': 0.21513129095925398, 'reg_lambda': 0.7005496904336213}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:46:09,079]\u001b[0m Trial 165 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.14176668436732248, 'n_estimators': 91, 'max_depth': 5, 'min_child_weight': 1.00003548358701, 'subsample': 0.904213977932183, 'colsample_bytree': 0.5177329509855011, 'colsample_bylevel': 0.62652535414183, 'gamma': 0.1398912732743207, 'reg_alpha': 0.23505854055776748, 'reg_lambda': 0.31312300800724086}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:46:08] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:46:09] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:46:09,241]\u001b[0m Trial 166 finished with value: 0.8285714285714286 and parameters: {'booster': 'gblinear', 'learning_rate': 0.15816070588891795, 'n_estimators': 93, 'max_depth': 6, 'min_child_weight': 1.2992354797889443, 'subsample': 0.69508867785413, 'colsample_bytree': 0.5482713769357672, 'colsample_bylevel': 0.9686589038865822, 'gamma': 0.15386164806301963, 'reg_alpha': 0.2388283101688917, 'reg_lambda': 0.2405105408631818}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:46:09,406]\u001b[0m Trial 167 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.12926196072910082, 'n_estimators': 96, 'max_depth': 7, 'min_child_weight': 5.468287258187408, 'subsample': 0.72362671099627, 'colsample_bytree': 0.767848897080072, 'colsample_bylevel': 0.8223437340929525, 'gamma': 0.06279200508537267, 'reg_alpha': 0.31366094607302153, 'reg_lambda': 0.9215092896430055}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:46:09] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:46:09] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:46:09,572]\u001b[0m Trial 168 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.14569997471890941, 'n_estimators': 95, 'max_depth': 5, 'min_child_weight': 4.275866854618818, 'subsample': 0.9629274147249537, 'colsample_bytree': 0.7807840217699727, 'colsample_bylevel': 0.8890915158639492, 'gamma': 0.14977299662341753, 'reg_alpha': 0.2730643549608017, 'reg_lambda': 0.9596934039024774}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:46:09,730]\u001b[0m Trial 169 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.07762213706524192, 'n_estimators': 90, 'max_depth': 6, 'min_child_weight': 4.5566034806154345, 'subsample': 0.9705462014451277, 'colsample_bytree': 0.8122356353051751, 'colsample_bylevel': 0.9766491978441392, 'gamma': 0.0803241988708176, 'reg_alpha': 0.33965197567274763, 'reg_lambda': 0.3352670723190978}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:46:09] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:46:09] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:46:09,881]\u001b[0m Trial 170 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.10342213290300531, 'n_estimators': 85, 'max_depth': 5, 'min_child_weight': 1.609078436543561, 'subsample': 0.9984807019772143, 'colsample_bytree': 0.7466642468311911, 'colsample_bylevel': 0.5028334031309675, 'gamma': 0.3912205138147793, 'reg_alpha': 0.2227735696231612, 'reg_lambda': 0.32377033926065424}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:46:10,041]\u001b[0m Trial 171 finished with value: 0.6249999999999999 and parameters: {'booster': 'gblinear', 'learning_rate': 0.02210008155099229, 'n_estimators': 92, 'max_depth': 7, 'min_child_weight': 2.5662705092750104, 'subsample': 0.8873448216367481, 'colsample_bytree': 0.5621063427498175, 'colsample_bylevel': 0.6069171446149499, 'gamma': 0.02119006718005649, 'reg_alpha': 0.290021887911519, 'reg_lambda': 0.9814001059664519}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:46:09] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:46:10] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:46:10,202]\u001b[0m Trial 172 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.08366560559139875, 'n_estimators': 92, 'max_depth': 3, 'min_child_weight': 3.6509731674588277, 'subsample': 0.9159297964845624, 'colsample_bytree': 0.917966592785833, 'colsample_bylevel': 0.6833968728512477, 'gamma': 0.4431579303841892, 'reg_alpha': 0.19180035878001034, 'reg_lambda': 0.7375937313524492}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:46:10,362]\u001b[0m Trial 173 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.06575110906529039, 'n_estimators': 88, 'max_depth': 3, 'min_child_weight': 6.7173191624321245, 'subsample': 0.8121913057362152, 'colsample_bytree': 0.7216029730930988, 'colsample_bylevel': 0.7563016323300388, 'gamma': 0.20157622554517002, 'reg_alpha': 0.19389945900427583, 'reg_lambda': 0.5133425625044922}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:46:10] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:46:10] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:46:10,536]\u001b[0m Trial 174 finished with value: 0.8285714285714286 and parameters: {'booster': 'gblinear', 'learning_rate': 0.059911940845356414, 'n_estimators': 100, 'max_depth': 3, 'min_child_weight': 3.7814109831968166, 'subsample': 0.8275466585341416, 'colsample_bytree': 0.7161977256639998, 'colsample_bylevel': 0.5710405093174072, 'gamma': 0.17122026547658334, 'reg_alpha': 0.18429134434324473, 'reg_lambda': 0.50902796945124}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:46:10,709]\u001b[0m Trial 175 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.06530470632078492, 'n_estimators': 99, 'max_depth': 3, 'min_child_weight': 6.619039921675917, 'subsample': 0.8618093506247564, 'colsample_bytree': 0.7054470425616864, 'colsample_bylevel': 0.5459537804742871, 'gamma': 0.20417247400432598, 'reg_alpha': 0.2319848098194271, 'reg_lambda': 0.4591398833820486}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:46:10] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:46:10] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:46:10,880]\u001b[0m Trial 176 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.05614005785267711, 'n_estimators': 99, 'max_depth': 3, 'min_child_weight': 4.171809248545199, 'subsample': 0.8639669157460881, 'colsample_bytree': 0.8015052731543729, 'colsample_bylevel': 0.5514308671609445, 'gamma': 0.21756076566241156, 'reg_alpha': 0.2282747127235931, 'reg_lambda': 0.5582357340194491}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:46:11,034]\u001b[0m Trial 177 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.0954152101009855, 'n_estimators': 87, 'max_depth': 5, 'min_child_weight': 1.8311441776931363, 'subsample': 0.8369037770964172, 'colsample_bytree': 0.7592147951396546, 'colsample_bylevel': 0.8749477450832759, 'gamma': 0.47153433889499047, 'reg_alpha': 0.3151381457399898, 'reg_lambda': 0.926368947720938}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:46:10] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:46:11] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:46:11,194]\u001b[0m Trial 178 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.11883427353818381, 'n_estimators': 90, 'max_depth': 5, 'min_child_weight': 4.556156048466748, 'subsample': 0.6261560632940656, 'colsample_bytree': 0.5025418456134825, 'colsample_bylevel': 0.6005263815897631, 'gamma': 0.05366658329100122, 'reg_alpha': 0.25571926486122754, 'reg_lambda': 0.9024963210686512}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:46:11,341]\u001b[0m Trial 179 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.09050836948213098, 'n_estimators': 80, 'max_depth': 6, 'min_child_weight': 5.807122144826334, 'subsample': 0.6397764685227548, 'colsample_bytree': 0.7374815243990982, 'colsample_bylevel': 0.9580977850024636, 'gamma': 0.11882103192475738, 'reg_alpha': 0.3164693156995388, 'reg_lambda': 0.8972759711384592}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:46:11] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:46:11] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:46:11,488]\u001b[0m Trial 180 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.1294923754766775, 'n_estimators': 80, 'max_depth': 6, 'min_child_weight': 5.823692708949917, 'subsample': 0.6531496915043408, 'colsample_bytree': 0.7385665990124206, 'colsample_bylevel': 0.5975037168870775, 'gamma': 0.12327159034637372, 'reg_alpha': 0.30596018951143633, 'reg_lambda': 0.8660764721436427}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:46:11,653]\u001b[0m Trial 181 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.08795548691871674, 'n_estimators': 92, 'max_depth': 4, 'min_child_weight': 4.023874032049654, 'subsample': 0.9540625444207733, 'colsample_bytree': 0.9482935450941047, 'colsample_bylevel': 0.6568382365496594, 'gamma': 0.4539907288594214, 'reg_alpha': 0.19241080086747253, 'reg_lambda': 0.7525264139231895}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:46:11] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:46:11] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:46:11,816]\u001b[0m Trial 182 finished with value: 0.8285714285714286 and parameters: {'booster': 'gblinear', 'learning_rate': 0.08759904322227592, 'n_estimators': 93, 'max_depth': 4, 'min_child_weight': 3.17915294676284, 'subsample': 0.9486985868462673, 'colsample_bytree': 0.94555754402044, 'colsample_bylevel': 0.6585606837198984, 'gamma': 0.04337331871364223, 'reg_alpha': 0.15899683182430982, 'reg_lambda': 0.6953967277473664}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:46:11,981]\u001b[0m Trial 183 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.05599561792409585, 'n_estimators': 95, 'max_depth': 8, 'min_child_weight': 4.115945673600311, 'subsample': 0.8231251804174454, 'colsample_bytree': 0.6954498543048674, 'colsample_bylevel': 0.7621732271482491, 'gamma': 0.06129392241256382, 'reg_alpha': 0.2174046536779603, 'reg_lambda': 0.7904827741988746}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:46:11] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:46:12] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:46:12,146]\u001b[0m Trial 184 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.054124330364664706, 'n_estimators': 95, 'max_depth': 8, 'min_child_weight': 4.077866993720895, 'subsample': 0.8226435883986115, 'colsample_bytree': 0.7302067008086619, 'colsample_bylevel': 0.7730216858559901, 'gamma': 0.07363323204520172, 'reg_alpha': 0.20337288805635587, 'reg_lambda': 0.5476759518079944}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:46:12,310]\u001b[0m Trial 185 finished with value: 0.8285714285714286 and parameters: {'booster': 'gblinear', 'learning_rate': 0.05339178161757302, 'n_estimators': 94, 'max_depth': 9, 'min_child_weight': 3.631473619099891, 'subsample': 0.8453720225543039, 'colsample_bytree': 0.9128607896520865, 'colsample_bylevel': 0.6158749563008208, 'gamma': 0.09976381388170913, 'reg_alpha': 0.18298819327175572, 'reg_lambda': 0.5494205473715147}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:46:12] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:46:12] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:46:12,463]\u001b[0m Trial 186 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.037240867713967056, 'n_estimators': 84, 'max_depth': 10, 'min_child_weight': 3.171160668305658, 'subsample': 0.5395777549595865, 'colsample_bytree': 0.9356037696768247, 'colsample_bylevel': 0.708489588810239, 'gamma': 0.3120213096949588, 'reg_alpha': 0.25823292382721763, 'reg_lambda': 0.7890819711613464}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:46:12,638]\u001b[0m Trial 187 finished with value: 0.3333333333333333 and parameters: {'booster': 'dart', 'learning_rate': 0.0724994612903857, 'n_estimators': 84, 'max_depth': 6, 'min_child_weight': 3.2654850914528795, 'subsample': 0.532555659115489, 'colsample_bytree': 0.9879359638786922, 'colsample_bylevel': 0.535656249196093, 'gamma': 0.3372279339069046, 'reg_alpha': 0.2556571240030755, 'reg_lambda': 0.6430165420592113}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:46:12,805]\u001b[0m Trial 188 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.0771791515221145, 'n_estimators': 97, 'max_depth': 7, 'min_child_weight': 6.405087939472457, 'subsample': 0.8577158205446127, 'colsample_bytree': 0.6301889438049812, 'colsample_bylevel': 0.9555279322892254, 'gamma': 0.08039312226120433, 'reg_alpha': 0.28110096773783294, 'reg_lambda': 0.3524095574946056}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:46:12] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:46:12,962]\u001b[0m Trial 189 finished with value: 0.3333333333333333 and parameters: {'booster': 'gbtree', 'learning_rate': 0.07471469402652961, 'n_estimators': 86, 'max_depth': 7, 'min_child_weight': 6.304736591946192, 'subsample': 0.7701702563007811, 'colsample_bytree': 0.9606736796483868, 'colsample_bylevel': 0.5226633968129459, 'gamma': 0.13353834909834816, 'reg_alpha': 0.24456388751462974, 'reg_lambda': 0.3480534277204985}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:46:13,115]\u001b[0m Trial 190 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.06194756300071585, 'n_estimators': 86, 'max_depth': 6, 'min_child_weight': 5.081262050427689, 'subsample': 0.6209678716903735, 'colsample_bytree': 0.6291056653673659, 'colsample_bylevel': 0.997591180775849, 'gamma': 0.0857046824428879, 'reg_alpha': 0.27734170684574355, 'reg_lambda': 0.3776018116369658}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:46:12] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:46:13] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:46:13,283]\u001b[0m Trial 191 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.10778274251929644, 'n_estimators': 98, 'max_depth': 7, 'min_child_weight': 7.0791003379318855, 'subsample': 0.5755465176808767, 'colsample_bytree': 0.8493076761160817, 'colsample_bylevel': 0.5805108928998127, 'gamma': 0.02029609958347282, 'reg_alpha': 0.3327543060744183, 'reg_lambda': 0.7164910455866308}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:46:13,455]\u001b[0m Trial 192 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.09795767633100158, 'n_estimators': 100, 'max_depth': 9, 'min_child_weight': 5.274626470833101, 'subsample': 0.8576931150958873, 'colsample_bytree': 0.5178806499990236, 'colsample_bylevel': 0.586307178070664, 'gamma': 0.012609877698007164, 'reg_alpha': 0.2647044236122496, 'reg_lambda': 0.8491152546682448}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:46:13] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:46:13] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:46:13,617]\u001b[0m Trial 193 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.08160229111533637, 'n_estimators': 93, 'max_depth': 8, 'min_child_weight': 6.814970920806154, 'subsample': 0.982222866854528, 'colsample_bytree': 0.973461287916034, 'colsample_bylevel': 0.6284607173181854, 'gamma': 0.049658781085877256, 'reg_alpha': 0.2995608922712316, 'reg_lambda': 0.830069737867465}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:46:13,780]\u001b[0m Trial 194 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.12277047975859776, 'n_estimators': 93, 'max_depth': 7, 'min_child_weight': 6.903237162261374, 'subsample': 0.9205010532855734, 'colsample_bytree': 0.5359590515577267, 'colsample_bylevel': 0.615945080733763, 'gamma': 0.06184699442755815, 'reg_alpha': 0.3016342088420755, 'reg_lambda': 0.8055452042900555}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:46:13] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:46:13] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:46:13,955]\u001b[0m Trial 195 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.11288207170363665, 'n_estimators': 92, 'max_depth': 7, 'min_child_weight': 6.536228373094364, 'subsample': 0.9716921000583462, 'colsample_bytree': 0.5264279270587409, 'colsample_bylevel': 0.8670974915243577, 'gamma': 0.09324866408696114, 'reg_alpha': 0.34850311531312983, 'reg_lambda': 0.753460357622723}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:46:14,114]\u001b[0m Trial 196 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.09021041234060881, 'n_estimators': 88, 'max_depth': 9, 'min_child_weight': 7.360504004569268, 'subsample': 0.9433520868585663, 'colsample_bytree': 0.9470892804197606, 'colsample_bylevel': 0.9829113789751227, 'gamma': 0.06887146230466998, 'reg_alpha': 0.2957321459230633, 'reg_lambda': 0.9745206164324398}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:46:13] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:46:14] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:46:14,267]\u001b[0m Trial 197 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.07966068506420793, 'n_estimators': 85, 'max_depth': 7, 'min_child_weight': 3.4174584326943402, 'subsample': 0.940853390969801, 'colsample_bytree': 0.9218411485060836, 'colsample_bylevel': 0.6372875594919574, 'gamma': 0.07203230808965949, 'reg_alpha': 0.1675349936312906, 'reg_lambda': 0.8715330305436799}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:46:14,424]\u001b[0m Trial 198 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.11743496084820784, 'n_estimators': 85, 'max_depth': 7, 'min_child_weight': 3.6030846834716264, 'subsample': 0.978402080273354, 'colsample_bytree': 0.9276939222634383, 'colsample_bylevel': 0.6504530327717215, 'gamma': 0.40794759031120076, 'reg_alpha': 0.32883112402384373, 'reg_lambda': 0.9401483193747355}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:46:14] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:46:14] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:46:14,583]\u001b[0m Trial 199 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.13261626144641686, 'n_estimators': 85, 'max_depth': 7, 'min_child_weight': 3.422629996475834, 'subsample': 0.926441090241405, 'colsample_bytree': 0.8997766156814626, 'colsample_bylevel': 0.5126252265150538, 'gamma': 0.4167724162108981, 'reg_alpha': 0.20947667295804492, 'reg_lambda': 0.8864113554696387}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:46:14,738]\u001b[0m Trial 200 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.14415691651028978, 'n_estimators': 85, 'max_depth': 8, 'min_child_weight': 3.8781611326409995, 'subsample': 0.9605487742477189, 'colsample_bytree': 0.8848055857204109, 'colsample_bylevel': 0.8901936413897136, 'gamma': 0.4176270728664585, 'reg_alpha': 0.21895639002038017, 'reg_lambda': 0.9982255365524513}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:46:14] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:46:14] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:46:14,895]\u001b[0m Trial 201 finished with value: 0.8285714285714286 and parameters: {'booster': 'gblinear', 'learning_rate': 0.04041246047337192, 'n_estimators': 82, 'max_depth': 6, 'min_child_weight': 2.883975811066236, 'subsample': 0.8026674311127305, 'colsample_bytree': 0.8235035002595207, 'colsample_bylevel': 0.7466932231404009, 'gamma': 0.4181608504257599, 'reg_alpha': 0.17682926151687323, 'reg_lambda': 0.4964226836954213}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:46:15,047]\u001b[0m Trial 202 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.04167209669099519, 'n_estimators': 81, 'max_depth': 6, 'min_child_weight': 3.0365853548715425, 'subsample': 0.6831658880904171, 'colsample_bytree': 0.8002582969116994, 'colsample_bylevel': 0.7867791147661691, 'gamma': 0.12674696667893084, 'reg_alpha': 0.23845481129619564, 'reg_lambda': 0.2906484211240771}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:46:14] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:46:15] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:46:15,213]\u001b[0m Trial 203 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.048555098600670886, 'n_estimators': 95, 'max_depth': 4, 'min_child_weight': 3.457494450804389, 'subsample': 0.8464933332599668, 'colsample_bytree': 0.9544606230311209, 'colsample_bylevel': 0.6840655602319999, 'gamma': 0.047622417239776546, 'reg_alpha': 0.24065597836220426, 'reg_lambda': 0.9182855814719895}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:46:15,385]\u001b[0m Trial 204 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.09332168940064625, 'n_estimators': 98, 'max_depth': 7, 'min_child_weight': 3.9946049455969006, 'subsample': 0.6042312460460858, 'colsample_bytree': 0.7097306827801143, 'colsample_bylevel': 0.9466871782350958, 'gamma': 0.053601240648408544, 'reg_alpha': 0.2250986662155011, 'reg_lambda': 0.25962731765130487}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:46:15] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:46:15] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:46:15,555]\u001b[0m Trial 205 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.09245017988888382, 'n_estimators': 98, 'max_depth': 10, 'min_child_weight': 4.72273688929344, 'subsample': 0.5776462754425145, 'colsample_bytree': 0.5973246779072863, 'colsample_bylevel': 0.8584761725954414, 'gamma': 0.28726506816968367, 'reg_alpha': 0.2251359212508951, 'reg_lambda': 0.1788304323469603}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:46:15,705]\u001b[0m Trial 206 finished with value: 0.6249999999999999 and parameters: {'booster': 'gblinear', 'learning_rate': 0.09884471068626641, 'n_estimators': 81, 'max_depth': 4, 'min_child_weight': 2.8025071725528936, 'subsample': 0.5549800428894734, 'colsample_bytree': 0.5904623105014479, 'colsample_bylevel': 0.6164977460061145, 'gamma': 0.3012493900036935, 'reg_alpha': 0.17637337836785788, 'reg_lambda': 0.12492526890928762}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:46:15] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:46:15] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:46:15,874]\u001b[0m Trial 207 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.1061227763837379, 'n_estimators': 98, 'max_depth': 10, 'min_child_weight': 1.1639501437538706, 'subsample': 0.8980495342749336, 'colsample_bytree': 0.9866791920281929, 'colsample_bylevel': 0.9859214990649485, 'gamma': 0.11185233647720892, 'reg_alpha': 0.28129509153477733, 'reg_lambda': 0.4028800550954039}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:46:16,023]\u001b[0m Trial 208 finished with value: 0.3333333333333333 and parameters: {'booster': 'gblinear', 'learning_rate': 0.04559373924470849, 'n_estimators': 81, 'max_depth': 9, 'min_child_weight': 3.702077388674554, 'subsample': 0.6455551998859094, 'colsample_bytree': 0.9364051341696802, 'colsample_bylevel': 0.67135062522996, 'gamma': 0.18294743783088477, 'reg_alpha': 0.3652334581467684, 'reg_lambda': 0.8516656582165338}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:46:15] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:46:16] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:46:16,183]\u001b[0m Trial 209 finished with value: 0.3333333333333333 and parameters: {'booster': 'gblinear', 'learning_rate': 0.016916779909451975, 'n_estimators': 91, 'max_depth': 5, 'min_child_weight': 1.3461794714458737, 'subsample': 0.9002085433033663, 'colsample_bytree': 0.9930364402889099, 'colsample_bylevel': 0.8008775371176623, 'gamma': 0.19780979940649326, 'reg_alpha': 0.8676684821207598, 'reg_lambda': 0.6153472868686378}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:46:16,345]\u001b[0m Trial 210 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.011660355649905259, 'n_estimators': 91, 'max_depth': 3, 'min_child_weight': 6.599912756718056, 'subsample': 0.9117543078509255, 'colsample_bytree': 0.6803369766176982, 'colsample_bylevel': 0.5606210536036094, 'gamma': 0.20846656215504852, 'reg_alpha': 0.15418908625249866, 'reg_lambda': 0.46395574034537673}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:46:16] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:46:16] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:46:16,517]\u001b[0m Trial 211 finished with value: 0.8285714285714286 and parameters: {'booster': 'gblinear', 'learning_rate': 0.04429341786245755, 'n_estimators': 99, 'max_depth': 7, 'min_child_weight': 3.068666601373093, 'subsample': 0.5978156216130813, 'colsample_bytree': 0.8449246664381417, 'colsample_bylevel': 0.5271794183523979, 'gamma': 0.18847703226543067, 'reg_alpha': 0.1519766080740632, 'reg_lambda': 0.9547112167002442}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:46:16,688]\u001b[0m Trial 212 finished with value: 0.8285714285714286 and parameters: {'booster': 'gblinear', 'learning_rate': 0.060349094276255615, 'n_estimators': 99, 'max_depth': 5, 'min_child_weight': 1.0539184308402352, 'subsample': 0.6073825428727756, 'colsample_bytree': 0.8595814500296475, 'colsample_bylevel': 0.5391314104089068, 'gamma': 0.1201006255448001, 'reg_alpha': 0.11874097170851995, 'reg_lambda': 0.5901926466933192}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:46:16] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:46:16] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:46:16,856]\u001b[0m Trial 213 finished with value: 0.8285714285714286 and parameters: {'booster': 'gblinear', 'learning_rate': 0.14413821152718748, 'n_estimators': 97, 'max_depth': 6, 'min_child_weight': 5.500356866683745, 'subsample': 0.6957718491425131, 'colsample_bytree': 0.7901266613671871, 'colsample_bylevel': 0.9320066341741189, 'gamma': 0.14212049472689042, 'reg_alpha': 0.20759043384807815, 'reg_lambda': 0.33532909786057735}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:46:17,011]\u001b[0m Trial 214 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.06994980984996832, 'n_estimators': 87, 'max_depth': 6, 'min_child_weight': 1.0265145044397461, 'subsample': 0.927031929121527, 'colsample_bytree': 0.9752841477941638, 'colsample_bylevel': 0.9669782380331374, 'gamma': 0.14448305655296453, 'reg_alpha': 0.27166787835106654, 'reg_lambda': 0.3875987044153558}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:46:16] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:46:17] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:46:17,187]\u001b[0m Trial 215 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.10024286641447679, 'n_estimators': 99, 'max_depth': 4, 'min_child_weight': 4.224046918457066, 'subsample': 0.9512822351218192, 'colsample_bytree': 0.9104932626842025, 'colsample_bylevel': 0.6703570132919874, 'gamma': 0.10005296005028826, 'reg_alpha': 0.253883154200431, 'reg_lambda': 0.906045676347204}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:46:17,513]\u001b[0m Trial 217 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.1289432482380743, 'n_estimators': 99, 'max_depth': 6, 'min_child_weight': 1.2987898988212638, 'subsample': 0.873901427606731, 'colsample_bytree': 0.8698824480388496, 'colsample_bylevel': 0.5464500721986199, 'gamma': 0.11257466883460043, 'reg_alpha': 0.25676857043260753, 'reg_lambda': 0.7161272290934465}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:46:17,686]\u001b[0m Trial 218 finished with value: 0.6249999999999999 and parameters: {'booster': 'gblinear', 'learning_rate': 0.08498965704771584, 'n_estimators': 100, 'max_depth': 4, 'min_child_weight': 3.5623130643515224, 'subsample': 0.7638364091419421, 'colsample_bytree': 0.9253613023988726, 'colsample_bylevel': 0.9101704125412742, 'gamma': 0.018965541016263486, 'reg_alpha': 0.1325053400016012, 'reg_lambda': 0.7007708251882206}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:46:17] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:46:17] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:46:17,854]\u001b[0m Trial 219 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.13785749960383836, 'n_estimators': 90, 'max_depth': 5, 'min_child_weight': 1.5549970764875582, 'subsample': 0.9979572256219582, 'colsample_bytree': 0.6664272654726072, 'colsample_bylevel': 0.5014779609835461, 'gamma': 0.1567040667531641, 'reg_alpha': 0.2563797599178469, 'reg_lambda': 0.31818117218884456}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:46:18,019]\u001b[0m Trial 220 finished with value: 0.8285714285714286 and parameters: {'booster': 'gblinear', 'learning_rate': 0.05336729570224921, 'n_estimators': 94, 'max_depth': 8, 'min_child_weight': 3.8101765355058936, 'subsample': 0.6224834235630253, 'colsample_bytree': 0.9567701902619017, 'colsample_bylevel': 0.927859775018741, 'gamma': 0.17679576896386898, 'reg_alpha': 0.09334773527250106, 'reg_lambda': 0.7387876092176414}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:46:17] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:46:18] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:46:18,176]\u001b[0m Trial 221 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.06424338285632274, 'n_estimators': 87, 'max_depth': 6, 'min_child_weight': 5.035281248706533, 'subsample': 0.8858055278812594, 'colsample_bytree': 0.7943245459089141, 'colsample_bylevel': 0.9466914083782622, 'gamma': 0.07918551298659404, 'reg_alpha': 0.28000464598835284, 'reg_lambda': 0.5257378670015154}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:46:18,343]\u001b[0m Trial 222 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.09503863468175477, 'n_estimators': 96, 'max_depth': 8, 'min_child_weight': 4.90158819455698, 'subsample': 0.7165812962386977, 'colsample_bytree': 0.5492938172289223, 'colsample_bylevel': 0.5646911169197614, 'gamma': 0.007786898670797078, 'reg_alpha': 0.2904836339403403, 'reg_lambda': 0.8127526694585046}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:46:18] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:46:18] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:46:18,514]\u001b[0m Trial 223 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.1504845797390316, 'n_estimators': 98, 'max_depth': 8, 'min_child_weight': 5.576353523421216, 'subsample': 0.742606000371556, 'colsample_bytree': 0.9653055434650085, 'colsample_bylevel': 0.5362541051939089, 'gamma': 0.13982066763819215, 'reg_alpha': 0.2394021340709062, 'reg_lambda': 0.4120309457314386}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:46:18,683]\u001b[0m Trial 224 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.14930436054810126, 'n_estimators': 98, 'max_depth': 6, 'min_child_weight': 5.652074300304851, 'subsample': 0.7438270654414637, 'colsample_bytree': 0.965145124893212, 'colsample_bylevel': 0.7368755395282343, 'gamma': 0.135951799537878, 'reg_alpha': 0.2389880250822204, 'reg_lambda': 0.4303236453120142}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:46:18] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:46:18] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:46:18,846]\u001b[0m Trial 225 finished with value: 0.8285714285714286 and parameters: {'booster': 'gblinear', 'learning_rate': 0.05839366410189682, 'n_estimators': 92, 'max_depth': 8, 'min_child_weight': 3.384223267377328, 'subsample': 0.9355412017909608, 'colsample_bytree': 0.8915402561750111, 'colsample_bylevel': 0.5325156761920765, 'gamma': 0.4316624759045678, 'reg_alpha': 0.14983885105604844, 'reg_lambda': 0.159462601047259}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:46:19,027]\u001b[0m Trial 226 finished with value: 0.3333333333333333 and parameters: {'booster': 'dart', 'learning_rate': 0.1123447406504411, 'n_estimators': 86, 'max_depth': 7, 'min_child_weight': 5.368505635056905, 'subsample': 0.7258173473398992, 'colsample_bytree': 0.7718085692820563, 'colsample_bylevel': 0.9818583478681413, 'gamma': 0.1314911583462773, 'reg_alpha': 0.2920000011902048, 'reg_lambda': 0.27857188506291786}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:46:19,201]\u001b[0m Trial 227 finished with value: 0.8285714285714286 and parameters: {'booster': 'gbtree', 'learning_rate': 0.01691022808294391, 'n_estimators': 97, 'max_depth': 5, 'min_child_weight': 2.185401493220159, 'subsample': 0.8780073287814372, 'colsample_bytree': 0.5565604599053348, 'colsample_bylevel': 0.587817460277741, 'gamma': 0.027396071707857652, 'reg_alpha': 0.25965259335324165, 'reg_lambda': 0.768215306501652}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:46:19,370]\u001b[0m Trial 228 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.0773243794220568, 'n_estimators': 97, 'max_depth': 7, 'min_child_weight': 5.23265148284744, 'subsample': 0.8960157418616811, 'colsample_bytree': 0.7670050196129676, 'colsample_bylevel': 0.9697540743077708, 'gamma': 0.059634974058063235, 'reg_alpha': 0.21677099872261385, 'reg_lambda': 0.32337611190037185}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:46:19] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:46:19] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:46:19,523]\u001b[0m Trial 229 finished with value: 0.8285714285714286 and parameters: {'booster': 'gblinear', 'learning_rate': 0.04929088201531348, 'n_estimators': 83, 'max_depth': 4, 'min_child_weight': 3.243345510460857, 'subsample': 0.6101267449702188, 'colsample_bytree': 0.9443015805817396, 'colsample_bylevel': 0.5521232868451345, 'gamma': 0.09740412050238112, 'reg_alpha': 0.07758502622916551, 'reg_lambda': 0.643553512111602}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:46:19,681]\u001b[0m Trial 230 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.10803781649280697, 'n_estimators': 88, 'max_depth': 5, 'min_child_weight': 6.197920130898604, 'subsample': 0.8121373142333487, 'colsample_bytree': 0.6444036917743939, 'colsample_bylevel': 0.8931421021764785, 'gamma': 0.24771597681199287, 'reg_alpha': 0.19744228373054498, 'reg_lambda': 0.5807985100508445}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:46:19] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:46:19] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:46:19,847]\u001b[0m Trial 231 finished with value: 0.8285714285714286 and parameters: {'booster': 'gblinear', 'learning_rate': 0.05691440083753009, 'n_estimators': 95, 'max_depth': 8, 'min_child_weight': 3.804084701266986, 'subsample': 0.6692583607572218, 'colsample_bytree': 0.9544568880136053, 'colsample_bylevel': 0.5575197888205264, 'gamma': 0.1089326545466966, 'reg_alpha': 0.17760689523093842, 'reg_lambda': 0.677474574992405}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:46:20,014]\u001b[0m Trial 232 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.14032666481566644, 'n_estimators': 96, 'max_depth': 3, 'min_child_weight': 5.706576322661138, 'subsample': 0.8665543449367749, 'colsample_bytree': 0.8040706947521546, 'colsample_bylevel': 0.5444114063093204, 'gamma': 0.14467609951346608, 'reg_alpha': 0.2271853433994307, 'reg_lambda': 0.4533591902315416}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:46:19] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:46:20] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:46:20,182]\u001b[0m Trial 233 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.11506667922037138, 'n_estimators': 96, 'max_depth': 6, 'min_child_weight': 3.6884440451110048, 'subsample': 0.922160611079338, 'colsample_bytree': 0.9455173814600543, 'colsample_bylevel': 0.6318089514019619, 'gamma': 0.12184545939063383, 'reg_alpha': 0.31306771509954207, 'reg_lambda': 0.36006737194632066}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:46:20,347]\u001b[0m Trial 234 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.11956189914733104, 'n_estimators': 95, 'max_depth': 6, 'min_child_weight': 4.007209887661499, 'subsample': 0.9656759131039261, 'colsample_bytree': 0.9718498794595961, 'colsample_bylevel': 0.6939576580100785, 'gamma': 0.26672499670595384, 'reg_alpha': 0.32072813910250375, 'reg_lambda': 0.4135679019955996}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:46:20] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:46:20] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:46:20,512]\u001b[0m Trial 235 finished with value: 0.6249999999999999 and parameters: {'booster': 'gblinear', 'learning_rate': 0.06693413622062394, 'n_estimators': 93, 'max_depth': 6, 'min_child_weight': 1.1670005059200093, 'subsample': 0.9305094829904654, 'colsample_bytree': 0.909020846074079, 'colsample_bylevel': 0.643068101967951, 'gamma': 0.3570549598748646, 'reg_alpha': 0.33356602591225964, 'reg_lambda': 0.9709321275267608}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:46:20,675]\u001b[0m Trial 236 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.13166128203813535, 'n_estimators': 92, 'max_depth': 5, 'min_child_weight': 5.883483920016566, 'subsample': 0.721303076078866, 'colsample_bytree': 0.7489750983923602, 'colsample_bylevel': 0.5955831060521332, 'gamma': 0.45499878979762326, 'reg_alpha': 0.31880650338310756, 'reg_lambda': 0.8893071961448712}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:46:20] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:46:20] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:46:20,829]\u001b[0m Trial 237 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.13090797817007466, 'n_estimators': 84, 'max_depth': 7, 'min_child_weight': 7.686735007950715, 'subsample': 0.9735166644878278, 'colsample_bytree': 0.529360830506109, 'colsample_bylevel': 0.8809486658326253, 'gamma': 0.42458837079484074, 'reg_alpha': 0.33289397280053423, 'reg_lambda': 0.9372568626073976}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:46:20,982]\u001b[0m Trial 238 finished with value: 0.3333333333333333 and parameters: {'booster': 'gblinear', 'learning_rate': 0.13542723800718975, 'n_estimators': 84, 'max_depth': 7, 'min_child_weight': 7.392511591405297, 'subsample': 0.9397939722645894, 'colsample_bytree': 0.9241318897936031, 'colsample_bylevel': 0.8840803193641693, 'gamma': 0.06994220107575014, 'reg_alpha': 0.36074904204373065, 'reg_lambda': 0.8813935282344182}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:46:20] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:46:21] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:46:21,136]\u001b[0m Trial 239 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.12464109741610521, 'n_estimators': 85, 'max_depth': 10, 'min_child_weight': 7.664507374222578, 'subsample': 0.983028958289946, 'colsample_bytree': 0.897909713253985, 'colsample_bylevel': 0.8537410216011897, 'gamma': 0.04355912440011496, 'reg_alpha': 0.2045419650657564, 'reg_lambda': 0.9448471772982309}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:46:21,300]\u001b[0m Trial 240 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.0823479692043454, 'n_estimators': 92, 'max_depth': 7, 'min_child_weight': 1.379854125989746, 'subsample': 0.9464348096116776, 'colsample_bytree': 0.9415690676121901, 'colsample_bylevel': 0.6433518845021837, 'gamma': 0.43864506306507534, 'reg_alpha': 0.24807128760740157, 'reg_lambda': 0.2711745796847971}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:46:21] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:46:21] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:46:21,467]\u001b[0m Trial 241 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.08987874292787384, 'n_estimators': 94, 'max_depth': 7, 'min_child_weight': 4.041314463958449, 'subsample': 0.9162303034514959, 'colsample_bytree': 0.9329697988277785, 'colsample_bylevel': 0.6513783365084211, 'gamma': 0.4455352234766123, 'reg_alpha': 0.270614462509973, 'reg_lambda': 0.6937217415340067}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:46:21,631]\u001b[0m Trial 242 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.10447000202668413, 'n_estimators': 92, 'max_depth': 8, 'min_child_weight': 1.2152975430643354, 'subsample': 0.9068221564899153, 'colsample_bytree': 0.9575670464033684, 'colsample_bylevel': 0.6238710338532675, 'gamma': 0.04299249506540538, 'reg_alpha': 0.22106763917032216, 'reg_lambda': 0.4404353598150099}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:46:21] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:46:21] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:46:21,799]\u001b[0m Trial 243 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.14732190893104116, 'n_estimators': 95, 'max_depth': 7, 'min_child_weight': 4.17896432556721, 'subsample': 0.9523622771077028, 'colsample_bytree': 0.7835585424433995, 'colsample_bylevel': 0.9157608872491394, 'gamma': 0.12931717991270916, 'reg_alpha': 0.2889113885679656, 'reg_lambda': 0.917465344825469}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:46:21,968]\u001b[0m Trial 244 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.09113706919918181, 'n_estimators': 97, 'max_depth': 7, 'min_child_weight': 1.0783463489868836, 'subsample': 0.9622662370269097, 'colsample_bytree': 0.7800467551404109, 'colsample_bylevel': 0.8384852129470842, 'gamma': 0.1524547188457113, 'reg_alpha': 0.3058938369668807, 'reg_lambda': 0.968900840628239}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:46:21] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:46:22] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:46:22,140]\u001b[0m Trial 245 finished with value: 0.3333333333333333 and parameters: {'booster': 'gblinear', 'learning_rate': 0.10924041879933324, 'n_estimators': 99, 'max_depth': 5, 'min_child_weight': 4.336798766361706, 'subsample': 0.6137749200534477, 'colsample_bytree': 0.5217916780157295, 'colsample_bylevel': 0.923526406166877, 'gamma': 0.05415664638502258, 'reg_alpha': 0.4910381697529943, 'reg_lambda': 0.7248054932325988}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:46:22,310]\u001b[0m Trial 246 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.09297315782024197, 'n_estimators': 98, 'max_depth': 10, 'min_child_weight': 6.881182718375865, 'subsample': 0.8423889791385084, 'colsample_bytree': 0.9981237811409233, 'colsample_bylevel': 0.8649550967287797, 'gamma': 0.08926800187248408, 'reg_alpha': 0.23991408842549233, 'reg_lambda': 0.988599334372304}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:46:22] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:46:22] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:46:22,478]\u001b[0m Trial 247 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.07882282372875968, 'n_estimators': 95, 'max_depth': 7, 'min_child_weight': 9.011592765377243, 'subsample': 0.6287328784458655, 'colsample_bytree': 0.8160249662666852, 'colsample_bylevel': 0.518341910855173, 'gamma': 0.3890887443177577, 'reg_alpha': 0.2117143365026297, 'reg_lambda': 0.30309064422702}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:46:22,645]\u001b[0m Trial 248 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.08009979710170148, 'n_estimators': 96, 'max_depth': 8, 'min_child_weight': 4.238241710465117, 'subsample': 0.7068130357523451, 'colsample_bytree': 0.5033718078018721, 'colsample_bylevel': 0.8253753483422094, 'gamma': 0.07976381677162862, 'reg_alpha': 0.2616032864157827, 'reg_lambda': 0.9555823217449888}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:46:22] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:46:22] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:46:22,795]\u001b[0m Trial 249 finished with value: 0.8285714285714286 and parameters: {'booster': 'gblinear', 'learning_rate': 0.10220334576362584, 'n_estimators': 81, 'max_depth': 5, 'min_child_weight': 1.529802551274331, 'subsample': 0.7824049162410446, 'colsample_bytree': 0.7484682953763302, 'colsample_bylevel': 0.938516018379947, 'gamma': 0.13483797570758002, 'reg_alpha': 0.18771295898664134, 'reg_lambda': 0.33023073662458285}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:46:22,954]\u001b[0m Trial 250 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.09676323791375241, 'n_estimators': 89, 'max_depth': 5, 'min_child_weight': 1.736987321349138, 'subsample': 0.6404914064097569, 'colsample_bytree': 0.8291596317339237, 'colsample_bylevel': 0.6135553568185963, 'gamma': 0.22662465079165228, 'reg_alpha': 0.27756318891115267, 'reg_lambda': 0.4635461028641496}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:46:22] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:46:22] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:46:23,128]\u001b[0m Trial 251 finished with value: 0.8285714285714286 and parameters: {'booster': 'gblinear', 'learning_rate': 0.0626629362498971, 'n_estimators': 100, 'max_depth': 3, 'min_child_weight': 5.461898016544782, 'subsample': 0.9882118600560483, 'colsample_bytree': 0.7309073988329491, 'colsample_bylevel': 0.527067485627531, 'gamma': 0.4635336470138463, 'reg_alpha': 0.18410638443913055, 'reg_lambda': 0.36735758809064584}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:46:23,289]\u001b[0m Trial 252 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.12537764212532648, 'n_estimators': 90, 'max_depth': 5, 'min_child_weight': 1.9854318658902224, 'subsample': 0.6504135069599444, 'colsample_bytree': 0.5430621765216195, 'colsample_bylevel': 0.6015071118718391, 'gamma': 0.1632528322239006, 'reg_alpha': 0.26644203183241044, 'reg_lambda': 0.7567861454211846}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:46:23] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:46:23] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:46:23,462]\u001b[0m Trial 253 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.06602987248907508, 'n_estimators': 99, 'max_depth': 3, 'min_child_weight': 1.01320760499243, 'subsample': 0.8301779655369218, 'colsample_bytree': 0.8163398135941451, 'colsample_bylevel': 0.6898518897844933, 'gamma': 0.21418535273557585, 'reg_alpha': 0.2221278105752835, 'reg_lambda': 0.5286670230639942}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:46:23,635]\u001b[0m Trial 254 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.07055176490392995, 'n_estimators': 99, 'max_depth': 3, 'min_child_weight': 1.299621348542631, 'subsample': 0.9965348343966473, 'colsample_bytree': 0.7963967699997643, 'colsample_bylevel': 0.5419287161806481, 'gamma': 0.06138514700894558, 'reg_alpha': 0.22409884725968193, 'reg_lambda': 0.33507319546320274}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:46:23] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:46:23] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:46:23,811]\u001b[0m Trial 255 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.06966520355047807, 'n_estimators': 99, 'max_depth': 4, 'min_child_weight': 6.652218288570627, 'subsample': 0.8364055377713987, 'colsample_bytree': 0.5992984380220175, 'colsample_bylevel': 0.6738551969970135, 'gamma': 0.08703855034890953, 'reg_alpha': 0.3093684450460296, 'reg_lambda': 0.9874142352754861}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:46:24,003]\u001b[0m Trial 256 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.06997814669910654, 'n_estimators': 100, 'max_depth': 4, 'min_child_weight': 7.165779156705394, 'subsample': 0.8389180441981096, 'colsample_bytree': 0.9798065821372121, 'colsample_bylevel': 0.6764593287014665, 'gamma': 0.08697141374372422, 'reg_alpha': 0.31143094845898317, 'reg_lambda': 0.997793459017582}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:46:23] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:46:24] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:46:24,180]\u001b[0m Trial 257 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.0998383107389934, 'n_estimators': 100, 'max_depth': 4, 'min_child_weight': 7.138256161661865, 'subsample': 0.8349918102211015, 'colsample_bytree': 0.5797396377988793, 'colsample_bylevel': 0.6727276771061177, 'gamma': 0.10449116625891677, 'reg_alpha': 0.24754163658840855, 'reg_lambda': 0.8566452971474319}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:46:24,357]\u001b[0m Trial 258 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.09911377060447259, 'n_estimators': 100, 'max_depth': 4, 'min_child_weight': 7.193633824500963, 'subsample': 0.85594418659073, 'colsample_bytree': 0.5868328545899383, 'colsample_bylevel': 0.6754105192393035, 'gamma': 0.10406261284444185, 'reg_alpha': 0.24921086982144508, 'reg_lambda': 0.9179497362501294}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:46:24] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:46:24] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:46:24,536]\u001b[0m Trial 259 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.08811852053087868, 'n_estimators': 100, 'max_depth': 4, 'min_child_weight': 7.100253635579201, 'subsample': 0.8568210986246981, 'colsample_bytree': 0.5891241881311851, 'colsample_bylevel': 0.6626340933774822, 'gamma': 0.10460499933269363, 'reg_alpha': 0.25010765634743143, 'reg_lambda': 0.8327919867647556}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:46:24,711]\u001b[0m Trial 260 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.0907333636741192, 'n_estimators': 100, 'max_depth': 4, 'min_child_weight': 6.989966314203305, 'subsample': 0.8539702178118371, 'colsample_bytree': 0.5891828320026347, 'colsample_bylevel': 0.6286851054362502, 'gamma': 0.09816846446418373, 'reg_alpha': 0.2542857089579182, 'reg_lambda': 0.8273966784476372}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:46:24] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:46:24] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:46:24,875]\u001b[0m Trial 261 finished with value: 0.6249999999999999 and parameters: {'booster': 'gblinear', 'learning_rate': 0.11824314253529197, 'n_estimators': 87, 'max_depth': 3, 'min_child_weight': 5.842950811674581, 'subsample': 0.6681133377542846, 'colsample_bytree': 0.7589860344579508, 'colsample_bylevel': 0.8986825081206123, 'gamma': 0.2148051822701429, 'reg_alpha': 0.12185602337976496, 'reg_lambda': 0.39068026031123493}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:46:25,056]\u001b[0m Trial 262 finished with value: 0.3333333333333333 and parameters: {'booster': 'gbtree', 'learning_rate': 0.08485241579667868, 'n_estimators': 98, 'max_depth': 9, 'min_child_weight': 3.0090618674821936, 'subsample': 0.5848439996638247, 'colsample_bytree': 0.6126129259048091, 'colsample_bylevel': 0.7789348953393175, 'gamma': 0.04752650494832499, 'reg_alpha': 0.2239995250724134, 'reg_lambda': 0.1776108415352026}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:46:25,225]\u001b[0m Trial 263 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.07500650529654453, 'n_estimators': 96, 'max_depth': 5, 'min_child_weight': 4.847917212890833, 'subsample': 0.8902207335742911, 'colsample_bytree': 0.7850737609817275, 'colsample_bylevel': 0.7534942970443654, 'gamma': 0.16064529405002753, 'reg_alpha': 0.2797589731870793, 'reg_lambda': 0.3023273905623557}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:46:25] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:46:25,413]\u001b[0m Trial 264 finished with value: 0.8285714285714286 and parameters: {'booster': 'dart', 'learning_rate': 0.010405447673942442, 'n_estimators': 88, 'max_depth': 3, 'min_child_weight': 1.8085962683181804, 'subsample': 0.8808404335285946, 'colsample_bytree': 0.5578745740102178, 'colsample_bylevel': 0.7354303707503389, 'gamma': 0.4736693894921434, 'reg_alpha': 0.2032785314458141, 'reg_lambda': 0.28408454760661744}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:46:25,575]\u001b[0m Trial 265 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.10355365535290856, 'n_estimators': 89, 'max_depth': 7, 'min_child_weight': 4.5388512840809705, 'subsample': 0.5634928244264001, 'colsample_bytree': 0.5191253015440221, 'colsample_bylevel': 0.5872607112859072, 'gamma': 0.013265291899486923, 'reg_alpha': 0.3433198291744569, 'reg_lambda': 0.35088364545355377}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:46:25] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:46:25] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:46:25,726]\u001b[0m Trial 266 finished with value: 0.3333333333333333 and parameters: {'booster': 'gblinear', 'learning_rate': 0.11312463033501603, 'n_estimators': 80, 'max_depth': 5, 'min_child_weight': 5.667531838489757, 'subsample': 0.6235911896785228, 'colsample_bytree': 0.7536724909556936, 'colsample_bylevel': 0.5493340559201678, 'gamma': 0.11499488109420755, 'reg_alpha': 0.5493752122007556, 'reg_lambda': 0.4329652145095598}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:46:25,898]\u001b[0m Trial 267 finished with value: 0.8285714285714286 and parameters: {'booster': 'gblinear', 'learning_rate': 0.043900144541032665, 'n_estimators': 98, 'max_depth': 7, 'min_child_weight': 4.534195625625605, 'subsample': 0.5610914439037575, 'colsample_bytree': 0.9627311968722081, 'colsample_bylevel': 0.6515366482390904, 'gamma': 0.2972792312783083, 'reg_alpha': 0.1600672805403906, 'reg_lambda': 0.4835025284502753}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:46:25] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:46:25] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:46:26,066]\u001b[0m Trial 268 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.09526997356007459, 'n_estimators': 93, 'max_depth': 7, 'min_child_weight': 6.777306392105308, 'subsample': 0.8461316851995684, 'colsample_bytree': 0.5333751574527177, 'colsample_bylevel': 0.5780737253651338, 'gamma': 0.036404588915431135, 'reg_alpha': 0.30664088713746807, 'reg_lambda': 0.8066773512345672}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:46:26,215]\u001b[0m Trial 269 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.05628497653885494, 'n_estimators': 80, 'max_depth': 3, 'min_child_weight': 4.172235949483778, 'subsample': 0.9343325922682137, 'colsample_bytree': 0.7399486101767891, 'colsample_bylevel': 0.7625899301957884, 'gamma': 0.45993626148568745, 'reg_alpha': 0.19799671749883233, 'reg_lambda': 0.7259897644196869}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:46:26] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:46:26] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:46:26,366]\u001b[0m Trial 270 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.08209488437580316, 'n_estimators': 80, 'max_depth': 5, 'min_child_weight': 5.838008271219916, 'subsample': 0.9562535794748119, 'colsample_bytree': 0.717520866696301, 'colsample_bylevel': 0.9941584161967735, 'gamma': 0.4394258917886406, 'reg_alpha': 0.2914428285603807, 'reg_lambda': 0.9043146595035513}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:46:26,529]\u001b[0m Trial 271 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.08504759792975072, 'n_estimators': 91, 'max_depth': 8, 'min_child_weight': 4.396048419415493, 'subsample': 0.8189787438561478, 'colsample_bytree': 0.7381810057237937, 'colsample_bylevel': 0.9728743551080133, 'gamma': 0.49334579868129536, 'reg_alpha': 0.1974939430386744, 'reg_lambda': 0.5659683963594946}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:46:26] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:46:26] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:46:26,693]\u001b[0m Trial 272 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.15582993400151984, 'n_estimators': 91, 'max_depth': 8, 'min_child_weight': 3.9500053648783853, 'subsample': 0.9637952911586968, 'colsample_bytree': 0.7244891333637762, 'colsample_bylevel': 0.6002516248557878, 'gamma': 0.449990697380238, 'reg_alpha': 0.34701436573910877, 'reg_lambda': 0.5574383269782252}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:46:26,850]\u001b[0m Trial 273 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.10594708677712472, 'n_estimators': 86, 'max_depth': 8, 'min_child_weight': 4.072125060356362, 'subsample': 0.6437763977888447, 'colsample_bytree': 0.7313723582530863, 'colsample_bylevel': 0.7109645280052308, 'gamma': 0.31374694227716216, 'reg_alpha': 0.20795927744798867, 'reg_lambda': 0.7982045758163845}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:46:26] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:46:26] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:46:27,023]\u001b[0m Trial 274 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.07806085512844783, 'n_estimators': 97, 'max_depth': 8, 'min_child_weight': 4.217999266544635, 'subsample': 0.6550939322999898, 'colsample_bytree': 0.7645391621977735, 'colsample_bylevel': 0.948273781016413, 'gamma': 0.46996702909769383, 'reg_alpha': 0.2300060551572963, 'reg_lambda': 0.7926869019244337}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:46:27,183]\u001b[0m Trial 275 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.11749977756887461, 'n_estimators': 86, 'max_depth': 6, 'min_child_weight': 5.069081390319844, 'subsample': 0.930474113981948, 'colsample_bytree': 0.6275745320578415, 'colsample_bylevel': 0.9613273476130101, 'gamma': 0.07168858860732448, 'reg_alpha': 0.27412450409502165, 'reg_lambda': 0.3744994214639669}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:46:27] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:46:27] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:46:27,343]\u001b[0m Trial 276 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.07555653740755265, 'n_estimators': 87, 'max_depth': 6, 'min_child_weight': 5.548604006963834, 'subsample': 0.826830421315077, 'colsample_bytree': 0.742920711924213, 'colsample_bylevel': 0.9940293055370161, 'gamma': 0.38748647705665384, 'reg_alpha': 0.27954515912464784, 'reg_lambda': 0.35101592864210424}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:46:27,514]\u001b[0m Trial 277 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.11988662708060208, 'n_estimators': 95, 'max_depth': 6, 'min_child_weight': 3.6941680077893277, 'subsample': 0.9240421272933821, 'colsample_bytree': 0.9462575153325375, 'colsample_bylevel': 0.7021722127295535, 'gamma': 0.2721535187741698, 'reg_alpha': 0.3221022009207323, 'reg_lambda': 0.36654454998007124}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:46:27] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:46:27] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:46:27,682]\u001b[0m Trial 278 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.11660458816887198, 'n_estimators': 95, 'max_depth': 6, 'min_child_weight': 3.6494721802969066, 'subsample': 0.9186235068410409, 'colsample_bytree': 0.9503873414058872, 'colsample_bylevel': 0.7012355929810616, 'gamma': 0.27074320431067384, 'reg_alpha': 0.31953487265851, 'reg_lambda': 0.39647339090863637}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:46:27,845]\u001b[0m Trial 279 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.11252490923155618, 'n_estimators': 85, 'max_depth': 7, 'min_child_weight': 3.5360267032928845, 'subsample': 0.9787650674422453, 'colsample_bytree': 0.9036506010520631, 'colsample_bylevel': 0.5735447385327737, 'gamma': 0.41108243579825665, 'reg_alpha': 0.3325235085581844, 'reg_lambda': 0.6772496482723811}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:46:27] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:46:27] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:46:28,007]\u001b[0m Trial 280 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.11205000936097013, 'n_estimators': 86, 'max_depth': 7, 'min_child_weight': 3.4766360764494877, 'subsample': 0.9435105619607866, 'colsample_bytree': 0.8803371600827022, 'colsample_bylevel': 0.8721985219772596, 'gamma': 0.41186952016605566, 'reg_alpha': 0.2723385769346477, 'reg_lambda': 0.7150021604308332}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:46:28,172]\u001b[0m Trial 281 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.09190865555935793, 'n_estimators': 92, 'max_depth': 8, 'min_child_weight': 3.7877229676005113, 'subsample': 0.9521518600921798, 'colsample_bytree': 0.8091785763281167, 'colsample_bylevel': 0.7811057375173196, 'gamma': 0.4000000834756965, 'reg_alpha': 0.26432334974625105, 'reg_lambda': 0.7773928157553331}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:46:28] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:46:28,366]\u001b[0m Trial 282 finished with value: 0.3333333333333333 and parameters: {'booster': 'dart', 'learning_rate': 0.09172484428289672, 'n_estimators': 92, 'max_depth': 9, 'min_child_weight': 4.701690713427217, 'subsample': 0.567583760056119, 'colsample_bytree': 0.8008081887750271, 'colsample_bylevel': 0.7839868832189847, 'gamma': 0.056710616426728463, 'reg_alpha': 0.23715532057147437, 'reg_lambda': 0.7836292797887471}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:46:28,522]\u001b[0m Trial 283 finished with value: 0.6249999999999999 and parameters: {'booster': 'gblinear', 'learning_rate': 0.1281049551827928, 'n_estimators': 84, 'max_depth': 7, 'min_child_weight': 1.448660237611785, 'subsample': 0.9764373876431985, 'colsample_bytree': 0.8928574815162189, 'colsample_bylevel': 0.8430545758912978, 'gamma': 0.3959814203565458, 'reg_alpha': 0.13115047811858885, 'reg_lambda': 0.2004540274178033}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:46:28] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:46:28] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:46:28,694]\u001b[0m Trial 284 finished with value: 0.8285714285714286 and parameters: {'booster': 'gblinear', 'learning_rate': 0.04683478655514465, 'n_estimators': 98, 'max_depth': 8, 'min_child_weight': 3.285834376575152, 'subsample': 0.7416169889434038, 'colsample_bytree': 0.6920272760829128, 'colsample_bylevel': 0.6580341479876952, 'gamma': 0.004430422167954067, 'reg_alpha': 0.16914116485468927, 'reg_lambda': 0.14735038679815785}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:46:28,870]\u001b[0m Trial 285 finished with value: 0.3333333333333333 and parameters: {'booster': 'gbtree', 'learning_rate': 0.08715224279308623, 'n_estimators': 98, 'max_depth': 10, 'min_child_weight': 4.78717041891386, 'subsample': 0.8729887415033393, 'colsample_bytree': 0.6989343087368091, 'colsample_bylevel': 0.6425965306878108, 'gamma': 0.05292003027149253, 'reg_alpha': 0.2299213322425147, 'reg_lambda': 0.23424246779786073}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:46:29,038]\u001b[0m Trial 286 finished with value: 0.6249999999999999 and parameters: {'booster': 'gblinear', 'learning_rate': 0.16948901051857346, 'n_estimators': 94, 'max_depth': 7, 'min_child_weight': 1.2321965477767143, 'subsample': 0.9064573128442699, 'colsample_bytree': 0.9351019282706482, 'colsample_bylevel': 0.913286249532499, 'gamma': 0.1279819881882498, 'reg_alpha': 0.10603534824419245, 'reg_lambda': 0.6884431772955375}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:46:28] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:46:29] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:46:29,199]\u001b[0m Trial 287 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.13305028515501907, 'n_estimators': 88, 'max_depth': 5, 'min_child_weight': 6.119532986443023, 'subsample': 0.8680615371151796, 'colsample_bytree': 0.5294848102356319, 'colsample_bylevel': 0.9993965232373715, 'gamma': 0.02556624276270011, 'reg_alpha': 0.29268717459204285, 'reg_lambda': 0.5926477762595881}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:46:29,357]\u001b[0m Trial 288 finished with value: 0.6249999999999999 and parameters: {'booster': 'gblinear', 'learning_rate': 0.06417318832626678, 'n_estimators': 87, 'max_depth': 9, 'min_child_weight': 6.809251719730003, 'subsample': 0.9966875749316533, 'colsample_bytree': 0.5123125931785055, 'colsample_bylevel': 0.9774655349156247, 'gamma': 0.07770444875796698, 'reg_alpha': 0.344087570276542, 'reg_lambda': 0.8505494761830777}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:46:29] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:46:29] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:46:29,525]\u001b[0m Trial 289 finished with value: 0.8285714285714286 and parameters: {'booster': 'gblinear', 'learning_rate': 0.03839881137349969, 'n_estimators': 94, 'max_depth': 6, 'min_child_weight': 6.095928974012138, 'subsample': 0.79881650294754, 'colsample_bytree': 0.5720724113884126, 'colsample_bylevel': 0.8556594044981964, 'gamma': 0.24041813969911746, 'reg_alpha': 0.1381063994580012, 'reg_lambda': 0.5853151391668701}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:46:29,698]\u001b[0m Trial 290 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.09727746713304983, 'n_estimators': 98, 'max_depth': 9, 'min_child_weight': 6.292763143953511, 'subsample': 0.7526478714492101, 'colsample_bytree': 0.5504810763899377, 'colsample_bylevel': 0.5802256115005815, 'gamma': 0.014957985123667215, 'reg_alpha': 0.2640938666538941, 'reg_lambda': 0.8563557141895544}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:46:29] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:46:29] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:46:29,872]\u001b[0m Trial 291 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.09534757606225, 'n_estimators': 98, 'max_depth': 10, 'min_child_weight': 6.349992501547571, 'subsample': 0.5276939075272755, 'colsample_bytree': 0.5653185274724699, 'colsample_bylevel': 0.683366293629752, 'gamma': 0.28627644762666515, 'reg_alpha': 0.18332243350595512, 'reg_lambda': 0.8181643003835195}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:46:30,045]\u001b[0m Trial 292 finished with value: 0.6249999999999999 and parameters: {'booster': 'gblinear', 'learning_rate': 0.09543133073951619, 'n_estimators': 97, 'max_depth': 10, 'min_child_weight': 6.424132445350006, 'subsample': 0.5129698841630949, 'colsample_bytree': 0.6777903427462244, 'colsample_bylevel': 0.9539315512688792, 'gamma': 0.284479736307647, 'reg_alpha': 0.14714363450703835, 'reg_lambda': 0.7494843689766574}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:46:29] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:46:30] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:46:30,197]\u001b[0m Trial 293 finished with value: 0.8285714285714286 and parameters: {'booster': 'gblinear', 'learning_rate': 0.054064025814018604, 'n_estimators': 82, 'max_depth': 6, 'min_child_weight': 3.8764319286561357, 'subsample': 0.5480864134216843, 'colsample_bytree': 0.5690264104050047, 'colsample_bylevel': 0.8931925072457363, 'gamma': 0.261337628929999, 'reg_alpha': 0.1806416818341327, 'reg_lambda': 0.4864903865030675}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:46:30,368]\u001b[0m Trial 294 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.10525076004695894, 'n_estimators': 97, 'max_depth': 10, 'min_child_weight': 6.529728833334233, 'subsample': 0.8947620179259956, 'colsample_bytree': 0.6014928632940417, 'colsample_bylevel': 0.95771870164689, 'gamma': 0.08340357502475546, 'reg_alpha': 0.3001894445946516, 'reg_lambda': 0.4588727472520836}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:46:30] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:46:30] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:46:30,519]\u001b[0m Trial 295 finished with value: 0.3333333333333333 and parameters: {'booster': 'gblinear', 'learning_rate': 0.08172630832447417, 'n_estimators': 80, 'max_depth': 9, 'min_child_weight': 6.833607636038725, 'subsample': 0.9876035958357622, 'colsample_bytree': 0.7102931036216142, 'colsample_bylevel': 0.8661184852591451, 'gamma': 0.06482991323318435, 'reg_alpha': 0.39086324791336724, 'reg_lambda': 0.8760209063022434}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:46:30,690]\u001b[0m Trial 296 finished with value: 0.6249999999999999 and parameters: {'booster': 'gblinear', 'learning_rate': 0.015111984930333158, 'n_estimators': 96, 'max_depth': 5, 'min_child_weight': 1.7056484868758452, 'subsample': 0.639204629108888, 'colsample_bytree': 0.8314462015645389, 'colsample_bylevel': 0.6187320450646445, 'gamma': 0.14688276649522197, 'reg_alpha': 0.2817988453250681, 'reg_lambda': 0.4719361746006509}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:46:30] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:46:30] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:46:30,844]\u001b[0m Trial 297 finished with value: 0.3333333333333333 and parameters: {'booster': 'gblinear', 'learning_rate': 0.1269460843506553, 'n_estimators': 83, 'max_depth': 6, 'min_child_weight': 7.70619025668929, 'subsample': 0.9653755373663664, 'colsample_bytree': 0.9173760784666022, 'colsample_bylevel': 0.8543168833482074, 'gamma': 0.11964486666371973, 'reg_alpha': 0.3767969152987979, 'reg_lambda': 0.9441002290264789}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:46:31,007]\u001b[0m Trial 298 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.019998548850735914, 'n_estimators': 90, 'max_depth': 3, 'min_child_weight': 1.1465706846660653, 'subsample': 0.659098760418675, 'colsample_bytree': 0.8159565296408037, 'colsample_bylevel': 0.5338636032138476, 'gamma': 0.17299421084029104, 'reg_alpha': 0.2184472526574541, 'reg_lambda': 0.3390275077407045}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:46:30] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:46:31] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:46:31,163]\u001b[0m Trial 299 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.07933399905780836, 'n_estimators': 84, 'max_depth': 3, 'min_child_weight': 6.990711029143325, 'subsample': 0.9713518718452595, 'colsample_bytree': 0.7521036798884145, 'colsample_bylevel': 0.9830454717514673, 'gamma': 0.191459443266404, 'reg_alpha': 0.3054843059252592, 'reg_lambda': 0.8669438551507783}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:46:31,320]\u001b[0m Trial 300 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.12187391893539645, 'n_estimators': 85, 'max_depth': 3, 'min_child_weight': 6.681950916240802, 'subsample': 0.628099012867227, 'colsample_bytree': 0.8516332914843986, 'colsample_bylevel': 0.9007071252963016, 'gamma': 0.47866957526103215, 'reg_alpha': 0.33692117449887604, 'reg_lambda': 0.9027914730218147}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:46:31] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:46:31] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:46:31,477]\u001b[0m Trial 301 finished with value: 0.3333333333333333 and parameters: {'booster': 'gblinear', 'learning_rate': 0.0615711411266334, 'n_estimators': 85, 'max_depth': 9, 'min_child_weight': 6.900156154040396, 'subsample': 0.9836199795889298, 'colsample_bytree': 0.9215630302036427, 'colsample_bylevel': 0.6041562632237754, 'gamma': 0.07549611602014082, 'reg_alpha': 0.35273113884376056, 'reg_lambda': 0.9642512889123006}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:46:31,657]\u001b[0m Trial 302 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.06999685837671094, 'n_estimators': 99, 'max_depth': 5, 'min_child_weight': 1.585120343305377, 'subsample': 0.8355629948248331, 'colsample_bytree': 0.9818654127568455, 'colsample_bylevel': 0.6842937269600192, 'gamma': 0.08799268163097901, 'reg_alpha': 0.3167004128442312, 'reg_lambda': 0.9261054926286261}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:46:31] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:46:31] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:46:31,815]\u001b[0m Trial 303 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.12204275410851809, 'n_estimators': 85, 'max_depth': 7, 'min_child_weight': 7.479686988372993, 'subsample': 0.9924821125911563, 'colsample_bytree': 0.7275474335749123, 'colsample_bylevel': 0.5037361764062808, 'gamma': 0.42758364639561003, 'reg_alpha': 0.2134344018648115, 'reg_lambda': 0.8784966828095958}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:46:31,984]\u001b[0m Trial 304 finished with value: 0.3333333333333333 and parameters: {'booster': 'gblinear', 'learning_rate': 0.08405423008726103, 'n_estimators': 93, 'max_depth': 3, 'min_child_weight': 7.369594515388523, 'subsample': 0.9745131928176671, 'colsample_bytree': 0.9214717414609984, 'colsample_bylevel': 0.6319227966193844, 'gamma': 0.37347041671903314, 'reg_alpha': 0.41577753178587806, 'reg_lambda': 0.83855061863465}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:46:31] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:46:32] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:46:32,162]\u001b[0m Trial 305 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.10854437496034536, 'n_estimators': 100, 'max_depth': 5, 'min_child_weight': 7.192707578286242, 'subsample': 0.8550274854462572, 'colsample_bytree': 0.7734864180047352, 'colsample_bylevel': 0.5067805742330704, 'gamma': 0.20831595194933947, 'reg_alpha': 0.2334619426099774, 'reg_lambda': 0.40243087823786644}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:46:32,342]\u001b[0m Trial 306 finished with value: 0.8285714285714286 and parameters: {'booster': 'dart', 'learning_rate': 0.08813827811779398, 'n_estimators': 83, 'max_depth': 7, 'min_child_weight': 1.3749829323092737, 'subsample': 0.9159677189603551, 'colsample_bytree': 0.9313310222800534, 'colsample_bylevel': 0.6510261360234554, 'gamma': 0.4388431471788915, 'reg_alpha': 0.24955912795185042, 'reg_lambda': 0.2664870365049818}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:46:32,518]\u001b[0m Trial 307 finished with value: 0.3333333333333333 and parameters: {'booster': 'gbtree', 'learning_rate': 0.1358629452952599, 'n_estimators': 96, 'max_depth': 4, 'min_child_weight': 5.074018020881477, 'subsample': 0.8848117393862429, 'colsample_bytree': 0.5399747513341973, 'colsample_bylevel': 0.5617579231991983, 'gamma': 0.03151055820633747, 'reg_alpha': 0.2528482703792113, 'reg_lambda': 0.7454390557751568}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:46:32,682]\u001b[0m Trial 308 finished with value: 0.6249999999999999 and parameters: {'booster': 'gblinear', 'learning_rate': 0.1438244090643594, 'n_estimators': 90, 'max_depth': 5, 'min_child_weight': 5.241234549443556, 'subsample': 0.9098830256960102, 'colsample_bytree': 0.7922196553251016, 'colsample_bylevel': 0.551181414739928, 'gamma': 0.19269982436519204, 'reg_alpha': 0.11365330935560777, 'reg_lambda': 0.3053254754561543}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:46:32] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:46:32] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:46:32,849]\u001b[0m Trial 309 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.025380468006395148, 'n_estimators': 94, 'max_depth': 7, 'min_child_weight': 4.063556476717203, 'subsample': 0.9462231684088271, 'colsample_bytree': 0.9400358522177682, 'colsample_bylevel': 0.6421430810974927, 'gamma': 0.04115609510760463, 'reg_alpha': 0.2504347852267159, 'reg_lambda': 0.6573917384862414}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:46:33,006]\u001b[0m Trial 310 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.057678512278453546, 'n_estimators': 85, 'max_depth': 8, 'min_child_weight': 6.496642275735768, 'subsample': 0.6157265330138395, 'colsample_bytree': 0.7198311276371713, 'colsample_bylevel': 0.7658510023820745, 'gamma': 0.07065256910624831, 'reg_alpha': 0.29659485209466185, 'reg_lambda': 0.8387021990320037}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:46:32] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:46:33] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:46:33,165]\u001b[0m Trial 311 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.03624512995026731, 'n_estimators': 86, 'max_depth': 7, 'min_child_weight': 6.588877022138133, 'subsample': 0.959238916439848, 'colsample_bytree': 0.885728245381608, 'colsample_bylevel': 0.7723450088861383, 'gamma': 0.06590508365554032, 'reg_alpha': 0.29907266563926377, 'reg_lambda': 0.8772877878315942}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:46:33,317]\u001b[0m Trial 312 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.06638806233230413, 'n_estimators': 81, 'max_depth': 7, 'min_child_weight': 9.875778700921684, 'subsample': 0.545980305528048, 'colsample_bytree': 0.9058399086747805, 'colsample_bylevel': 0.5897858399478886, 'gamma': 0.07587006848834074, 'reg_alpha': 0.32759289985606665, 'reg_lambda': 0.8026133495231913}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:46:33] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:46:33] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:46:33,468]\u001b[0m Trial 313 finished with value: 0.3333333333333333 and parameters: {'booster': 'gblinear', 'learning_rate': 0.04929986930459946, 'n_estimators': 80, 'max_depth': 9, 'min_child_weight': 3.4686608441699973, 'subsample': 0.6790144214000665, 'colsample_bytree': 0.5015237814193941, 'colsample_bylevel': 0.9938886456979937, 'gamma': 0.06201312428402634, 'reg_alpha': 0.35726685878731984, 'reg_lambda': 0.9038678937078097}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:46:33,622]\u001b[0m Trial 314 finished with value: 0.8285714285714286 and parameters: {'booster': 'gblinear', 'learning_rate': 0.06368175855991887, 'n_estimators': 81, 'max_depth': 3, 'min_child_weight': 4.527336279103686, 'subsample': 0.8467813666905649, 'colsample_bytree': 0.9112344701386101, 'colsample_bylevel': 0.5598302863170647, 'gamma': 0.2006459592302334, 'reg_alpha': 0.15849243905022026, 'reg_lambda': 0.9385207421717319}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:46:33] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:46:33] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:46:33,792]\u001b[0m Trial 315 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.1548549398398378, 'n_estimators': 95, 'max_depth': 8, 'min_child_weight': 5.313912694510896, 'subsample': 0.7459643435473392, 'colsample_bytree': 0.9728393333664055, 'colsample_bylevel': 0.5122331265152359, 'gamma': 0.016873825690108504, 'reg_alpha': 0.23416691980493856, 'reg_lambda': 0.8221546934553065}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:46:33,982]\u001b[0m Trial 316 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.15406078150002228, 'n_estimators': 98, 'max_depth': 8, 'min_child_weight': 4.903660244313684, 'subsample': 0.7390948351961595, 'colsample_bytree': 0.9635314640071145, 'colsample_bylevel': 0.5159960034984616, 'gamma': 0.022484392604171073, 'reg_alpha': 0.23442161837759662, 'reg_lambda': 0.8274384126757229}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:46:33] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:46:34] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:46:34,151]\u001b[0m Trial 317 finished with value: 0.8285714285714286 and parameters: {'booster': 'gblinear', 'learning_rate': 0.16102479603373313, 'n_estimators': 91, 'max_depth': 8, 'min_child_weight': 5.4698037592433355, 'subsample': 0.7339446753900132, 'colsample_bytree': 0.7389612528829189, 'colsample_bylevel': 0.9651367904742001, 'gamma': 0.46188985807204075, 'reg_alpha': 0.20269379082291192, 'reg_lambda': 0.42329750987371856}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:46:34,311]\u001b[0m Trial 318 finished with value: 0.3333333333333333 and parameters: {'booster': 'gblinear', 'learning_rate': 0.1581085717885678, 'n_estimators': 86, 'max_depth': 6, 'min_child_weight': 4.246345676413724, 'subsample': 0.6467964638093284, 'colsample_bytree': 0.7279471710766905, 'colsample_bylevel': 0.9621207597894214, 'gamma': 0.4727935259537102, 'reg_alpha': 0.9772082940730837, 'reg_lambda': 0.3865629882629799}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:46:34] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:46:34] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:46:34,480]\u001b[0m Trial 319 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.10680439230615488, 'n_estimators': 91, 'max_depth': 8, 'min_child_weight': 5.759312623575447, 'subsample': 0.6598404650478105, 'colsample_bytree': 0.7613090372301151, 'colsample_bylevel': 0.944358732780336, 'gamma': 0.4552978694771176, 'reg_alpha': 0.20861020482556084, 'reg_lambda': 0.5047906259994964}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:46:34,645]\u001b[0m Trial 320 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.117036587967625, 'n_estimators': 84, 'max_depth': 10, 'min_child_weight': 3.0948791496516197, 'subsample': 0.6027186627426717, 'colsample_bytree': 0.6305425161468533, 'colsample_bylevel': 0.9850299743150197, 'gamma': 0.052981326061707405, 'reg_alpha': 0.2844955628901595, 'reg_lambda': 0.891282860364237}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:46:34] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:46:34] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:46:34,813]\u001b[0m Trial 321 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.10554001464079361, 'n_estimators': 90, 'max_depth': 10, 'min_child_weight': 5.818914422017607, 'subsample': 0.6682261006682217, 'colsample_bytree': 0.7427969428184767, 'colsample_bylevel': 0.9373984091335239, 'gamma': 0.45259563484621945, 'reg_alpha': 0.1957673428686063, 'reg_lambda': 0.5198883853851474}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:46:35,006]\u001b[0m Trial 322 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.11518525798423218, 'n_estimators': 90, 'max_depth': 10, 'min_child_weight': 5.584087311735079, 'subsample': 0.8225028329276447, 'colsample_bytree': 0.7487569626572737, 'colsample_bylevel': 0.7013209841196084, 'gamma': 0.3523660753121554, 'reg_alpha': 0.1866472412637241, 'reg_lambda': 0.5231424447451452}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:46:34] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:46:35] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:46:35,165]\u001b[0m Trial 323 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.11055988231874676, 'n_estimators': 85, 'max_depth': 10, 'min_child_weight': 3.926074997419407, 'subsample': 0.592384910960285, 'colsample_bytree': 0.647963962748714, 'colsample_bylevel': 0.6270233387509069, 'gamma': 0.048264237344353235, 'reg_alpha': 0.2970885927867077, 'reg_lambda': 0.9130610263591302}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:46:35,315]\u001b[0m Trial 324 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.11958535992168577, 'n_estimators': 80, 'max_depth': 10, 'min_child_weight': 6.793032764596671, 'subsample': 0.626641519660385, 'colsample_bytree': 0.619617465681561, 'colsample_bylevel': 0.6001227178010359, 'gamma': 0.0657401014069573, 'reg_alpha': 0.2857632225319109, 'reg_lambda': 0.7883967643278281}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:46:35] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:46:35] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:46:35,467]\u001b[0m Trial 325 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.0335179027425052, 'n_estimators': 80, 'max_depth': 10, 'min_child_weight': 7.038303499346573, 'subsample': 0.5758969572746818, 'colsample_bytree': 0.505892715302857, 'colsample_bylevel': 0.9486925710285745, 'gamma': 0.05770830368632144, 'reg_alpha': 0.2704351098251679, 'reg_lambda': 0.8508891470658089}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:46:35,620]\u001b[0m Trial 326 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.10226597125816296, 'n_estimators': 81, 'max_depth': 10, 'min_child_weight': 3.2187077468402165, 'subsample': 0.9363621651337493, 'colsample_bytree': 0.985057830555351, 'colsample_bylevel': 0.9696591573877766, 'gamma': 0.3322127406764566, 'reg_alpha': 0.267894441443539, 'reg_lambda': 0.8095149714626346}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:46:35] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:46:35] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:46:35,772]\u001b[0m Trial 327 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.10218462040997775, 'n_estimators': 80, 'max_depth': 3, 'min_child_weight': 4.30865813759908, 'subsample': 0.9650360665871907, 'colsample_bytree': 0.7058866160191543, 'colsample_bylevel': 0.8759760857913553, 'gamma': 0.2233144266834637, 'reg_alpha': 0.26443384955671634, 'reg_lambda': 0.7748261023674293}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:46:35,923]\u001b[0m Trial 328 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.10309250162258148, 'n_estimators': 80, 'max_depth': 3, 'min_child_weight': 4.35563341218255, 'subsample': 0.9539277788447629, 'colsample_bytree': 0.7076709071566359, 'colsample_bylevel': 0.8772721733871001, 'gamma': 0.401466481190955, 'reg_alpha': 0.26458327234499696, 'reg_lambda': 0.7721805211727765}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:46:35] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:46:36,105]\u001b[0m Trial 329 finished with value: 0.3333333333333333 and parameters: {'booster': 'dart', 'learning_rate': 0.09761527226096828, 'n_estimators': 84, 'max_depth': 7, 'min_child_weight': 3.5233961312570004, 'subsample': 0.9401352931316106, 'colsample_bytree': 0.9273954378368956, 'colsample_bylevel': 0.9815146609754767, 'gamma': 0.09593182309751386, 'reg_alpha': 0.21465027160052877, 'reg_lambda': 0.7564469621044845}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:46:36,272]\u001b[0m Trial 330 finished with value: 0.3333333333333333 and parameters: {'booster': 'gblinear', 'learning_rate': 0.19887296807066351, 'n_estimators': 92, 'max_depth': 3, 'min_child_weight': 4.452113785109592, 'subsample': 0.9559168521262482, 'colsample_bytree': 0.7677212064016039, 'colsample_bylevel': 0.7555906168215265, 'gamma': 0.39606616413815066, 'reg_alpha': 0.44892292765507763, 'reg_lambda': 0.7695329757104089}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:46:36] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:46:36,445]\u001b[0m Trial 331 finished with value: 0.3333333333333333 and parameters: {'booster': 'gbtree', 'learning_rate': 0.14876317292951538, 'n_estimators': 92, 'max_depth': 3, 'min_child_weight': 5.4063541937093005, 'subsample': 0.8610313855013798, 'colsample_bytree': 0.807526106044976, 'colsample_bylevel': 0.7714920022922171, 'gamma': 0.3819602661376821, 'reg_alpha': 0.2098592777058017, 'reg_lambda': 0.3253990450292093}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:46:36,610]\u001b[0m Trial 332 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.14170444276990032, 'n_estimators': 91, 'max_depth': 3, 'min_child_weight': 5.5612086792034665, 'subsample': 0.8292413844382375, 'colsample_bytree': 0.7722075438397266, 'colsample_bylevel': 0.7273038587860902, 'gamma': 0.4865131082709642, 'reg_alpha': 0.32015812809690236, 'reg_lambda': 0.8560650190177366}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:46:36] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:46:36] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:46:36,776]\u001b[0m Trial 333 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.1316501312594407, 'n_estimators': 92, 'max_depth': 4, 'min_child_weight': 8.135399010513225, 'subsample': 0.9756589065431258, 'colsample_bytree': 0.9533927089338716, 'colsample_bylevel': 0.696482264204843, 'gamma': 0.44159927502727275, 'reg_alpha': 0.32831024138544496, 'reg_lambda': 0.6607218875071404}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:46:36,944]\u001b[0m Trial 334 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.1278939783385105, 'n_estimators': 93, 'max_depth': 4, 'min_child_weight': 8.423427113175592, 'subsample': 0.9711284780852669, 'colsample_bytree': 0.9514748950688916, 'colsample_bylevel': 0.663452904364066, 'gamma': 0.42405910320845686, 'reg_alpha': 0.32659847946366505, 'reg_lambda': 0.6320985031022432}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:46:36] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:46:36] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:46:37,102]\u001b[0m Trial 335 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.13883749587418742, 'n_estimators': 85, 'max_depth': 7, 'min_child_weight': 3.3067802034823357, 'subsample': 0.9212297892922805, 'colsample_bytree': 0.8710790005978578, 'colsample_bylevel': 0.7882892439881302, 'gamma': 0.3131983967190519, 'reg_alpha': 0.25633145752569303, 'reg_lambda': 0.9260029810867175}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:46:37,264]\u001b[0m Trial 336 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.13452714094648943, 'n_estimators': 87, 'max_depth': 7, 'min_child_weight': 2.666771207853577, 'subsample': 0.9841418234758694, 'colsample_bytree': 0.6627601414859214, 'colsample_bylevel': 0.5000395016423657, 'gamma': 0.08428178162499488, 'reg_alpha': 0.28813241703313697, 'reg_lambda': 0.2541730806830824}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:46:37] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:46:37] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:46:37,422]\u001b[0m Trial 337 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.14394116754975006, 'n_estimators': 85, 'max_depth': 10, 'min_child_weight': 5.149556898088146, 'subsample': 0.9841044056210829, 'colsample_bytree': 0.8655757447220633, 'colsample_bylevel': 0.9801953847317942, 'gamma': 0.08036420705599873, 'reg_alpha': 0.2736582470468629, 'reg_lambda': 0.9601337817965071}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:46:37,581]\u001b[0m Trial 338 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.13741914795136892, 'n_estimators': 86, 'max_depth': 7, 'min_child_weight': 4.975359857191931, 'subsample': 0.9339712707591372, 'colsample_bytree': 0.9002689230685831, 'colsample_bylevel': 0.5700643375705181, 'gamma': 0.1098119003193751, 'reg_alpha': 0.28411569517192536, 'reg_lambda': 0.999325691228399}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:46:37] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:46:37] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:46:37,742]\u001b[0m Trial 339 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.07074528911727061, 'n_estimators': 87, 'max_depth': 6, 'min_child_weight': 4.728385313362875, 'subsample': 0.9275188407789827, 'colsample_bytree': 0.9792883950317819, 'colsample_bylevel': 0.9736319472704061, 'gamma': 0.09838277960631427, 'reg_alpha': 0.29913748070174134, 'reg_lambda': 0.37299393789028457}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:46:37,928]\u001b[0m Trial 340 finished with value: 0.8285714285714286 and parameters: {'booster': 'gblinear', 'learning_rate': 0.07609970953565427, 'n_estimators': 98, 'max_depth': 7, 'min_child_weight': 4.676604128622007, 'subsample': 0.5793317333913667, 'colsample_bytree': 0.666796732628828, 'colsample_bylevel': 0.5687476873497068, 'gamma': 0.08222467075932222, 'reg_alpha': 0.3709413384699651, 'reg_lambda': 0.34520213715057513}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:46:37] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:46:37] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:46:38,102]\u001b[0m Trial 341 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.01599667203752355, 'n_estimators': 95, 'max_depth': 7, 'min_child_weight': 1.181518972354654, 'subsample': 0.6929815097440235, 'colsample_bytree': 0.999300431733798, 'colsample_bylevel': 0.8404741567967887, 'gamma': 0.1251158118611724, 'reg_alpha': 0.08858276424530408, 'reg_lambda': 0.9738878055408127}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:46:38,276]\u001b[0m Trial 342 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.09934785420006656, 'n_estimators': 98, 'max_depth': 7, 'min_child_weight': 5.129360879552514, 'subsample': 0.9995568000423051, 'colsample_bytree': 0.8793546387972707, 'colsample_bylevel': 0.5184118643161644, 'gamma': 0.4064409899404179, 'reg_alpha': 0.2325127571594209, 'reg_lambda': 0.3161038535195223}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:46:38] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:46:38] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:46:38,444]\u001b[0m Trial 343 finished with value: 0.3333333333333333 and parameters: {'booster': 'gblinear', 'learning_rate': 0.10873903964775078, 'n_estimators': 91, 'max_depth': 5, 'min_child_weight': 4.416895338602476, 'subsample': 0.8130761250305089, 'colsample_bytree': 0.7307977215242302, 'colsample_bylevel': 0.5253468952715956, 'gamma': 0.4980042140517258, 'reg_alpha': 0.039073894702636856, 'reg_lambda': 0.4400657993127306}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:46:38,620]\u001b[0m Trial 344 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.01991143719971283, 'n_estimators': 99, 'max_depth': 3, 'min_child_weight': 1.7702440735153941, 'subsample': 0.9984931386255723, 'colsample_bytree': 0.7842657408899826, 'colsample_bylevel': 0.5334668075003691, 'gamma': 0.1827456515300524, 'reg_alpha': 0.21710270530932502, 'reg_lambda': 0.41916273870925397}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:46:38] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:46:38] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:46:38,783]\u001b[0m Trial 345 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.1487914808929069, 'n_estimators': 89, 'max_depth': 9, 'min_child_weight': 5.3339432970628415, 'subsample': 0.8997877147786154, 'colsample_bytree': 0.9697546838269928, 'colsample_bylevel': 0.9728518028646681, 'gamma': 0.14090720345786328, 'reg_alpha': 0.24010849691958644, 'reg_lambda': 0.4455855659377715}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:46:38,939]\u001b[0m Trial 346 finished with value: 0.6249999999999999 and parameters: {'booster': 'gblinear', 'learning_rate': 0.11923945557595228, 'n_estimators': 84, 'max_depth': 10, 'min_child_weight': 7.969350425579063, 'subsample': 0.9256457199670316, 'colsample_bytree': 0.8915752296400992, 'colsample_bylevel': 0.8838090498884987, 'gamma': 0.41790258447070716, 'reg_alpha': 0.34998657804461436, 'reg_lambda': 0.9362785967937908}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:46:38] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:46:38] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:46:39,102]\u001b[0m Trial 347 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.15210677675299877, 'n_estimators': 89, 'max_depth': 6, 'min_child_weight': 3.971998393921607, 'subsample': 0.6508474857862996, 'colsample_bytree': 0.7554677022673975, 'colsample_bylevel': 0.9544731615495373, 'gamma': 0.14584346184646724, 'reg_alpha': 0.232867764525282, 'reg_lambda': 0.4004424982767902}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:46:39,262]\u001b[0m Trial 348 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.1243304549778639, 'n_estimators': 85, 'max_depth': 10, 'min_child_weight': 7.820315421424467, 'subsample': 0.9438771163630316, 'colsample_bytree': 0.9760796271528003, 'colsample_bylevel': 0.6377210722027978, 'gamma': 0.41964773453873544, 'reg_alpha': 0.3384281710772734, 'reg_lambda': 0.9505770726708739}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:46:39] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:46:39] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:46:39,438]\u001b[0m Trial 349 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.07281170313657216, 'n_estimators': 99, 'max_depth': 5, 'min_child_weight': 8.980382848754731, 'subsample': 0.6340242266274728, 'colsample_bytree': 0.8378509161785199, 'colsample_bylevel': 0.5412564849562197, 'gamma': 0.24925678494278447, 'reg_alpha': 0.2442081746645932, 'reg_lambda': 0.29322852750785666}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:46:39,614]\u001b[0m Trial 350 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.06827135661662335, 'n_estimators': 99, 'max_depth': 5, 'min_child_weight': 8.627672850619183, 'subsample': 0.6350363172848081, 'colsample_bytree': 0.8179762655822683, 'colsample_bylevel': 0.5367758483721239, 'gamma': 0.23490114837184406, 'reg_alpha': 0.22608954842031848, 'reg_lambda': 0.3267122118878905}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:46:39] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:46:39] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:46:39,772]\u001b[0m Trial 351 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.13116573875280843, 'n_estimators': 85, 'max_depth': 7, 'min_child_weight': 7.685140226951945, 'subsample': 0.9803778302390742, 'colsample_bytree': 0.8942304868180022, 'colsample_bylevel': 0.6478710976050475, 'gamma': 0.44637855907136126, 'reg_alpha': 0.25846275129701973, 'reg_lambda': 0.2579421197104458}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:46:39,929]\u001b[0m Trial 352 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.1299655709797408, 'n_estimators': 84, 'max_depth': 7, 'min_child_weight': 1.3413029453024867, 'subsample': 0.9121353701072278, 'colsample_bytree': 0.8989428056518146, 'colsample_bylevel': 0.8737554825572654, 'gamma': 0.4271265416775303, 'reg_alpha': 0.24966313272174112, 'reg_lambda': 0.9374767739284458}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:46:39] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:46:40,092]\u001b[0m Trial 353 finished with value: 0.6249999999999999 and parameters: {'booster': 'gbtree', 'learning_rate': 0.12648241949050143, 'n_estimators': 85, 'max_depth': 7, 'min_child_weight': 1.4482538865449923, 'subsample': 0.9478474017002286, 'colsample_bytree': 0.9083721463715343, 'colsample_bylevel': 0.8600422001121107, 'gamma': 0.4286200316551817, 'reg_alpha': 0.2456057348262349, 'reg_lambda': 0.9150566121709928}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:46:40,248]\u001b[0m Trial 354 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.11200844303833087, 'n_estimators': 83, 'max_depth': 6, 'min_child_weight': 3.625732016483415, 'subsample': 0.8145096885194995, 'colsample_bytree': 0.7613546026943265, 'colsample_bylevel': 0.7092559718495909, 'gamma': 0.43681883213007455, 'reg_alpha': 0.19266829337643354, 'reg_lambda': 0.7482961471154658}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:46:40] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:46:40,437]\u001b[0m Trial 355 finished with value: 0.6249999999999999 and parameters: {'booster': 'dart', 'learning_rate': 0.09084341870901382, 'n_estimators': 87, 'max_depth': 7, 'min_child_weight': 1.2920269756644274, 'subsample': 0.900485594346925, 'colsample_bytree': 0.9584559896485075, 'colsample_bylevel': 0.9196728969576716, 'gamma': 0.13435316914934262, 'reg_alpha': 0.26678247888927115, 'reg_lambda': 0.9786067146529536}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:46:40,597]\u001b[0m Trial 356 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.11091964991189741, 'n_estimators': 86, 'max_depth': 6, 'min_child_weight': 3.8123607572641607, 'subsample': 0.8069146301572614, 'colsample_bytree': 0.7609952600248817, 'colsample_bylevel': 0.7512710152217026, 'gamma': 0.4385167799654242, 'reg_alpha': 0.18702565424165413, 'reg_lambda': 0.7164760603472464}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:46:40] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:46:40] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:46:40,776]\u001b[0m Trial 357 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.09272783116624167, 'n_estimators': 98, 'max_depth': 7, 'min_child_weight': 1.0344767085171374, 'subsample': 0.7121405248084373, 'colsample_bytree': 0.992983764910737, 'colsample_bylevel': 0.9123806806409618, 'gamma': 0.15369770465951704, 'reg_alpha': 0.24823999081763123, 'reg_lambda': 0.982170138667853}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:46:40,950]\u001b[0m Trial 358 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.09244290919074147, 'n_estimators': 97, 'max_depth': 7, 'min_child_weight': 1.1940576677630705, 'subsample': 0.9432921433561021, 'colsample_bytree': 0.9401964952873852, 'colsample_bylevel': 0.648386947845666, 'gamma': 0.1144073565573075, 'reg_alpha': 0.24484608305189895, 'reg_lambda': 0.9616622302872067}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:46:40] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:46:40] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:46:41,144]\u001b[0m Trial 359 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.0907670556279771, 'n_estimators': 98, 'max_depth': 6, 'min_child_weight': 1.0178673369118791, 'subsample': 0.911359564547698, 'colsample_bytree': 0.9235124022995351, 'colsample_bylevel': 0.9435065201798247, 'gamma': 0.12935689802345104, 'reg_alpha': 0.27862392455708895, 'reg_lambda': 0.23335035618143246}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:46:41,323]\u001b[0m Trial 360 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.14625775644227784, 'n_estimators': 97, 'max_depth': 7, 'min_child_weight': 9.733589518656268, 'subsample': 0.9055173314928666, 'colsample_bytree': 0.9963962914270065, 'colsample_bylevel': 0.9280050660939931, 'gamma': 0.11248880552788103, 'reg_alpha': 0.22101360348730487, 'reg_lambda': 0.8952278720086774}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:46:41] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:46:41] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:46:41,499]\u001b[0m Trial 361 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.1502550152660308, 'n_estimators': 97, 'max_depth': 8, 'min_child_weight': 1.1596501199107807, 'subsample': 0.7047643055766819, 'colsample_bytree': 0.9637098817297369, 'colsample_bylevel': 0.8236654144525947, 'gamma': 0.13966348645106186, 'reg_alpha': 0.2278782737869164, 'reg_lambda': 0.4365649601470768}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:46:41,664]\u001b[0m Trial 362 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.13828497866273623, 'n_estimators': 88, 'max_depth': 6, 'min_child_weight': 6.026281326108961, 'subsample': 0.6934804067216861, 'colsample_bytree': 0.9878487114025261, 'colsample_bylevel': 0.904934883417569, 'gamma': 0.15791396622912215, 'reg_alpha': 0.2395784598048632, 'reg_lambda': 0.4686717628308204}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:46:41] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:46:41] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:46:41,838]\u001b[0m Trial 363 finished with value: 0.8285714285714286 and parameters: {'booster': 'gblinear', 'learning_rate': 0.1467245151885284, 'n_estimators': 97, 'max_depth': 6, 'min_child_weight': 4.202190825428359, 'subsample': 0.7063456105156214, 'colsample_bytree': 0.6567853571895782, 'colsample_bylevel': 0.8346375596153506, 'gamma': 0.13564505497126764, 'reg_alpha': 0.21014876269132246, 'reg_lambda': 0.4567018347220158}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:46:42,016]\u001b[0m Trial 364 finished with value: 0.3333333333333333 and parameters: {'booster': 'gblinear', 'learning_rate': 0.10152828279930638, 'n_estimators': 100, 'max_depth': 4, 'min_child_weight': 4.6505445478007745, 'subsample': 0.8444552008259676, 'colsample_bytree': 0.5786589668548346, 'colsample_bylevel': 0.6634430121777055, 'gamma': 0.10890764910069946, 'reg_alpha': 0.6820666995831047, 'reg_lambda': 0.30012581697557444}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:46:41] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:46:42] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:46:42,185]\u001b[0m Trial 365 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.08273395232016252, 'n_estimators': 93, 'max_depth': 8, 'min_child_weight': 6.615198589614528, 'subsample': 0.8639576444056005, 'colsample_bytree': 0.5242595123308839, 'colsample_bylevel': 0.6208567077565477, 'gamma': 0.46612549569953643, 'reg_alpha': 0.1666723196016323, 'reg_lambda': 0.8794458387794168}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:46:42,361]\u001b[0m Trial 366 finished with value: 0.6249999999999999 and parameters: {'booster': 'gblinear', 'learning_rate': 0.08084723847915393, 'n_estimators': 96, 'max_depth': 5, 'min_child_weight': 1.5385907754392523, 'subsample': 0.8481928897799786, 'colsample_bytree': 0.5217466790548478, 'colsample_bylevel': 0.6253854306949268, 'gamma': 0.03692312843918718, 'reg_alpha': 0.14426202940240615, 'reg_lambda': 0.35315105044926015}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:46:42] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:46:42] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:46:42,528]\u001b[0m Trial 367 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.09559144745975678, 'n_estimators': 91, 'max_depth': 10, 'min_child_weight': 6.357019153613356, 'subsample': 0.5281636190170591, 'colsample_bytree': 0.5097881681289245, 'colsample_bylevel': 0.5812045076720962, 'gamma': 0.4757308563668701, 'reg_alpha': 0.19729460988387648, 'reg_lambda': 0.7350224961159814}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:46:42,700]\u001b[0m Trial 368 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.072255024820121, 'n_estimators': 96, 'max_depth': 6, 'min_child_weight': 1.602595184365734, 'subsample': 0.8860665991894872, 'colsample_bytree': 0.8256614054534381, 'colsample_bylevel': 0.6930770906185124, 'gamma': 0.22048317207803347, 'reg_alpha': 0.2281379772279004, 'reg_lambda': 0.40422703949516914}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:46:42] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:46:42] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:46:42,876]\u001b[0m Trial 369 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.0757991511678911, 'n_estimators': 99, 'max_depth': 6, 'min_child_weight': 1.9675862959028334, 'subsample': 0.7502030291634517, 'colsample_bytree': 0.7976950299090172, 'colsample_bylevel': 0.5442192698889164, 'gamma': 0.2503121160010075, 'reg_alpha': 0.2583322637048962, 'reg_lambda': 0.5394951177061458}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:46:43,048]\u001b[0m Trial 370 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.06823143108207484, 'n_estimators': 96, 'max_depth': 3, 'min_child_weight': 1.349359004138035, 'subsample': 0.7334230902620609, 'colsample_bytree': 0.6754043231591274, 'colsample_bylevel': 0.5527206231248064, 'gamma': 0.16295831425972715, 'reg_alpha': 0.21754194556809628, 'reg_lambda': 0.4185078929236458}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:46:42] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:46:43] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:46:43,222]\u001b[0m Trial 371 finished with value: 0.8285714285714286 and parameters: {'booster': 'gblinear', 'learning_rate': 0.07478229683552676, 'n_estimators': 96, 'max_depth': 5, 'min_child_weight': 4.897866787142928, 'subsample': 0.9329134258499525, 'colsample_bytree': 0.7446190100536072, 'colsample_bylevel': 0.5591280580261787, 'gamma': 0.4911257252642932, 'reg_alpha': 0.16784821952746692, 'reg_lambda': 0.37064862225582185}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:46:43,413]\u001b[0m Trial 372 finished with value: 0.3333333333333333 and parameters: {'booster': 'dart', 'learning_rate': 0.16185538528848595, 'n_estimators': 89, 'max_depth': 3, 'min_child_weight': 5.722149833520968, 'subsample': 0.8511389143439486, 'colsample_bytree': 0.7220966631341016, 'colsample_bylevel': 0.5753181759434427, 'gamma': 0.4489055990066992, 'reg_alpha': 0.06763454730072582, 'reg_lambda': 0.5597064479549784}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:46:43,566]\u001b[0m Trial 373 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.03004362813907924, 'n_estimators': 80, 'max_depth': 10, 'min_child_weight': 6.977200883447384, 'subsample': 0.5774113828798678, 'colsample_bytree': 0.6390543086078408, 'colsample_bylevel': 0.5934696779477159, 'gamma': 0.06141433117800861, 'reg_alpha': 0.29935719961878043, 'reg_lambda': 0.7906394446798989}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:46:43] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:46:43] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:46:43,743]\u001b[0m Trial 374 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.06861940071729157, 'n_estimators': 99, 'max_depth': 4, 'min_child_weight': 6.1235481905636, 'subsample': 0.7227474866420218, 'colsample_bytree': 0.978832899601521, 'colsample_bylevel': 0.6902575299918051, 'gamma': 0.1055713408936676, 'reg_alpha': 0.2647544955265649, 'reg_lambda': 0.4118845722165843}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:46:43,911]\u001b[0m Trial 375 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.028344526845600244, 'n_estimators': 81, 'max_depth': 10, 'min_child_weight': 3.432086657043111, 'subsample': 0.5735780823662091, 'colsample_bytree': 0.9863700075022924, 'colsample_bylevel': 0.9844667959037083, 'gamma': 0.3234114380143983, 'reg_alpha': 0.291299739677101, 'reg_lambda': 0.8096600215761889}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:46:43] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:46:43] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:46:44,072]\u001b[0m Trial 376 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.10291662610353346, 'n_estimators': 82, 'max_depth': 10, 'min_child_weight': 3.1254931129521375, 'subsample': 0.5577029811506886, 'colsample_bytree': 0.9912285984253002, 'colsample_bylevel': 0.8870419108371052, 'gamma': 0.3055214461869313, 'reg_alpha': 0.2720981351668922, 'reg_lambda': 0.8047611435502147}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:46:44,258]\u001b[0m Trial 377 finished with value: 0.3333333333333333 and parameters: {'booster': 'gbtree', 'learning_rate': 0.1340651957149245, 'n_estimators': 100, 'max_depth': 4, 'min_child_weight': 5.9356374880185285, 'subsample': 0.8724861353323256, 'colsample_bytree': 0.6460625942927772, 'colsample_bylevel': 0.6754876994553732, 'gamma': 0.10140687957920339, 'reg_alpha': 0.31577155578008265, 'reg_lambda': 0.3901877482578156}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:46:44,417]\u001b[0m Trial 378 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.04176646867760632, 'n_estimators': 82, 'max_depth': 9, 'min_child_weight': 3.2133267507085996, 'subsample': 0.5514222379220662, 'colsample_bytree': 0.7067866295231543, 'colsample_bylevel': 0.8858208252571367, 'gamma': 0.30917355160536525, 'reg_alpha': 0.273964198476138, 'reg_lambda': 0.7821602315073887}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:46:44] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:46:44] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:46:44,589]\u001b[0m Trial 379 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.07879302221772577, 'n_estimators': 91, 'max_depth': 3, 'min_child_weight': 8.405771446407607, 'subsample': 0.7686387251005011, 'colsample_bytree': 0.7765557901281925, 'colsample_bylevel': 0.7325006530845152, 'gamma': 0.48627118665502506, 'reg_alpha': 0.328823988091944, 'reg_lambda': 0.6747898480550193}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:46:44,762]\u001b[0m Trial 380 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.04488099892860159, 'n_estimators': 91, 'max_depth': 9, 'min_child_weight': 3.051145954634094, 'subsample': 0.5411362721194937, 'colsample_bytree': 0.7022587841896788, 'colsample_bylevel': 0.9969432542765769, 'gamma': 0.4843114365158775, 'reg_alpha': 0.3174174450769302, 'reg_lambda': 0.8482308380317287}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:46:44] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:46:44] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:46:44,951]\u001b[0m Trial 381 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.1292421360609957, 'n_estimators': 99, 'max_depth': 4, 'min_child_weight': 7.220537887483459, 'subsample': 0.876437406252143, 'colsample_bytree': 0.5887559059726979, 'colsample_bylevel': 0.6623722685209799, 'gamma': 0.0986541568954958, 'reg_alpha': 0.25152541125844685, 'reg_lambda': 0.44643925525269507}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:46:45,133]\u001b[0m Trial 382 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.14158952383890622, 'n_estimators': 100, 'max_depth': 4, 'min_child_weight': 7.443412315803671, 'subsample': 0.8907043649642427, 'colsample_bytree': 0.6087214932757445, 'colsample_bylevel': 0.6682312086934754, 'gamma': 0.09754178412159809, 'reg_alpha': 0.2564174501565631, 'reg_lambda': 0.4285929486271403}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:46:44] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:46:45] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:46:45,312]\u001b[0m Trial 383 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.14807251329134805, 'n_estimators': 100, 'max_depth': 4, 'min_child_weight': 7.599329876863377, 'subsample': 0.8914194223571775, 'colsample_bytree': 0.9667502556758759, 'colsample_bylevel': 0.6309934856295708, 'gamma': 0.12220204285777711, 'reg_alpha': 0.3831971797157473, 'reg_lambda': 0.28435401534922705}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:46:45,491]\u001b[0m Trial 384 finished with value: 0.8285714285714286 and parameters: {'booster': 'gblinear', 'learning_rate': 0.13968546796531692, 'n_estimators': 96, 'max_depth': 4, 'min_child_weight': 7.876152148375811, 'subsample': 0.885832206046584, 'colsample_bytree': 0.9596353488466385, 'colsample_bylevel': 0.6555857751502574, 'gamma': 0.007252857412660296, 'reg_alpha': 0.34252227874633423, 'reg_lambda': 0.9181754444106099}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:46:45] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:46:45] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:46:45,658]\u001b[0m Trial 385 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.11615638737642377, 'n_estimators': 90, 'max_depth': 6, 'min_child_weight': 5.598808462753856, 'subsample': 0.9577268127463993, 'colsample_bytree': 0.7432290973648782, 'colsample_bylevel': 0.6093860431008781, 'gamma': 0.406537770364791, 'reg_alpha': 0.307001746508458, 'reg_lambda': 0.34843508736226186}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:46:45,833]\u001b[0m Trial 386 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.09754006044466722, 'n_estimators': 96, 'max_depth': 7, 'min_child_weight': 4.906197348543736, 'subsample': 0.9624579488865256, 'colsample_bytree': 0.7856569609911999, 'colsample_bylevel': 0.5822751698119856, 'gamma': 0.15060343125091605, 'reg_alpha': 0.3021642554301217, 'reg_lambda': 0.44598427186678813}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:46:45] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:46:45] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:46:46,004]\u001b[0m Trial 387 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.08781859457890091, 'n_estimators': 93, 'max_depth': 7, 'min_child_weight': 5.269150692033664, 'subsample': 0.9806015837666137, 'colsample_bytree': 0.9721532475303125, 'colsample_bylevel': 0.6350703522185502, 'gamma': 0.15910758672575165, 'reg_alpha': 0.3115723945905787, 'reg_lambda': 0.4121820698968066}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:46:46,176]\u001b[0m Trial 388 finished with value: 0.3333333333333333 and parameters: {'booster': 'gblinear', 'learning_rate': 0.08535965531532225, 'n_estimators': 94, 'max_depth': 7, 'min_child_weight': 5.033500735642953, 'subsample': 0.9784130497053909, 'colsample_bytree': 0.7779118673363853, 'colsample_bylevel': 0.8511699412208283, 'gamma': 0.4533709445629258, 'reg_alpha': 0.5785315231282644, 'reg_lambda': 0.9455069281380141}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:46:46] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:46:46] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:46:46,338]\u001b[0m Trial 389 finished with value: 0.3333333333333333 and parameters: {'booster': 'gblinear', 'learning_rate': 0.13900318515469268, 'n_estimators': 86, 'max_depth': 7, 'min_child_weight': 7.267007589836023, 'subsample': 0.9297742067963123, 'colsample_bytree': 0.9780500820531629, 'colsample_bylevel': 0.989700798421267, 'gamma': 0.09076994259321813, 'reg_alpha': 0.8049855335276075, 'reg_lambda': 0.9970285493846931}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:46:46,501]\u001b[0m Trial 390 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.13457617126415133, 'n_estimators': 86, 'max_depth': 7, 'min_child_weight': 6.9056285327134, 'subsample': 0.9990509559887033, 'colsample_bytree': 0.860899480379418, 'colsample_bylevel': 0.9677295590221027, 'gamma': 0.11066945849112235, 'reg_alpha': 0.2917471881548141, 'reg_lambda': 0.9783373315696325}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:46:46] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:46:46] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:46:46,676]\u001b[0m Trial 391 finished with value: 0.3333333333333333 and parameters: {'booster': 'gblinear', 'learning_rate': 0.08354108630747559, 'n_estimators': 96, 'max_depth': 8, 'min_child_weight': 5.865670597779459, 'subsample': 0.7252707922530995, 'colsample_bytree': 0.5332600855378066, 'colsample_bylevel': 0.8560203611114929, 'gamma': 0.463547286963611, 'reg_alpha': 0.3627468446171639, 'reg_lambda': 0.9499287348309474}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:46:46,852]\u001b[0m Trial 392 finished with value: 0.8285714285714286 and parameters: {'booster': 'gblinear', 'learning_rate': 0.14409472687378627, 'n_estimators': 98, 'max_depth': 7, 'min_child_weight': 6.898218538355189, 'subsample': 0.9944488678705575, 'colsample_bytree': 0.6342306067459968, 'colsample_bylevel': 0.9657739980009822, 'gamma': 0.07286444544347348, 'reg_alpha': 0.1990769385243087, 'reg_lambda': 0.19370336429473026}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:46:46] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:46:46] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:46:47,016]\u001b[0m Trial 393 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.09574421320084192, 'n_estimators': 88, 'max_depth': 5, 'min_child_weight': 6.214073487503493, 'subsample': 0.8263459089193609, 'colsample_bytree': 0.7349811636119108, 'colsample_bylevel': 0.7969975777296341, 'gamma': 0.054397418803570435, 'reg_alpha': 0.18566351809001141, 'reg_lambda': 0.511220024095531}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:46:47,192]\u001b[0m Trial 394 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.16455914075115086, 'n_estimators': 95, 'max_depth': 8, 'min_child_weight': 8.743764809453298, 'subsample': 0.9517710058455872, 'colsample_bytree': 0.9436971847195558, 'colsample_bylevel': 0.6171234340151116, 'gamma': 0.46041193213491266, 'reg_alpha': 0.2914167669339991, 'reg_lambda': 0.4787569163000413}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:46:47] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:46:47] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:46:47,370]\u001b[0m Trial 395 finished with value: 0.3333333333333333 and parameters: {'booster': 'gblinear', 'learning_rate': 0.08490497675886541, 'n_estimators': 96, 'max_depth': 8, 'min_child_weight': 5.7366882556968735, 'subsample': 0.9688792778287548, 'colsample_bytree': 0.9573167234027254, 'colsample_bylevel': 0.8635380899729739, 'gamma': 0.440529437007835, 'reg_alpha': 0.9443471475844423, 'reg_lambda': 0.9381734122137424}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:46:47,544]\u001b[0m Trial 396 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.11484720287091801, 'n_estimators': 95, 'max_depth': 9, 'min_child_weight': 3.911660772354314, 'subsample': 0.9892601329718166, 'colsample_bytree': 0.9462138041748682, 'colsample_bylevel': 0.6176488604826947, 'gamma': 0.04677031771618294, 'reg_alpha': 0.2170425418270717, 'reg_lambda': 0.3852672361112259}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:46:47] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:46:47] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:46:47,718]\u001b[0m Trial 397 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.15547164821237872, 'n_estimators': 95, 'max_depth': 8, 'min_child_weight': 9.367267598785475, 'subsample': 0.7140281163927927, 'colsample_bytree': 0.720301745773881, 'colsample_bylevel': 0.870342039422083, 'gamma': 0.4340049012912009, 'reg_alpha': 0.33705321986304954, 'reg_lambda': 0.9615939175970615}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:46:47,930]\u001b[0m Trial 398 finished with value: 0.3333333333333333 and parameters: {'booster': 'dart', 'learning_rate': 0.15271477538024583, 'n_estimators': 97, 'max_depth': 8, 'min_child_weight': 5.954819927065261, 'subsample': 0.9645352516586683, 'colsample_bytree': 0.9676069884871726, 'colsample_bylevel': 0.8440027281273476, 'gamma': 0.4455896785703901, 'reg_alpha': 0.3520250397756929, 'reg_lambda': 0.9505322029450897}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:46:48,099]\u001b[0m Trial 399 finished with value: 0.3333333333333333 and parameters: {'booster': 'gblinear', 'learning_rate': 0.12510750631927162, 'n_estimators': 85, 'max_depth': 10, 'min_child_weight': 9.380132252639063, 'subsample': 0.9245194381327684, 'colsample_bytree': 0.8397649558388324, 'colsample_bylevel': 0.634949878828724, 'gamma': 0.24778317011079698, 'reg_alpha': 0.3838216910649179, 'reg_lambda': 0.8805105880924463}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:46:47] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:46:48] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:46:48,279]\u001b[0m Trial 400 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.12086613692126869, 'n_estimators': 98, 'max_depth': 7, 'min_child_weight': 9.150492572461452, 'subsample': 0.5988985818461924, 'colsample_bytree': 0.9794335144585571, 'colsample_bylevel': 0.6352677750748341, 'gamma': 0.2373870045560465, 'reg_alpha': 0.34076315794796946, 'reg_lambda': 0.31585313913796287}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:46:48,463]\u001b[0m Trial 401 finished with value: 0.3333333333333333 and parameters: {'booster': 'gbtree', 'learning_rate': 0.1502609563762421, 'n_estimators': 97, 'max_depth': 8, 'min_child_weight': 4.055083079118737, 'subsample': 0.9676005541771231, 'colsample_bytree': 0.812496809319068, 'colsample_bylevel': 0.8954253819307547, 'gamma': 0.4484699034474247, 'reg_alpha': 0.21079788990096565, 'reg_lambda': 0.5677667401104096}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:46:48,625]\u001b[0m Trial 402 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.13172545088985596, 'n_estimators': 84, 'max_depth': 7, 'min_child_weight': 3.657939534507465, 'subsample': 0.9760025881184871, 'colsample_bytree': 0.899293609679316, 'colsample_bylevel': 0.6364233016739, 'gamma': 0.42505006522905325, 'reg_alpha': 0.34857346604087336, 'reg_lambda': 0.2624753929411561}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:46:48] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:46:48] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:46:48,793]\u001b[0m Trial 403 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.053032929270623745, 'n_estimators': 90, 'max_depth': 3, 'min_child_weight': 6.701622149583654, 'subsample': 0.6538844738343542, 'colsample_bytree': 0.7216203728953777, 'colsample_bylevel': 0.6839499760170176, 'gamma': 0.19329619698263414, 'reg_alpha': 0.18548707257054708, 'reg_lambda': 0.8629293919677016}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:46:48,960]\u001b[0m Trial 404 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.05419185327031254, 'n_estimators': 90, 'max_depth': 3, 'min_child_weight': 6.460477360264891, 'subsample': 0.6537150352044978, 'colsample_bytree': 0.749127848338678, 'colsample_bylevel': 0.7441498897118246, 'gamma': 0.2013153719440248, 'reg_alpha': 0.18296160453371527, 'reg_lambda': 0.8579821981005895}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:46:48] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:46:49] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:46:49,135]\u001b[0m Trial 405 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.12308573850645088, 'n_estimators': 95, 'max_depth': 6, 'min_child_weight': 4.254464759591335, 'subsample': 0.6860323661964339, 'colsample_bytree': 0.803240448979313, 'colsample_bylevel': 0.5943038297902251, 'gamma': 0.46987910487418794, 'reg_alpha': 0.22499754609901618, 'reg_lambda': 0.4910722999034659}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:46:49,299]\u001b[0m Trial 406 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.07599382307441611, 'n_estimators': 88, 'max_depth': 6, 'min_child_weight': 8.228992715545257, 'subsample': 0.6375894473955096, 'colsample_bytree': 0.7989838421075349, 'colsample_bylevel': 0.8335973293321384, 'gamma': 0.3645356217010471, 'reg_alpha': 0.2120396329981486, 'reg_lambda': 0.2834016336774037}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:46:49] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:46:49] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:46:49,474]\u001b[0m Trial 407 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.12125802056988705, 'n_estimators': 95, 'max_depth': 6, 'min_child_weight': 1.0068311802737535, 'subsample': 0.8300327961039452, 'colsample_bytree': 0.8244544623046227, 'colsample_bylevel': 0.6919662729763969, 'gamma': 0.3841746621639749, 'reg_alpha': 0.314428960647708, 'reg_lambda': 0.3596679143686496}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:46:49,649]\u001b[0m Trial 408 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.12505329464374484, 'n_estimators': 95, 'max_depth': 6, 'min_child_weight': 1.38470204010731, 'subsample': 0.9166389214288883, 'colsample_bytree': 0.9556486666160493, 'colsample_bylevel': 0.699954097657458, 'gamma': 0.2722409091963718, 'reg_alpha': 0.3165804851422237, 'reg_lambda': 0.9848661486269622}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:46:49] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:46:49] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:46:49,821]\u001b[0m Trial 409 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.1191137376494271, 'n_estimators': 94, 'max_depth': 6, 'min_child_weight': 1.3217039419564107, 'subsample': 0.8364362730506061, 'colsample_bytree': 0.7945146600385143, 'colsample_bylevel': 0.7102879500783285, 'gamma': 0.25502437922357535, 'reg_alpha': 0.3130148834148008, 'reg_lambda': 0.36587640319879255}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:46:49,994]\u001b[0m Trial 410 finished with value: 0.3333333333333333 and parameters: {'booster': 'gblinear', 'learning_rate': 0.11488320324911094, 'n_estimators': 95, 'max_depth': 7, 'min_child_weight': 2.3246898584210625, 'subsample': 0.9877103937814103, 'colsample_bytree': 0.8117477495638483, 'colsample_bylevel': 0.7156980473439931, 'gamma': 0.27294489850965503, 'reg_alpha': 0.36448996702348446, 'reg_lambda': 0.968885403952161}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:46:49] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:46:50] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:46:50,171]\u001b[0m Trial 411 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.14517760459417037, 'n_estimators': 97, 'max_depth': 8, 'min_child_weight': 9.919745800911834, 'subsample': 0.9068049693291242, 'colsample_bytree': 0.93559601560876, 'colsample_bylevel': 0.9303077084744055, 'gamma': 0.11366395326028758, 'reg_alpha': 0.23777546029432542, 'reg_lambda': 0.9154246771136434}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:46:50,353]\u001b[0m Trial 412 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.08950197717294774, 'n_estimators': 100, 'max_depth': 4, 'min_child_weight': 7.211422173808539, 'subsample': 0.9553806801139743, 'colsample_bytree': 0.594764098655513, 'colsample_bylevel': 0.849049588633541, 'gamma': 0.4112497980562462, 'reg_alpha': 0.2719207831411816, 'reg_lambda': 0.9844979001497028}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:46:50] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:46:50] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:46:50,517]\u001b[0m Trial 413 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.13543074042379805, 'n_estimators': 85, 'max_depth': 4, 'min_child_weight': 5.024605060129088, 'subsample': 0.9684427050503467, 'colsample_bytree': 0.9133924510599785, 'colsample_bylevel': 0.56578271800012, 'gamma': 0.08048596689733542, 'reg_alpha': 0.2842360166904333, 'reg_lambda': 0.994989523644714}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:46:50,681]\u001b[0m Trial 414 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.13434818862214148, 'n_estimators': 85, 'max_depth': 7, 'min_child_weight': 5.037761647198253, 'subsample': 0.9365886966211098, 'colsample_bytree': 0.8742265470347603, 'colsample_bylevel': 0.7950175090278964, 'gamma': 0.09198995749104588, 'reg_alpha': 0.29410136139610293, 'reg_lambda': 0.8172130922309387}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:46:50] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:46:50] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:46:50,863]\u001b[0m Trial 415 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.09979707124742138, 'n_estimators': 100, 'max_depth': 4, 'min_child_weight': 7.158183065800331, 'subsample': 0.835247615728245, 'colsample_bytree': 0.5792555416916392, 'colsample_bylevel': 0.6781873742936837, 'gamma': 0.09078666628136221, 'reg_alpha': 0.2741235256979674, 'reg_lambda': 0.9547533538915834}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:46:51,043]\u001b[0m Trial 416 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.07279904693692871, 'n_estimators': 100, 'max_depth': 4, 'min_child_weight': 7.516149360067038, 'subsample': 0.8469370477107677, 'colsample_bytree': 0.8279019778510897, 'colsample_bylevel': 0.6751223267689758, 'gamma': 0.10497055985881476, 'reg_alpha': 0.2696453221389736, 'reg_lambda': 0.970590224086253}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:46:50] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:46:51] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:46:51,221]\u001b[0m Trial 417 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.09746177453063266, 'n_estimators': 98, 'max_depth': 10, 'min_child_weight': 4.533641929944804, 'subsample': 0.7516439366887925, 'colsample_bytree': 0.6740925900095364, 'colsample_bylevel': 0.5206325575236743, 'gamma': 0.12855561954492506, 'reg_alpha': 0.24741748560616034, 'reg_lambda': 0.9676822595297493}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:46:51,400]\u001b[0m Trial 418 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.072502457207058, 'n_estimators': 98, 'max_depth': 10, 'min_child_weight': 4.790202702014366, 'subsample': 0.8956251267261982, 'colsample_bytree': 0.9987677002537144, 'colsample_bylevel': 0.5126975053992916, 'gamma': 0.1381760398066273, 'reg_alpha': 0.2325843982103608, 'reg_lambda': 0.10572009377954207}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:46:51] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:46:51] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:46:51,581]\u001b[0m Trial 419 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.09403220616288646, 'n_estimators': 98, 'max_depth': 10, 'min_child_weight': 4.543570233440142, 'subsample': 0.7561300282926409, 'colsample_bytree': 0.6736866696965592, 'colsample_bylevel': 0.81359168157377, 'gamma': 0.13923725664678077, 'reg_alpha': 0.24148395590869529, 'reg_lambda': 0.5369833753905828}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:46:51,793]\u001b[0m Trial 420 finished with value: 0.3333333333333333 and parameters: {'booster': 'dart', 'learning_rate': 0.09436102708346829, 'n_estimators': 98, 'max_depth': 10, 'min_child_weight': 4.85663015811111, 'subsample': 0.734816356260407, 'colsample_bytree': 0.6777697236571588, 'colsample_bylevel': 0.817125312615518, 'gamma': 0.1659381397331398, 'reg_alpha': 0.2505041366676633, 'reg_lambda': 0.13299696439557107}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:46:51,969]\u001b[0m Trial 421 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.09388120846413954, 'n_estimators': 97, 'max_depth': 10, 'min_child_weight': 1.462192064411536, 'subsample': 0.7344501035460758, 'colsample_bytree': 0.6699124109424832, 'colsample_bylevel': 0.5461046988689446, 'gamma': 0.14687838352440283, 'reg_alpha': 0.23259729294454065, 'reg_lambda': 0.5401956417355809}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:46:51] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:46:52] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:46:52,151]\u001b[0m Trial 422 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.07811360849478538, 'n_estimators': 100, 'max_depth': 3, 'min_child_weight': 7.068082596846653, 'subsample': 0.8422607943546041, 'colsample_bytree': 0.594414111747536, 'colsample_bylevel': 0.6724090933756648, 'gamma': 0.1724305304356637, 'reg_alpha': 0.30052859066726423, 'reg_lambda': 0.8292646928165145}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:46:52,322]\u001b[0m Trial 423 finished with value: 0.8285714285714286 and parameters: {'booster': 'gblinear', 'learning_rate': 0.1873748782271037, 'n_estimators': 92, 'max_depth': 3, 'min_child_weight': 6.624544575264234, 'subsample': 0.6223614459582554, 'colsample_bytree': 0.7363727543741261, 'colsample_bylevel': 0.601454149608429, 'gamma': 0.21822082230930195, 'reg_alpha': 0.17476017481915435, 'reg_lambda': 0.519249296724584}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:46:52] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:46:52,493]\u001b[0m Trial 424 finished with value: 0.3333333333333333 and parameters: {'booster': 'gbtree', 'learning_rate': 0.10702302207987593, 'n_estimators': 90, 'max_depth': 10, 'min_child_weight': 5.74546579007608, 'subsample': 0.6286967485568343, 'colsample_bytree': 0.5127953222109348, 'colsample_bylevel': 0.9989523863741426, 'gamma': 0.06329466338857992, 'reg_alpha': 0.19990248620806353, 'reg_lambda': 0.513117782185896}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:46:52,652]\u001b[0m Trial 425 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.061046888570878505, 'n_estimators': 83, 'max_depth': 3, 'min_child_weight': 6.702398941897384, 'subsample': 0.5372265266506862, 'colsample_bytree': 0.7141441576707248, 'colsample_bylevel': 0.6056025098027895, 'gamma': 0.2099745920542357, 'reg_alpha': 0.33110754322252595, 'reg_lambda': 0.7672951570381711}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:46:52] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:46:52] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:46:52,806]\u001b[0m Trial 426 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.1116009396616662, 'n_estimators': 80, 'max_depth': 10, 'min_child_weight': 6.861585958413934, 'subsample': 0.6669200407121594, 'colsample_bytree': 0.6236119704839156, 'colsample_bylevel': 0.5992643768695879, 'gamma': 0.21247407507212712, 'reg_alpha': 0.1770307436494907, 'reg_lambda': 0.7546369618592657}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:46:52,971]\u001b[0m Trial 427 finished with value: 0.6249999999999999 and parameters: {'booster': 'gblinear', 'learning_rate': 0.10799576090399536, 'n_estimators': 89, 'max_depth': 10, 'min_child_weight': 6.792489695766614, 'subsample': 0.6705605257735092, 'colsample_bytree': 0.6253100417706255, 'colsample_bylevel': 0.5931723330881765, 'gamma': 0.055847616487706635, 'reg_alpha': 0.1706334399970192, 'reg_lambda': 0.5218714437526764}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:46:52] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:46:53] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:46:53,131]\u001b[0m Trial 428 finished with value: 0.3333333333333333 and parameters: {'booster': 'gblinear', 'learning_rate': 0.12937793432379885, 'n_estimators': 84, 'max_depth': 9, 'min_child_weight': 2.525901326781347, 'subsample': 0.5251073309885399, 'colsample_bytree': 0.5167383179390023, 'colsample_bylevel': 0.5824985778126358, 'gamma': 0.015317454048395088, 'reg_alpha': 0.7582415661021287, 'reg_lambda': 0.8451071865155597}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:46:53,310]\u001b[0m Trial 429 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.06855047927811293, 'n_estimators': 99, 'max_depth': 3, 'min_child_weight': 1.1993752589586475, 'subsample': 0.7747257080875087, 'colsample_bytree': 0.6893697559473887, 'colsample_bylevel': 0.545769276737354, 'gamma': 0.23055524538577568, 'reg_alpha': 0.2788095123252221, 'reg_lambda': 0.4057442721131502}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:46:53] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:46:53] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:46:53,491]\u001b[0m Trial 430 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.07027508987488917, 'n_estimators': 100, 'max_depth': 4, 'min_child_weight': 7.117992755095449, 'subsample': 0.8379848293642375, 'colsample_bytree': 0.5847915824030293, 'colsample_bylevel': 0.6825556187276224, 'gamma': 0.08798448686735277, 'reg_alpha': 0.29735962556729856, 'reg_lambda': 0.931419911469409}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:46:53,673]\u001b[0m Trial 431 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.09856384606137543, 'n_estimators': 100, 'max_depth': 4, 'min_child_weight': 7.326043114954106, 'subsample': 0.8537714006984831, 'colsample_bytree': 0.5949775604548978, 'colsample_bylevel': 0.6869559119025314, 'gamma': 0.0898266144306246, 'reg_alpha': 0.2587435161093111, 'reg_lambda': 0.9054004849912659}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:46:53] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:46:53] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:46:53,859]\u001b[0m Trial 432 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.0993781556567405, 'n_estimators': 99, 'max_depth': 4, 'min_child_weight': 7.167494805699328, 'subsample': 0.8368431620864999, 'colsample_bytree': 0.5790289158997753, 'colsample_bylevel': 0.6707768300253495, 'gamma': 0.09994363455831143, 'reg_alpha': 0.2846497656769743, 'reg_lambda': 0.8904517472547101}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:46:54,030]\u001b[0m Trial 433 finished with value: 0.6249999999999999 and parameters: {'booster': 'gblinear', 'learning_rate': 0.034780848610946165, 'n_estimators': 85, 'max_depth': 10, 'min_child_weight': 3.8665724912409973, 'subsample': 0.6106869131967311, 'colsample_bytree': 0.6202512310483674, 'colsample_bylevel': 0.6120763317450064, 'gamma': 0.3389409629362894, 'reg_alpha': 0.3150489920165747, 'reg_lambda': 0.9030853993747426}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:46:53] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:46:54] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:46:54,190]\u001b[0m Trial 434 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.1125366939036619, 'n_estimators': 83, 'max_depth': 6, 'min_child_weight': 3.439838392880226, 'subsample': 0.9241504136513392, 'colsample_bytree': 0.9078847253847282, 'colsample_bylevel': 0.6521794435714211, 'gamma': 0.24718273838216456, 'reg_alpha': 0.15439175479348458, 'reg_lambda': 0.884870290450458}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:46:54,370]\u001b[0m Trial 435 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.07894278952017135, 'n_estimators': 100, 'max_depth': 4, 'min_child_weight': 7.106127825885486, 'subsample': 0.8613384393811283, 'colsample_bytree': 0.6036395383656473, 'colsample_bylevel': 0.7539001692388748, 'gamma': 0.07487891551563083, 'reg_alpha': 0.256544381182436, 'reg_lambda': 0.8354551301611209}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:46:54] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:46:54] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:46:54,557]\u001b[0m Trial 436 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.18117263390612454, 'n_estimators': 100, 'max_depth': 4, 'min_child_weight': 6.931963279354879, 'subsample': 0.8542528770321437, 'colsample_bytree': 0.5689823619597, 'colsample_bylevel': 0.7671896059160481, 'gamma': 0.0832422176810185, 'reg_alpha': 0.2624226106246937, 'reg_lambda': 0.8278158190297195}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:46:54,721]\u001b[0m Trial 437 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.1276511483170915, 'n_estimators': 85, 'max_depth': 6, 'min_child_weight': 3.6942684344524244, 'subsample': 0.9412618302201833, 'colsample_bytree': 0.8891249405144415, 'colsample_bylevel': 0.7233028239163781, 'gamma': 0.4254173168272703, 'reg_alpha': 0.19199608522909722, 'reg_lambda': 0.9341398890159094}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:46:54] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:46:54] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:46:54,890]\u001b[0m Trial 438 finished with value: 0.8285714285714286 and parameters: {'booster': 'gblinear', 'learning_rate': 0.18236047081935147, 'n_estimators': 84, 'max_depth': 6, 'min_child_weight': 1.14070337313026, 'subsample': 0.9365174376909841, 'colsample_bytree': 0.8875811202862937, 'colsample_bylevel': 0.7165589575870984, 'gamma': 0.41768832271406214, 'reg_alpha': 0.20110154653246343, 'reg_lambda': 0.23950131532256777}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:46:55,050]\u001b[0m Trial 439 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.10520005803547253, 'n_estimators': 81, 'max_depth': 10, 'min_child_weight': 4.319053678035999, 'subsample': 0.5677902826384047, 'colsample_bytree': 0.5077740476014737, 'colsample_bylevel': 0.9489903684105956, 'gamma': 0.06276868548287054, 'reg_alpha': 0.3230770383970129, 'reg_lambda': 0.7870834340293675}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:46:54] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:46:55] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:46:55,218]\u001b[0m Trial 440 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.06474371409805038, 'n_estimators': 89, 'max_depth': 4, 'min_child_weight': 1.8121679602351495, 'subsample': 0.8270713622437262, 'colsample_bytree': 0.6117906611767732, 'colsample_bylevel': 0.697175336954963, 'gamma': 0.18085669326459286, 'reg_alpha': 0.27993969165527494, 'reg_lambda': 0.7982803507205712}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:46:55,373]\u001b[0m Trial 441 finished with value: 0.3333333333333333 and parameters: {'booster': 'gblinear', 'learning_rate': 0.10523801669503353, 'n_estimators': 80, 'max_depth': 9, 'min_child_weight': 8.196211630197757, 'subsample': 0.512613378866203, 'colsample_bytree': 0.6954722748184026, 'colsample_bylevel': 0.9874775487727248, 'gamma': 0.07473321753410346, 'reg_alpha': 0.5174030655797995, 'reg_lambda': 0.7961527868474749}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:46:55] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:46:55] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:46:55,531]\u001b[0m Trial 442 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.16640595684546797, 'n_estimators': 82, 'max_depth': 8, 'min_child_weight': 8.793536159647832, 'subsample': 0.5518763853239749, 'colsample_bytree': 0.684875066725718, 'colsample_bylevel': 0.5923378677273726, 'gamma': 0.06790631511272617, 'reg_alpha': 0.29608232767703135, 'reg_lambda': 0.17155963971311267}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:46:55,712]\u001b[0m Trial 443 finished with value: 0.3333333333333333 and parameters: {'booster': 'gblinear', 'learning_rate': 0.07430175624974042, 'n_estimators': 100, 'max_depth': 8, 'min_child_weight': 4.839703023661371, 'subsample': 0.8348119909890895, 'colsample_bytree': 0.7329972256324059, 'colsample_bylevel': 0.6997463142370802, 'gamma': 0.4666045660909054, 'reg_alpha': 0.4150788776714474, 'reg_lambda': 0.8443136215695564}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:46:55] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:46:55,891]\u001b[0m Trial 444 finished with value: 0.3333333333333333 and parameters: {'booster': 'gbtree', 'learning_rate': 0.1700116460663067, 'n_estimators': 96, 'max_depth': 7, 'min_child_weight': 5.37110865546907, 'subsample': 0.8849294602288842, 'colsample_bytree': 0.7864911432593488, 'colsample_bylevel': 0.5745730777678185, 'gamma': 0.15136479981444917, 'reg_alpha': 0.3070895826635368, 'reg_lambda': 0.3190088660760455}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:46:56,084]\u001b[0m Trial 445 finished with value: 0.3333333333333333 and parameters: {'booster': 'dart', 'learning_rate': 0.140559417901003, 'n_estimators': 88, 'max_depth': 5, 'min_child_weight': 6.190579848744001, 'subsample': 0.8762386296519712, 'colsample_bytree': 0.7720527105308161, 'colsample_bylevel': 0.7400112185646766, 'gamma': 0.03401577776003575, 'reg_alpha': 0.21237404733187001, 'reg_lambda': 0.4968294807309196}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:46:56,259]\u001b[0m Trial 446 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.1753610490555628, 'n_estimators': 94, 'max_depth': 7, 'min_child_weight': 6.104983158468559, 'subsample': 0.9924628523049959, 'colsample_bytree': 0.7843028456575452, 'colsample_bylevel': 0.898321554063381, 'gamma': 0.035026473364154216, 'reg_alpha': 0.2860361828938005, 'reg_lambda': 0.46313250567200936}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:46:56] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:46:56] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:46:56,439]\u001b[0m Trial 447 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.07160890590774836, 'n_estimators': 99, 'max_depth': 8, 'min_child_weight': 4.514362535902792, 'subsample': 0.8189974898780032, 'colsample_bytree': 0.7488072747164386, 'colsample_bylevel': 0.6893515105412028, 'gamma': 0.4731190960942006, 'reg_alpha': 0.3628845681756344, 'reg_lambda': 0.3476859239369714}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:46:56,620]\u001b[0m Trial 448 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.15964235863888013, 'n_estimators': 100, 'max_depth': 8, 'min_child_weight': 4.138116729771481, 'subsample': 0.8209999471272856, 'colsample_bytree': 0.7550928677837904, 'colsample_bylevel': 0.7059834056211522, 'gamma': 0.4609416223478668, 'reg_alpha': 0.3491764481683799, 'reg_lambda': 0.3780085620230758}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:46:56] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:46:56] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:46:56,783]\u001b[0m Trial 449 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.11684435723498367, 'n_estimators': 86, 'max_depth': 4, 'min_child_weight': 7.393132216877816, 'subsample': 0.8405587847889963, 'colsample_bytree': 0.5997251864109602, 'colsample_bylevel': 0.7232920458501735, 'gamma': 0.49832828614081903, 'reg_alpha': 0.33244598047913065, 'reg_lambda': 0.3665915982459316}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:46:56,963]\u001b[0m Trial 450 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.10584209778257576, 'n_estimators': 100, 'max_depth': 4, 'min_child_weight': 3.9365724083245066, 'subsample': 0.8562665946786795, 'colsample_bytree': 0.5868640867738235, 'colsample_bylevel': 0.9753872594308669, 'gamma': 0.4878491608839108, 'reg_alpha': 0.3288583073359527, 'reg_lambda': 0.34933308390313966}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:46:56] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:46:57] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:46:57,128]\u001b[0m Trial 451 finished with value: 0.3333333333333333 and parameters: {'booster': 'gblinear', 'learning_rate': 0.01430208871966504, 'n_estimators': 87, 'max_depth': 7, 'min_child_weight': 8.369322200889826, 'subsample': 0.9266756030260556, 'colsample_bytree': 0.9261243321373511, 'colsample_bylevel': 0.9781669142255531, 'gamma': 0.07074542689692073, 'reg_alpha': 0.3343079457850693, 'reg_lambda': 0.6419039561385046}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:46:57,305]\u001b[0m Trial 452 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.11500542065493743, 'n_estimators': 94, 'max_depth': 7, 'min_child_weight': 9.04384007773329, 'subsample': 0.7221677282309321, 'colsample_bytree': 0.9452428934951314, 'colsample_bylevel': 0.6215499794527364, 'gamma': 0.0030107808799656596, 'reg_alpha': 0.3737059177593562, 'reg_lambda': 0.3336487417744004}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:46:57] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:46:57] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:46:57,480]\u001b[0m Trial 453 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.155421413788594, 'n_estimators': 94, 'max_depth': 7, 'min_child_weight': 9.296184180761742, 'subsample': 0.7277521137095125, 'colsample_bytree': 0.9633076088787416, 'colsample_bylevel': 0.8679077447257549, 'gamma': 0.4557118711587046, 'reg_alpha': 0.377765931283862, 'reg_lambda': 0.27514167559686026}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:46:57,654]\u001b[0m Trial 454 finished with value: 0.8285714285714286 and parameters: {'booster': 'gblinear', 'learning_rate': 0.15164633975269867, 'n_estimators': 93, 'max_depth': 7, 'min_child_weight': 9.521439195329751, 'subsample': 0.7190582882252162, 'colsample_bytree': 0.9646091619550363, 'colsample_bylevel': 0.8688746956995832, 'gamma': 0.43324548595675905, 'reg_alpha': 0.388269819989957, 'reg_lambda': 0.2595864996239456}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:46:57] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:46:57] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:46:57,819]\u001b[0m Trial 455 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.1289285564448774, 'n_estimators': 87, 'max_depth': 9, 'min_child_weight': 2.7146945392985087, 'subsample': 0.9994834957472858, 'colsample_bytree': 0.8747225636179164, 'colsample_bylevel': 0.5131692134344044, 'gamma': 0.40437411363753706, 'reg_alpha': 0.260226039127898, 'reg_lambda': 0.8647590811657531}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:46:57,988]\u001b[0m Trial 456 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.14326291376464897, 'n_estimators': 87, 'max_depth': 9, 'min_child_weight': 2.655728916986266, 'subsample': 0.9837420919079242, 'colsample_bytree': 0.8648463712840134, 'colsample_bylevel': 0.9663026046558503, 'gamma': 0.07899726640470214, 'reg_alpha': 0.2663279973269614, 'reg_lambda': 0.392470481359421}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:46:57] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:46:58] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:46:58,161]\u001b[0m Trial 457 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.08656384482568966, 'n_estimators': 86, 'max_depth': 4, 'min_child_weight': 3.8774980203652243, 'subsample': 0.8660335129122902, 'colsample_bytree': 0.5678017423866503, 'colsample_bylevel': 0.9929751321689294, 'gamma': 0.2906043111949362, 'reg_alpha': 0.2889157754537236, 'reg_lambda': 0.8177485765549773}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:46:58,317]\u001b[0m Trial 458 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.08114980648604092, 'n_estimators': 80, 'max_depth': 8, 'min_child_weight': 3.7254304879168925, 'subsample': 0.8566890353025906, 'colsample_bytree': 0.5636221950505702, 'colsample_bylevel': 0.9999437016418404, 'gamma': 0.27792047656080054, 'reg_alpha': 0.3009412214917776, 'reg_lambda': 0.7957225917287699}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:46:58] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:46:58] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:46:58,481]\u001b[0m Trial 459 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.13721399934385292, 'n_estimators': 85, 'max_depth': 7, 'min_child_weight': 3.7593354733771744, 'subsample': 0.9837793453626359, 'colsample_bytree': 0.8593006841368125, 'colsample_bylevel': 0.5012630311341445, 'gamma': 0.08174140358656519, 'reg_alpha': 0.2861374269917383, 'reg_lambda': 0.2371511753746239}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:46:58,644]\u001b[0m Trial 460 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.1379061129949693, 'n_estimators': 85, 'max_depth': 7, 'min_child_weight': 5.05603074748598, 'subsample': 0.5941221186852743, 'colsample_bytree': 0.8526302248682763, 'colsample_bylevel': 0.9558050510391443, 'gamma': 0.07995720851199947, 'reg_alpha': 0.277469539509065, 'reg_lambda': 0.21869571473016847}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:46:58] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:46:58] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:46:58,811]\u001b[0m Trial 461 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.10227646388137485, 'n_estimators': 88, 'max_depth': 8, 'min_child_weight': 6.2850286893228, 'subsample': 0.9399726456792797, 'colsample_bytree': 0.5500722829940966, 'colsample_bylevel': 0.9602046762544045, 'gamma': 0.3950955316823181, 'reg_alpha': 0.30694223295807294, 'reg_lambda': 0.5696372341151937}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:46:58,976]\u001b[0m Trial 462 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.110530527319039, 'n_estimators': 88, 'max_depth': 8, 'min_child_weight': 6.160093797290335, 'subsample': 0.7922677560556851, 'colsample_bytree': 0.5286398341032236, 'colsample_bylevel': 0.9988866036415495, 'gamma': 0.3812735678095285, 'reg_alpha': 0.2748966162967743, 'reg_lambda': 0.6027167348114015}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:46:58] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:46:59] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:46:59,156]\u001b[0m Trial 463 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.10812066223353199, 'n_estimators': 98, 'max_depth': 7, 'min_child_weight': 8.547851956667417, 'subsample': 0.6825821919414813, 'colsample_bytree': 0.8374902135228128, 'colsample_bylevel': 0.5289977970384009, 'gamma': 0.3740246054859987, 'reg_alpha': 0.22213663298835656, 'reg_lambda': 0.2950370380617564}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:46:59,366]\u001b[0m Trial 464 finished with value: 0.3333333333333333 and parameters: {'booster': 'dart', 'learning_rate': 0.1321302802568443, 'n_estimators': 98, 'max_depth': 7, 'min_child_weight': 9.067454508695358, 'subsample': 0.6819813037811352, 'colsample_bytree': 0.5396741442022832, 'colsample_bylevel': 0.5262073811060421, 'gamma': 0.4155988383435573, 'reg_alpha': 0.239576900884824, 'reg_lambda': 0.2589339582371486}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:46:59,543]\u001b[0m Trial 465 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.12519104006862272, 'n_estimators': 96, 'max_depth': 6, 'min_child_weight': 9.932066699994058, 'subsample': 0.9515456221369071, 'colsample_bytree': 0.8188974665424514, 'colsample_bylevel': 0.8468024126962675, 'gamma': 0.40078741909025006, 'reg_alpha': 0.22796562809276755, 'reg_lambda': 0.980995951920929}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:46:59] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:46:59] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:46:59,708]\u001b[0m Trial 466 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.10455589537945574, 'n_estimators': 86, 'max_depth': 3, 'min_child_weight': 6.2278393784402875, 'subsample': 0.9585197346998754, 'colsample_bytree': 0.5349758280319733, 'colsample_bylevel': 0.9932126247229236, 'gamma': 0.48032577734763254, 'reg_alpha': 0.3484321921809001, 'reg_lambda': 0.620487493728442}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:46:59,872]\u001b[0m Trial 467 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.08256263437139476, 'n_estimators': 86, 'max_depth': 5, 'min_child_weight': 3.5745479375009754, 'subsample': 0.9241730809963298, 'colsample_bytree': 0.545948187447925, 'colsample_bylevel': 0.9857567107051945, 'gamma': 0.27307951583480494, 'reg_alpha': 0.31399291901952003, 'reg_lambda': 0.3540754872995932}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:46:59] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:46:59] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:47:00,036]\u001b[0m Trial 468 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.11959970438016276, 'n_estimators': 86, 'max_depth': 8, 'min_child_weight': 6.523432273993029, 'subsample': 0.9488527975805164, 'colsample_bytree': 0.7481320047036516, 'colsample_bylevel': 0.9870697888009016, 'gamma': 0.4817520901034105, 'reg_alpha': 0.32932156141168484, 'reg_lambda': 0.33459935940344837}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:47:00,203]\u001b[0m Trial 469 finished with value: 0.3333333333333333 and parameters: {'booster': 'gbtree', 'learning_rate': 0.11946480842826693, 'n_estimators': 86, 'max_depth': 8, 'min_child_weight': 4.082864243923002, 'subsample': 0.9591460404017869, 'colsample_bytree': 0.728285558032023, 'colsample_bylevel': 0.9765748254722109, 'gamma': 0.2625916350223778, 'reg_alpha': 0.34698665610591767, 'reg_lambda': 0.7152711234733778}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:47:00,378]\u001b[0m Trial 470 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.0989686432274072, 'n_estimators': 95, 'max_depth': 4, 'min_child_weight': 7.6161208988917375, 'subsample': 0.9767778787235513, 'colsample_bytree': 0.8202033025509613, 'colsample_bylevel': 0.6745993231793022, 'gamma': 0.2588607553715398, 'reg_alpha': 0.24716702581311104, 'reg_lambda': 0.9643633739997088}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:47:00] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:47:00] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:47:00,561]\u001b[0m Trial 471 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.14827180382756336, 'n_estimators': 100, 'max_depth': 4, 'min_child_weight': 7.9066554897376555, 'subsample': 0.8967554633120457, 'colsample_bytree': 0.9737384657981994, 'colsample_bylevel': 0.666991091452024, 'gamma': 0.11902199081677726, 'reg_alpha': 0.2511585712983133, 'reg_lambda': 0.4222673249500812}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:47:00,742]\u001b[0m Trial 472 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.09963261672236365, 'n_estimators': 99, 'max_depth': 4, 'min_child_weight': 5.619142830493339, 'subsample': 0.8711112355064106, 'colsample_bytree': 0.9676128784769696, 'colsample_bylevel': 0.678431776829562, 'gamma': 0.11980944181413945, 'reg_alpha': 0.2454729689154351, 'reg_lambda': 0.44126223506634304}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:47:00] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:47:00] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:47:00,907]\u001b[0m Trial 473 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.11275874306541495, 'n_estimators': 86, 'max_depth': 8, 'min_child_weight': 6.361371719175276, 'subsample': 0.6596162309784381, 'colsample_bytree': 0.7420532428744052, 'colsample_bylevel': 0.9739503284492013, 'gamma': 0.45370675894255136, 'reg_alpha': 0.3234542829088044, 'reg_lambda': 0.8987687606924131}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:47:01,071]\u001b[0m Trial 474 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.12248449023724844, 'n_estimators': 86, 'max_depth': 5, 'min_child_weight': 6.43767410511727, 'subsample': 0.6445149083845797, 'colsample_bytree': 0.7273634440766799, 'colsample_bylevel': 0.9877365666778021, 'gamma': 0.2838202609078674, 'reg_alpha': 0.3190566964807782, 'reg_lambda': 0.8792015994632634}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:47:00] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:47:01] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:47:01,236]\u001b[0m Trial 475 finished with value: 0.3333333333333333 and parameters: {'booster': 'gblinear', 'learning_rate': 0.025539184365553684, 'n_estimators': 87, 'max_depth': 8, 'min_child_weight': 6.52644448354095, 'subsample': 0.9697169940411516, 'colsample_bytree': 0.7354655878042538, 'colsample_bylevel': 0.7107249925298585, 'gamma': 0.4300224973941569, 'reg_alpha': 0.33208584140760866, 'reg_lambda': 0.6590220880563029}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:47:01,417]\u001b[0m Trial 476 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.08913167297377739, 'n_estimators': 99, 'max_depth': 4, 'min_child_weight': 7.0425950705538405, 'subsample': 0.8404180211668577, 'colsample_bytree': 0.5779670000234648, 'colsample_bylevel': 0.6595740614640164, 'gamma': 0.026993332431844878, 'reg_alpha': 0.2593486734128496, 'reg_lambda': 0.9871784803730956}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:47:01] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:47:01] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:47:01,598]\u001b[0m Trial 477 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.09210077103940896, 'n_estimators': 99, 'max_depth': 4, 'min_child_weight': 7.051467717130885, 'subsample': 0.8444353372217116, 'colsample_bytree': 0.5809964861904987, 'colsample_bylevel': 0.6627660225439471, 'gamma': 0.016853335037156022, 'reg_alpha': 0.2600391683865605, 'reg_lambda': 0.9937907657406186}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:47:01,779]\u001b[0m Trial 478 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.09282087298042922, 'n_estimators': 99, 'max_depth': 4, 'min_child_weight': 6.851131041036911, 'subsample': 0.500931733316268, 'colsample_bytree': 0.5644202339563561, 'colsample_bylevel': 0.6610049630852783, 'gamma': 0.09678740102848926, 'reg_alpha': 0.2534350354154139, 'reg_lambda': 0.8262535795162721}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:47:01] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:47:01] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:47:01,941]\u001b[0m Trial 479 finished with value: 0.6249999999999999 and parameters: {'booster': 'gblinear', 'learning_rate': 0.025367920277003485, 'n_estimators': 85, 'max_depth': 8, 'min_child_weight': 6.369324895528634, 'subsample': 0.96243549957077, 'colsample_bytree': 0.9050867984935447, 'colsample_bylevel': 0.7194374283109136, 'gamma': 0.3950663238589266, 'reg_alpha': 0.3078116885591148, 'reg_lambda': 0.8317292340170075}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:47:02,116]\u001b[0m Trial 480 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.08908934318749932, 'n_estimators': 95, 'max_depth': 5, 'min_child_weight': 5.372196181631524, 'subsample': 0.987080079007654, 'colsample_bytree': 0.9437346871866438, 'colsample_bylevel': 0.628230089631453, 'gamma': 0.04227107843218987, 'reg_alpha': 0.2113484665440966, 'reg_lambda': 0.49019753187386816}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:47:01] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:47:02] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:47:02,279]\u001b[0m Trial 481 finished with value: 0.6249999999999999 and parameters: {'booster': 'gblinear', 'learning_rate': 0.02243121617605524, 'n_estimators': 86, 'max_depth': 7, 'min_child_weight': 6.336689668507258, 'subsample': 0.5247452254991383, 'colsample_bytree': 0.8826160617302756, 'colsample_bylevel': 0.5056316986322558, 'gamma': 0.40691561117367264, 'reg_alpha': 0.2976850836327334, 'reg_lambda': 0.8586582086048175}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:47:02,455]\u001b[0m Trial 482 finished with value: 0.8285714285714286 and parameters: {'booster': 'gblinear', 'learning_rate': 0.1519206992886721, 'n_estimators': 95, 'max_depth': 5, 'min_child_weight': 9.244537271613508, 'subsample': 0.7087951024083743, 'colsample_bytree': 0.9330629822420493, 'colsample_bylevel': 0.6212213705266779, 'gamma': 0.1322941958506396, 'reg_alpha': 0.20939236107357953, 'reg_lambda': 0.48653784565235064}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:47:02] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:47:02] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:47:02,633]\u001b[0m Trial 483 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.11007888820332548, 'n_estimators': 96, 'max_depth': 6, 'min_child_weight': 2.04428203814114, 'subsample': 0.9115395507025692, 'colsample_bytree': 0.9878608481038618, 'colsample_bylevel': 0.5560665232230312, 'gamma': 0.24094032490552106, 'reg_alpha': 0.19241441747102045, 'reg_lambda': 0.46058965865531865}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:47:02,795]\u001b[0m Trial 484 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.11194767407947605, 'n_estimators': 85, 'max_depth': 8, 'min_child_weight': 5.973713649254495, 'subsample': 0.9334460244811155, 'colsample_bytree': 0.8798434100448227, 'colsample_bylevel': 0.5147137253859645, 'gamma': 0.01670276807763644, 'reg_alpha': 0.29832735837546553, 'reg_lambda': 0.6624098049521169}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:47:02] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:47:02] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:47:02,957]\u001b[0m Trial 485 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.1165896918814137, 'n_estimators': 84, 'max_depth': 8, 'min_child_weight': 6.404095579277981, 'subsample': 0.5193527646955723, 'colsample_bytree': 0.9520033000778001, 'colsample_bylevel': 0.5043664927154377, 'gamma': 0.024877814460574063, 'reg_alpha': 0.30486191355212044, 'reg_lambda': 0.8416087049830577}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:47:03,121]\u001b[0m Trial 486 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.10726488648193835, 'n_estimators': 86, 'max_depth': 8, 'min_child_weight': 6.683819339260422, 'subsample': 0.5410280793533957, 'colsample_bytree': 0.9558756138611961, 'colsample_bylevel': 0.5109099370018995, 'gamma': 0.025080843787231095, 'reg_alpha': 0.32115593356387334, 'reg_lambda': 0.8620737667096912}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:47:02] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:47:03] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:47:03,281]\u001b[0m Trial 487 finished with value: 0.6249999999999999 and parameters: {'booster': 'gblinear', 'learning_rate': 0.02010190441308001, 'n_estimators': 84, 'max_depth': 7, 'min_child_weight': 6.631536570489059, 'subsample': 0.6212670797083317, 'colsample_bytree': 0.765648740788801, 'colsample_bylevel': 0.5177794320183386, 'gamma': 0.03957505236311364, 'reg_alpha': 0.28786447515636937, 'reg_lambda': 0.8830311806038305}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:47:03,456]\u001b[0m Trial 488 finished with value: 0.8285714285714286 and parameters: {'booster': 'gblinear', 'learning_rate': 0.12281512661845742, 'n_estimators': 94, 'max_depth': 6, 'min_child_weight': 5.933013968973243, 'subsample': 0.7101385264136778, 'colsample_bytree': 0.7777983741782097, 'colsample_bylevel': 0.834876744031958, 'gamma': 0.36420636634755543, 'reg_alpha': 0.19520374661838652, 'reg_lambda': 0.43861109416591904}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:47:03] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:47:03,638]\u001b[0m Trial 489 finished with value: 0.8285714285714286 and parameters: {'booster': 'dart', 'learning_rate': 0.03598648083943476, 'n_estimators': 81, 'max_depth': 10, 'min_child_weight': 1.529139544030092, 'subsample': 0.5625855109608393, 'colsample_bytree': 0.9892981113446203, 'colsample_bylevel': 0.5633130526377875, 'gamma': 0.305049766611869, 'reg_alpha': 0.26931050511904653, 'reg_lambda': 0.40451335912152225}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:47:03,818]\u001b[0m Trial 490 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.03942498723105389, 'n_estimators': 97, 'max_depth': 10, 'min_child_weight': 1.0397974659457578, 'subsample': 0.5535001576481907, 'colsample_bytree': 0.9808090750456505, 'colsample_bylevel': 0.8895975859055889, 'gamma': 0.3122988873232125, 'reg_alpha': 0.2726604518412362, 'reg_lambda': 0.4025286400650729}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:47:03] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:47:03] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:47:04,019]\u001b[0m Trial 491 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.07786829163215, 'n_estimators': 100, 'max_depth': 3, 'min_child_weight': 6.9680738379080065, 'subsample': 0.8512276807595315, 'colsample_bytree': 0.8320870780191629, 'colsample_bylevel': 0.7443089393347148, 'gamma': 0.09341180900701226, 'reg_alpha': 0.23500176529692388, 'reg_lambda': 0.9214483281837802}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:47:04,209]\u001b[0m Trial 492 finished with value: 0.3333333333333333 and parameters: {'booster': 'gbtree', 'learning_rate': 0.07943637355934881, 'n_estimators': 100, 'max_depth': 3, 'min_child_weight': 7.2807658807741245, 'subsample': 0.8383012145281963, 'colsample_bytree': 0.5014687781389906, 'colsample_bylevel': 0.7240354267311544, 'gamma': 0.10102278695663909, 'reg_alpha': 0.4458400871365203, 'reg_lambda': 0.9281189348108758}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:47:04,394]\u001b[0m Trial 493 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.08413635275355161, 'n_estimators': 100, 'max_depth': 3, 'min_child_weight': 7.18260991844782, 'subsample': 0.8320283948010208, 'colsample_bytree': 0.8438713948665362, 'colsample_bylevel': 0.6831503860321195, 'gamma': 0.09983804362692204, 'reg_alpha': 0.22471018431497802, 'reg_lambda': 0.9378004516416246}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:47:04] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:47:04] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:47:04,562]\u001b[0m Trial 494 finished with value: 0.3333333333333333 and parameters: {'booster': 'gblinear', 'learning_rate': 0.11353266721183707, 'n_estimators': 84, 'max_depth': 10, 'min_child_weight': 6.56400899397321, 'subsample': 0.6629425637902072, 'colsample_bytree': 0.9986178141272243, 'colsample_bylevel': 0.9423893359717727, 'gamma': 0.0006895993546280482, 'reg_alpha': 0.876491432555985, 'reg_lambda': 0.8686557406447522}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:47:04,732]\u001b[0m Trial 495 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.06692668616248879, 'n_estimators': 89, 'max_depth': 4, 'min_child_weight': 1.8846392478947147, 'subsample': 0.8261198937593234, 'colsample_bytree': 0.5105919753417149, 'colsample_bylevel': 0.6834902341939053, 'gamma': 0.23730719662698185, 'reg_alpha': 0.22107862593264307, 'reg_lambda': 0.773206132146746}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:47:04] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:47:04] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:47:04,913]\u001b[0m Trial 496 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.11586327187782108, 'n_estimators': 90, 'max_depth': 10, 'min_child_weight': 6.0589707194693405, 'subsample': 0.6242930774488825, 'colsample_bytree': 0.7531045989852453, 'colsample_bylevel': 0.5196360923803452, 'gamma': 0.4118634705922661, 'reg_alpha': 0.17468708540871034, 'reg_lambda': 0.8665952078501186}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:47:05,090]\u001b[0m Trial 497 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.10718876482875969, 'n_estimators': 90, 'max_depth': 10, 'min_child_weight': 6.243840944928178, 'subsample': 0.6663102284135601, 'colsample_bytree': 0.7611067103657725, 'colsample_bylevel': 0.7743232486294422, 'gamma': 0.015455991402423217, 'reg_alpha': 0.2969899331353318, 'reg_lambda': 0.8748928482564131}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:47:04] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:47:05] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:47:05,255]\u001b[0m Trial 498 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.11012243822262531, 'n_estimators': 84, 'max_depth': 10, 'min_child_weight': 6.028395612031941, 'subsample': 0.6710718729240881, 'colsample_bytree': 0.8851764215493252, 'colsample_bylevel': 0.9354032388056331, 'gamma': 0.031288384373495674, 'reg_alpha': 0.29779900431179374, 'reg_lambda': 0.836832808202379}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:47:05,427]\u001b[0m Trial 499 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.14877027274906357, 'n_estimators': 91, 'max_depth': 10, 'min_child_weight': 3.077311033114353, 'subsample': 0.570463274330409, 'colsample_bytree': 0.6550499996280179, 'colsample_bylevel': 0.9378142256732345, 'gamma': 0.30317118073856747, 'reg_alpha': 0.27581439828762133, 'reg_lambda': 0.43002171146915624}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:47:05] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:47:05] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:47:05,598]\u001b[0m Trial 500 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.030519848612680884, 'n_estimators': 91, 'max_depth': 10, 'min_child_weight': 2.8884555670972345, 'subsample': 0.5610189294053897, 'colsample_bytree': 0.6554409547046618, 'colsample_bylevel': 0.9159631341694782, 'gamma': 0.3238798791233855, 'reg_alpha': 0.2809800994532326, 'reg_lambda': 0.4352803092388239}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:47:05,767]\u001b[0m Trial 501 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.03253397371347606, 'n_estimators': 89, 'max_depth': 10, 'min_child_weight': 6.657403415732447, 'subsample': 0.6013674441139331, 'colsample_bytree': 0.8472529193032279, 'colsample_bylevel': 0.5037267731536936, 'gamma': 0.05396046573573527, 'reg_alpha': 0.2866196231884314, 'reg_lambda': 0.8183148383085093}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:47:05] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:47:05] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:47:05,938]\u001b[0m Trial 502 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.1055513844123131, 'n_estimators': 90, 'max_depth': 10, 'min_child_weight': 6.4429725607746064, 'subsample': 0.6182918658818681, 'colsample_bytree': 0.7550682918265629, 'colsample_bylevel': 0.948055342325328, 'gamma': 0.04217557508874741, 'reg_alpha': 0.29283713222177493, 'reg_lambda': 0.8484248173949309}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:47:06,109]\u001b[0m Trial 503 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.14073476490900863, 'n_estimators': 90, 'max_depth': 9, 'min_child_weight': 5.293427984669027, 'subsample': 0.764766179147305, 'colsample_bytree': 0.7841254120877068, 'colsample_bylevel': 0.9062230065516972, 'gamma': 0.1450890528650894, 'reg_alpha': 0.2608380836951976, 'reg_lambda': 0.41202507648224024}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:47:05] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:47:06] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:47:06,281]\u001b[0m Trial 504 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.14394507471831403, 'n_estimators': 91, 'max_depth': 9, 'min_child_weight': 5.209851945273362, 'subsample': 0.7581328786178125, 'colsample_bytree': 0.782849759442259, 'colsample_bylevel': 0.9048898848507992, 'gamma': 0.14692707840963262, 'reg_alpha': 0.2643332003706654, 'reg_lambda': 0.44552146033717294}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:47:06,441]\u001b[0m Trial 505 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.10308460172470707, 'n_estimators': 80, 'max_depth': 10, 'min_child_weight': 6.636494097666739, 'subsample': 0.6080193357216309, 'colsample_bytree': 0.6210450310997252, 'colsample_bylevel': 0.9493600413350597, 'gamma': 0.18921194698722224, 'reg_alpha': 0.2345348081320972, 'reg_lambda': 0.8086240496230535}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:47:06] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:47:06] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:47:06,602]\u001b[0m Trial 506 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.10817277641979177, 'n_estimators': 81, 'max_depth': 10, 'min_child_weight': 6.801108542146623, 'subsample': 0.5937162872411177, 'colsample_bytree': 0.7114405951997502, 'colsample_bylevel': 0.9610903006289191, 'gamma': 0.2015417020606366, 'reg_alpha': 0.26656628586077247, 'reg_lambda': 0.8232426175015556}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:47:06,760]\u001b[0m Trial 507 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.026717161979199634, 'n_estimators': 81, 'max_depth': 10, 'min_child_weight': 6.684998846424085, 'subsample': 0.592975524372434, 'colsample_bytree': 0.6378456552392895, 'colsample_bylevel': 0.9332750060957864, 'gamma': 0.2236122023801455, 'reg_alpha': 0.20038649264049185, 'reg_lambda': 0.8495606783564693}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:47:06] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:47:06] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:47:06,919]\u001b[0m Trial 508 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.10367749323823373, 'n_estimators': 80, 'max_depth': 10, 'min_child_weight': 6.853491073832188, 'subsample': 0.6061307494462267, 'colsample_bytree': 0.611247160430277, 'colsample_bylevel': 0.9567685001051391, 'gamma': 0.21249872420185234, 'reg_alpha': 0.2103818377317557, 'reg_lambda': 0.530763329956748}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:47:07,077]\u001b[0m Trial 509 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.10627370589041873, 'n_estimators': 80, 'max_depth': 10, 'min_child_weight': 6.822938902897643, 'subsample': 0.6123274827669496, 'colsample_bytree': 0.7143715076784749, 'colsample_bylevel': 0.5106321824061135, 'gamma': 0.06020645607739732, 'reg_alpha': 0.23070533724389855, 'reg_lambda': 0.8039931386075653}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:47:06] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:47:07] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:47:07,238]\u001b[0m Trial 510 finished with value: 0.3333333333333333 and parameters: {'booster': 'gblinear', 'learning_rate': 0.021844038356166367, 'n_estimators': 80, 'max_depth': 3, 'min_child_weight': 5.54845300794919, 'subsample': 0.5894099193825386, 'colsample_bytree': 0.6467973664961862, 'colsample_bylevel': 0.5175042537836058, 'gamma': 0.04876964600790256, 'reg_alpha': 0.31007138413127777, 'reg_lambda': 0.8743778377014659}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:47:07,419]\u001b[0m Trial 511 finished with value: 0.8285714285714286 and parameters: {'booster': 'gblinear', 'learning_rate': 0.07426809311203929, 'n_estimators': 96, 'max_depth': 6, 'min_child_weight': 1.6380187907522934, 'subsample': 0.7449538099633788, 'colsample_bytree': 0.6395576656269617, 'colsample_bylevel': 0.5487935720171333, 'gamma': 0.16507628449612588, 'reg_alpha': 0.16204992970343152, 'reg_lambda': 0.7421230924936993}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:47:07] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:47:07,615]\u001b[0m Trial 512 finished with value: 0.3333333333333333 and parameters: {'booster': 'dart', 'learning_rate': 0.15908085148395912, 'n_estimators': 88, 'max_depth': 6, 'min_child_weight': 8.849107163924423, 'subsample': 0.9866558288504577, 'colsample_bytree': 0.7710355003752162, 'colsample_bylevel': 0.618145025486684, 'gamma': 0.1602969642928563, 'reg_alpha': 0.3364862159556696, 'reg_lambda': 0.47553574498236123}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:47:07,783]\u001b[0m Trial 513 finished with value: 0.3333333333333333 and parameters: {'booster': 'gbtree', 'learning_rate': 0.08711277285149462, 'n_estimators': 86, 'max_depth': 4, 'min_child_weight': 3.754188014721219, 'subsample': 0.8568494861211129, 'colsample_bytree': 0.5992045261433265, 'colsample_bylevel': 0.999180548235356, 'gamma': 0.4976843481475248, 'reg_alpha': 0.36272569621359824, 'reg_lambda': 0.794988380485599}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:47:07,949]\u001b[0m Trial 514 finished with value: 0.3333333333333333 and parameters: {'booster': 'gblinear', 'learning_rate': 0.08573535665724531, 'n_estimators': 87, 'max_depth': 4, 'min_child_weight': 3.6874769865422716, 'subsample': 0.8651994528775524, 'colsample_bytree': 0.5713425869763941, 'colsample_bylevel': 0.9941782438566734, 'gamma': 0.4997282693268382, 'reg_alpha': 0.3984484651948855, 'reg_lambda': 0.7940146330371739}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:47:07] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:47:07] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:47:08,139]\u001b[0m Trial 515 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.07895746015597661, 'n_estimators': 100, 'max_depth': 8, 'min_child_weight': 3.9854826903470526, 'subsample': 0.8581018609796559, 'colsample_bytree': 0.5903484473007781, 'colsample_bylevel': 0.9726688946780228, 'gamma': 0.27363580858454745, 'reg_alpha': 0.33174383065151125, 'reg_lambda': 0.21055294049160653}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:47:08,300]\u001b[0m Trial 516 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.032931521101190034, 'n_estimators': 80, 'max_depth': 3, 'min_child_weight': 7.444882551217132, 'subsample': 0.5824799266690913, 'colsample_bytree': 0.7200602411636615, 'colsample_bylevel': 0.5027245987036447, 'gamma': 0.048358386069770096, 'reg_alpha': 0.24226236465314455, 'reg_lambda': 0.8907299257823469}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:47:08] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:47:08] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:47:08,459]\u001b[0m Trial 517 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.02927411226662896, 'n_estimators': 81, 'max_depth': 3, 'min_child_weight': 5.502109867923306, 'subsample': 0.6310612797838029, 'colsample_bytree': 0.7003389988833346, 'colsample_bylevel': 0.5012215940721042, 'gamma': 0.06293273814472972, 'reg_alpha': 0.2793439553006422, 'reg_lambda': 0.9069106227492401}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:47:08,619]\u001b[0m Trial 518 finished with value: 0.3333333333333333 and parameters: {'booster': 'gblinear', 'learning_rate': 0.12073131425803328, 'n_estimators': 81, 'max_depth': 3, 'min_child_weight': 7.419033204483874, 'subsample': 0.6025516031118835, 'colsample_bytree': 0.747491611456792, 'colsample_bylevel': 0.6919389029395333, 'gamma': 0.4482826391365572, 'reg_alpha': 0.6034088213440048, 'reg_lambda': 0.8961846790553734}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:47:08] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:47:08] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:47:08,787]\u001b[0m Trial 519 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.10076468496340568, 'n_estimators': 87, 'max_depth': 8, 'min_child_weight': 4.265056819673934, 'subsample': 0.7854763080843584, 'colsample_bytree': 0.5472313495533822, 'colsample_bylevel': 0.580906974021672, 'gamma': 0.3743687980578574, 'reg_alpha': 0.2743571689581769, 'reg_lambda': 0.6889466883450037}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:47:08,945]\u001b[0m Trial 520 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.11772124798469438, 'n_estimators': 80, 'max_depth': 10, 'min_child_weight': 8.12645086617919, 'subsample': 0.6212525044345413, 'colsample_bytree': 0.9804121354988734, 'colsample_bylevel': 0.7817825513757719, 'gamma': 0.3399409505140827, 'reg_alpha': 0.3203204902033884, 'reg_lambda': 0.8194959591525541}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:47:08] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:47:08] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:47:09,103]\u001b[0m Trial 521 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.12241740407570613, 'n_estimators': 80, 'max_depth': 10, 'min_child_weight': 5.665467875471172, 'subsample': 0.585447062348915, 'colsample_bytree': 0.7101037649912986, 'colsample_bylevel': 0.9235035195683041, 'gamma': 0.3354408984470005, 'reg_alpha': 0.2594420752809394, 'reg_lambda': 0.8298541162556146}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:47:09,261]\u001b[0m Trial 522 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.11538694738632042, 'n_estimators': 81, 'max_depth': 10, 'min_child_weight': 9.599045862430382, 'subsample': 0.6021846666720708, 'colsample_bytree': 0.8700281760619862, 'colsample_bylevel': 0.7914665137409331, 'gamma': 0.3261345141328491, 'reg_alpha': 0.31154158378436264, 'reg_lambda': 0.8460592070829928}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:47:09] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:47:09] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:47:09,443]\u001b[0m Trial 523 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.09665936228693763, 'n_estimators': 100, 'max_depth': 4, 'min_child_weight': 9.602651027174335, 'subsample': 0.8517329449772238, 'colsample_bytree': 0.5620346540644438, 'colsample_bylevel': 0.7597998373082714, 'gamma': 0.08428835977081395, 'reg_alpha': 0.24734363955513272, 'reg_lambda': 0.9783502133609663}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:47:09,626]\u001b[0m Trial 524 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.07030257871700586, 'n_estimators': 100, 'max_depth': 4, 'min_child_weight': 7.069159019449542, 'subsample': 0.8491273936984253, 'colsample_bytree': 0.557153431923067, 'colsample_bylevel': 0.7547221827154388, 'gamma': 0.0895782202575348, 'reg_alpha': 0.25241381523116857, 'reg_lambda': 0.9545337281309301}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:47:09] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:47:09] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:47:09,791]\u001b[0m Trial 525 finished with value: 0.8285714285714286 and parameters: {'booster': 'gblinear', 'learning_rate': 0.10301298611560088, 'n_estimators': 86, 'max_depth': 5, 'min_child_weight': 3.568528526003849, 'subsample': 0.9487034447396364, 'colsample_bytree': 0.5318282287052655, 'colsample_bylevel': 0.9869768387008094, 'gamma': 0.482625543320491, 'reg_alpha': 0.36147539976280374, 'reg_lambda': 0.5749082882936479}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:47:09,957]\u001b[0m Trial 526 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.08268926470425182, 'n_estimators': 87, 'max_depth': 8, 'min_child_weight': 3.5626587231408164, 'subsample': 0.9177572755001325, 'colsample_bytree': 0.7391956987304662, 'colsample_bylevel': 0.986752781152047, 'gamma': 0.28705841302912843, 'reg_alpha': 0.31279331740423194, 'reg_lambda': 0.36850788785441}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:47:09] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:47:10] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:47:10,114]\u001b[0m Trial 527 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.13662233072903512, 'n_estimators': 80, 'max_depth': 10, 'min_child_weight': 2.458184670609975, 'subsample': 0.9705737747492023, 'colsample_bytree': 0.6589778309811446, 'colsample_bylevel': 0.7383086730241313, 'gamma': 0.35648509445652, 'reg_alpha': 0.28446465885775324, 'reg_lambda': 0.7799118831249569}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:47:10,272]\u001b[0m Trial 528 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.13522715889445586, 'n_estimators': 81, 'max_depth': 10, 'min_child_weight': 4.877153273675445, 'subsample': 0.9600359374752058, 'colsample_bytree': 0.903562244703284, 'colsample_bylevel': 0.7311008967692162, 'gamma': 0.06800162361396814, 'reg_alpha': 0.2770851545063421, 'reg_lambda': 0.7737184434173923}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:47:10] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:47:10] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:47:10,448]\u001b[0m Trial 529 finished with value: 0.8285714285714286 and parameters: {'booster': 'gblinear', 'learning_rate': 0.12508548093880723, 'n_estimators': 95, 'max_depth': 6, 'min_child_weight': 8.321233621171254, 'subsample': 0.6906008469980809, 'colsample_bytree': 0.8070003567667235, 'colsample_bylevel': 0.6386721548294091, 'gamma': 0.44290587504784057, 'reg_alpha': 0.18923967952043813, 'reg_lambda': 0.27102405627912146}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:47:10,625]\u001b[0m Trial 530 finished with value: 0.8285714285714286 and parameters: {'booster': 'gblinear', 'learning_rate': 0.12829237507944827, 'n_estimators': 95, 'max_depth': 6, 'min_child_weight': 7.721485618796098, 'subsample': 0.650588621066859, 'colsample_bytree': 0.9566648206896142, 'colsample_bylevel': 0.644950211556337, 'gamma': 0.4629889795307024, 'reg_alpha': 0.20923995296841003, 'reg_lambda': 0.2921035731305709}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:47:10] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:47:10] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:47:10,801]\u001b[0m Trial 531 finished with value: 0.3333333333333333 and parameters: {'booster': 'gblinear', 'learning_rate': 0.12420157954248391, 'n_estimators': 94, 'max_depth': 6, 'min_child_weight': 7.910053316763528, 'subsample': 0.7293041857214212, 'colsample_bytree': 0.7967382734481286, 'colsample_bylevel': 0.694779073513124, 'gamma': 0.12482498429981685, 'reg_alpha': 0.46814165024656096, 'reg_lambda': 0.36744987978966753}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:47:10,966]\u001b[0m Trial 532 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.1118095674360701, 'n_estimators': 86, 'max_depth': 8, 'min_child_weight': 6.299056707237391, 'subsample': 0.9376822172344124, 'colsample_bytree': 0.7301688108111336, 'colsample_bylevel': 0.9755691602591058, 'gamma': 0.26772459341156357, 'reg_alpha': 0.3595157629090198, 'reg_lambda': 0.38393017984241323}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:47:10] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:47:11] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:47:11,131]\u001b[0m Trial 533 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.1120135153701278, 'n_estimators': 86, 'max_depth': 8, 'min_child_weight': 6.274554815187284, 'subsample': 0.6435173247492076, 'colsample_bytree': 0.7273798374089708, 'colsample_bylevel': 0.976634716829992, 'gamma': 0.28322888814278707, 'reg_alpha': 0.3505718636084947, 'reg_lambda': 0.3726963346961554}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:47:11,321]\u001b[0m Trial 534 finished with value: 0.3333333333333333 and parameters: {'booster': 'dart', 'learning_rate': 0.1191037410336891, 'n_estimators': 86, 'max_depth': 8, 'min_child_weight': 6.3553087087698525, 'subsample': 0.6386693129437911, 'colsample_bytree': 0.5752845707796362, 'colsample_bylevel': 0.9666772590078944, 'gamma': 0.27921597063772524, 'reg_alpha': 0.3344961761072137, 'reg_lambda': 0.596514145851195}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:47:11,503]\u001b[0m Trial 535 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.09134235733586672, 'n_estimators': 99, 'max_depth': 8, 'min_child_weight': 3.9105807790137397, 'subsample': 0.5055851748812628, 'colsample_bytree': 0.565535708549518, 'colsample_bylevel': 0.7119367222524653, 'gamma': 0.26656207172738267, 'reg_alpha': 0.3223628758795832, 'reg_lambda': 0.5921412891389578}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:47:11] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:47:11,688]\u001b[0m Trial 536 finished with value: 0.3333333333333333 and parameters: {'booster': 'gbtree', 'learning_rate': 0.13508181105539085, 'n_estimators': 96, 'max_depth': 6, 'min_child_weight': 5.249728741790618, 'subsample': 0.9074111490690411, 'colsample_bytree': 0.9162505156483564, 'colsample_bylevel': 0.8033338416325229, 'gamma': 0.42326695885329724, 'reg_alpha': 0.31364234277593084, 'reg_lambda': 0.3047793452430833}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:47:11,869]\u001b[0m Trial 537 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.09559563763530925, 'n_estimators': 99, 'max_depth': 5, 'min_child_weight': 6.785120033825969, 'subsample': 0.9776192350710856, 'colsample_bytree': 0.5524129761727137, 'colsample_bylevel': 0.6608915707166038, 'gamma': 0.024790893649794227, 'reg_alpha': 0.3386505712685012, 'reg_lambda': 0.757713680731729}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:47:11] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:47:11] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:47:12,027]\u001b[0m Trial 538 finished with value: 0.6249999999999999 and parameters: {'booster': 'gblinear', 'learning_rate': 0.14010449752801313, 'n_estimators': 82, 'max_depth': 10, 'min_child_weight': 5.06622909251984, 'subsample': 0.9885827929641655, 'colsample_bytree': 0.8804930834395528, 'colsample_bylevel': 0.9445677046096651, 'gamma': 0.3555082385567136, 'reg_alpha': 0.09571381480682153, 'reg_lambda': 0.7755816350874246}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:47:12,212]\u001b[0m Trial 539 finished with value: 0.3333333333333333 and parameters: {'booster': 'gblinear', 'learning_rate': 0.07602702424010105, 'n_estimators': 100, 'max_depth': 4, 'min_child_weight': 4.522130545690784, 'subsample': 0.8153779414369584, 'colsample_bytree': 0.5929458317450031, 'colsample_bylevel': 0.6670837846811343, 'gamma': 0.4900924601090022, 'reg_alpha': 0.39847191759114037, 'reg_lambda': 0.34565043001348567}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:47:12] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:47:12] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:47:12,390]\u001b[0m Trial 540 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.11410876687766276, 'n_estimators': 97, 'max_depth': 5, 'min_child_weight': 3.841718243145293, 'subsample': 0.5096756908949339, 'colsample_bytree': 0.7466008606304864, 'colsample_bylevel': 0.7007027318466027, 'gamma': 0.47881651630968775, 'reg_alpha': 0.19390071413694834, 'reg_lambda': 0.7319549577449554}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:47:12,549]\u001b[0m Trial 541 finished with value: 0.6249999999999999 and parameters: {'booster': 'gblinear', 'learning_rate': 0.01744896152800324, 'n_estimators': 80, 'max_depth': 10, 'min_child_weight': 5.186499390314178, 'subsample': 0.9981154097324597, 'colsample_bytree': 0.9831062769832908, 'colsample_bylevel': 0.5258170072472611, 'gamma': 0.43293704589090887, 'reg_alpha': 0.28748185633626416, 'reg_lambda': 0.5052427939445007}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:47:12] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:47:12] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:47:12,732]\u001b[0m Trial 542 finished with value: 0.6249999999999999 and parameters: {'booster': 'gblinear', 'learning_rate': 0.07223900150550545, 'n_estimators': 100, 'max_depth': 4, 'min_child_weight': 4.4140300534079415, 'subsample': 0.8317915220600399, 'colsample_bytree': 0.7406711213625757, 'colsample_bylevel': 0.7037198050810796, 'gamma': 0.46987549693666475, 'reg_alpha': 0.3787793673474258, 'reg_lambda': 0.313442714109454}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:47:12,913]\u001b[0m Trial 543 finished with value: 0.3333333333333333 and parameters: {'booster': 'gblinear', 'learning_rate': 0.06447962596844989, 'n_estimators': 99, 'max_depth': 4, 'min_child_weight': 2.6610502728673375, 'subsample': 0.8016855700047347, 'colsample_bytree': 0.6070700718276293, 'colsample_bylevel': 0.5836807228278036, 'gamma': 0.46893301976396473, 'reg_alpha': 0.3742487025035391, 'reg_lambda': 0.5502681834313735}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:47:12] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:47:12] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:47:13,078]\u001b[0m Trial 544 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.11644162554680336, 'n_estimators': 85, 'max_depth': 8, 'min_child_weight': 6.1101861440629675, 'subsample': 0.5186060589307803, 'colsample_bytree': 0.954975496717784, 'colsample_bylevel': 0.5115564239028306, 'gamma': 0.024100110628966765, 'reg_alpha': 0.3038433505169012, 'reg_lambda': 0.7015218043156998}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:47:13,249]\u001b[0m Trial 545 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.15777568699008587, 'n_estimators': 91, 'max_depth': 10, 'min_child_weight': 5.379172915265126, 'subsample': 0.9999140817531701, 'colsample_bytree': 0.9743741704652394, 'colsample_bylevel': 0.9656359946954876, 'gamma': 0.4418902946318133, 'reg_alpha': 0.27071688987973386, 'reg_lambda': 0.5069829543050722}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:47:13] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:47:13] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:47:13,421]\u001b[0m Trial 546 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.1553578642053824, 'n_estimators': 91, 'max_depth': 10, 'min_child_weight': 4.753181000157886, 'subsample': 0.9736773446523463, 'colsample_bytree': 0.9925368182243964, 'colsample_bylevel': 0.5229358229090754, 'gamma': 0.3544105030568885, 'reg_alpha': 0.28412680046944355, 'reg_lambda': 0.5174346885413544}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:47:13,580]\u001b[0m Trial 547 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.14409474114783463, 'n_estimators': 80, 'max_depth': 10, 'min_child_weight': 5.142124630520656, 'subsample': 0.985516738527836, 'colsample_bytree': 0.9690927352692557, 'colsample_bylevel': 0.9661635834423092, 'gamma': 0.31690168356951864, 'reg_alpha': 0.23644328112869623, 'reg_lambda': 0.8201618698814013}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:47:13] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:47:13] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:47:13,750]\u001b[0m Trial 548 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.14719162418941234, 'n_estimators': 90, 'max_depth': 10, 'min_child_weight': 5.343820776551122, 'subsample': 0.6623333116589272, 'colsample_bytree': 0.9973061665249767, 'colsample_bylevel': 0.9502953844778596, 'gamma': 0.3309307831069726, 'reg_alpha': 0.2253646223469431, 'reg_lambda': 0.8209013503150642}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:47:13,931]\u001b[0m Trial 549 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.13391954317751933, 'n_estimators': 89, 'max_depth': 10, 'min_child_weight': 8.422316583940466, 'subsample': 0.6716956618407386, 'colsample_bytree': 0.9879358973470767, 'colsample_bylevel': 0.9561859448364054, 'gamma': 0.41982243461796404, 'reg_alpha': 0.23723565025317347, 'reg_lambda': 0.9484434288687187}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:47:13] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:47:13] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:47:14,094]\u001b[0m Trial 550 finished with value: 0.3333333333333333 and parameters: {'booster': 'gblinear', 'learning_rate': 0.15362519405003533, 'n_estimators': 80, 'max_depth': 10, 'min_child_weight': 4.722399894770484, 'subsample': 0.6765740830348176, 'colsample_bytree': 0.6495892460693843, 'colsample_bylevel': 0.940650808976818, 'gamma': 0.34794968680618715, 'reg_alpha': 0.01308949683969969, 'reg_lambda': 0.9223536866270221}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:47:14,262]\u001b[0m Trial 551 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.03748552054669341, 'n_estimators': 85, 'max_depth': 3, 'min_child_weight': 6.56901696228918, 'subsample': 0.9452654343459722, 'colsample_bytree': 0.838346071181103, 'colsample_bylevel': 0.7612678135149278, 'gamma': 0.40597116507671044, 'reg_alpha': 0.2991000259755996, 'reg_lambda': 0.3988564699461016}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:47:14] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:47:14] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:47:14,438]\u001b[0m Trial 552 finished with value: 0.6249999999999999 and parameters: {'booster': 'gblinear', 'learning_rate': 0.14274054886636053, 'n_estimators': 89, 'max_depth': 10, 'min_child_weight': 5.112409067920899, 'subsample': 0.6304194274014806, 'colsample_bytree': 0.8613479407713751, 'colsample_bylevel': 0.9629810961864017, 'gamma': 0.23104330333495993, 'reg_alpha': 0.07533145429767552, 'reg_lambda': 0.9530619749625547}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:47:14,612]\u001b[0m Trial 553 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.11055954423974747, 'n_estimators': 88, 'max_depth': 8, 'min_child_weight': 4.160323691291387, 'subsample': 0.9516414613955055, 'colsample_bytree': 0.8439578668149443, 'colsample_bylevel': 0.7515505969586046, 'gamma': 0.3850166632749007, 'reg_alpha': 0.2678567590597076, 'reg_lambda': 0.6065539974314854}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:47:14] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:47:14] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:47:14,784]\u001b[0m Trial 554 finished with value: 0.3333333333333333 and parameters: {'booster': 'gblinear', 'learning_rate': 0.12724062190193364, 'n_estimators': 87, 'max_depth': 10, 'min_child_weight': 8.66058014092023, 'subsample': 0.9789310527645604, 'colsample_bytree': 0.9810430989388095, 'colsample_bylevel': 0.5388941178341411, 'gamma': 0.38730686918808843, 'reg_alpha': 0.04904098094111971, 'reg_lambda': 0.24347241985254242}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:47:14,965]\u001b[0m Trial 555 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.11244699517904454, 'n_estimators': 90, 'max_depth': 3, 'min_child_weight': 6.071233343312945, 'subsample': 0.9530218258161967, 'colsample_bytree': 0.847580091205307, 'colsample_bylevel': 0.772693330619108, 'gamma': 0.41357371749000443, 'reg_alpha': 0.30719366513870666, 'reg_lambda': 0.6422584805903351}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:47:14] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:47:15] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:47:15,139]\u001b[0m Trial 556 finished with value: 0.3333333333333333 and parameters: {'booster': 'gblinear', 'learning_rate': 0.1303432307779595, 'n_estimators': 89, 'max_depth': 10, 'min_child_weight': 8.546350370757734, 'subsample': 0.9885446132638732, 'colsample_bytree': 0.971732764075185, 'colsample_bylevel': 0.5382207961741501, 'gamma': 0.24593031991942044, 'reg_alpha': 0.022088887785598144, 'reg_lambda': 0.26117497676635426}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:47:15,338]\u001b[0m Trial 557 finished with value: 0.3333333333333333 and parameters: {'booster': 'dart', 'learning_rate': 0.02320343649859398, 'n_estimators': 90, 'max_depth': 10, 'min_child_weight': 3.352806639388266, 'subsample': 0.9510800863560778, 'colsample_bytree': 0.8540961424627852, 'colsample_bylevel': 0.777635098872902, 'gamma': 0.30313930708702447, 'reg_alpha': 0.30989781578116865, 'reg_lambda': 0.8721106591043838}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:47:15,509]\u001b[0m Trial 558 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.10991693761492154, 'n_estimators': 89, 'max_depth': 10, 'min_child_weight': 2.9846067391663365, 'subsample': 0.6171010984708769, 'colsample_bytree': 0.8848920442645893, 'colsample_bylevel': 0.9187353439957164, 'gamma': 0.03470782170870121, 'reg_alpha': 0.2961236958913976, 'reg_lambda': 0.8743054427465046}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:47:15] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:47:15] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:47:15,675]\u001b[0m Trial 559 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.13157743123047314, 'n_estimators': 85, 'max_depth': 10, 'min_child_weight': 8.00560110063066, 'subsample': 0.9776503539256862, 'colsample_bytree': 0.8918713265925567, 'colsample_bylevel': 0.5334447162925399, 'gamma': 0.40536493413990243, 'reg_alpha': 0.2608641944872934, 'reg_lambda': 0.6287037706509008}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:47:15,843]\u001b[0m Trial 560 finished with value: 0.3333333333333333 and parameters: {'booster': 'gbtree', 'learning_rate': 0.1389430393994319, 'n_estimators': 85, 'max_depth': 10, 'min_child_weight': 8.707459651696519, 'subsample': 0.638432481699792, 'colsample_bytree': 0.8783195806675257, 'colsample_bylevel': 0.9704758914951284, 'gamma': 0.23289492347285884, 'reg_alpha': 0.2605871256302971, 'reg_lambda': 0.9637305557492087}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:47:16,017]\u001b[0m Trial 561 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.03244732221009625, 'n_estimators': 91, 'max_depth': 9, 'min_child_weight': 6.476007408530556, 'subsample': 0.7557454652554865, 'colsample_bytree': 0.8943175938780248, 'colsample_bylevel': 0.9096758585279033, 'gamma': 0.03278824070930053, 'reg_alpha': 0.2927106535796173, 'reg_lambda': 0.6365303839646073}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:47:15] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:47:16] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:47:16,185]\u001b[0m Trial 562 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.13259046124750043, 'n_estimators': 85, 'max_depth': 7, 'min_child_weight': 8.869880913892889, 'subsample': 0.9410790521124007, 'colsample_bytree': 0.8690758686497354, 'colsample_bylevel': 0.8765909525880685, 'gamma': 0.4229562031164765, 'reg_alpha': 0.2501545433225725, 'reg_lambda': 0.9373388888209438}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:47:16,358]\u001b[0m Trial 563 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.13134378043375144, 'n_estimators': 92, 'max_depth': 3, 'min_child_weight': 8.111785300738253, 'subsample': 0.9657481606578747, 'colsample_bytree': 0.8638791299900668, 'colsample_bylevel': 0.811395197121813, 'gamma': 0.4276883305573557, 'reg_alpha': 0.24390496351844895, 'reg_lambda': 0.9681329772996897}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:47:16] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:47:16] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:47:16,517]\u001b[0m Trial 564 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.1387406322837147, 'n_estimators': 80, 'max_depth': 3, 'min_child_weight': 8.256644613403566, 'subsample': 0.9311673825985859, 'colsample_bytree': 0.8908314599910296, 'colsample_bylevel': 0.8742900059906773, 'gamma': 0.3982103539754157, 'reg_alpha': 0.25495638438858065, 'reg_lambda': 0.9436336323799526}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:47:16,679]\u001b[0m Trial 565 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.028164450382523763, 'n_estimators': 82, 'max_depth': 9, 'min_child_weight': 6.571776348796273, 'subsample': 0.5995460510930284, 'colsample_bytree': 0.7707040450995675, 'colsample_bylevel': 0.9035814921979862, 'gamma': 0.17824499246912937, 'reg_alpha': 0.22358712029632, 'reg_lambda': 0.8541083970393107}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:47:16] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:47:16] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:47:16,840]\u001b[0m Trial 566 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.10398869418431932, 'n_estimators': 82, 'max_depth': 9, 'min_child_weight': 6.550624525448368, 'subsample': 0.6028617028393963, 'colsample_bytree': 0.7658339719145284, 'colsample_bylevel': 0.9027912020829948, 'gamma': 0.18579290450385139, 'reg_alpha': 0.21186214498519318, 'reg_lambda': 0.8499371732038042}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:47:17,008]\u001b[0m Trial 567 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.09984110604658558, 'n_estimators': 86, 'max_depth': 9, 'min_child_weight': 5.94251926081507, 'subsample': 0.8997568631218701, 'colsample_bytree': 0.7392160763508017, 'colsample_bylevel': 0.9900802760845228, 'gamma': 0.2543490105801522, 'reg_alpha': 0.3417175062404303, 'reg_lambda': 0.726963254916037}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:47:16] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:47:17] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:47:17,177]\u001b[0m Trial 568 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.09762851709362634, 'n_estimators': 86, 'max_depth': 9, 'min_child_weight': 5.874702376930904, 'subsample': 0.9190387569881918, 'colsample_bytree': 0.7406072127615588, 'colsample_bylevel': 0.9894735276141445, 'gamma': 0.25803506988588787, 'reg_alpha': 0.33543717992787964, 'reg_lambda': 0.7060348736141852}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:47:17,362]\u001b[0m Trial 569 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.07016104027582268, 'n_estimators': 99, 'max_depth': 4, 'min_child_weight': 7.249819185371513, 'subsample': 0.8338253477986685, 'colsample_bytree': 0.5851829430333898, 'colsample_bylevel': 0.8293323521030966, 'gamma': 0.10612482008106955, 'reg_alpha': 0.2751269825450045, 'reg_lambda': 0.9148931880264003}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:47:17] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:47:17] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:47:17,546]\u001b[0m Trial 570 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.06868403511748128, 'n_estimators': 99, 'max_depth': 4, 'min_child_weight': 7.387537798850767, 'subsample': 0.8350871491721982, 'colsample_bytree': 0.8141576629666648, 'colsample_bylevel': 0.654435155106598, 'gamma': 0.10560158151642972, 'reg_alpha': 0.2461635839410964, 'reg_lambda': 0.9178239747255195}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:47:17,710]\u001b[0m Trial 571 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.02943708956172475, 'n_estimators': 83, 'max_depth': 10, 'min_child_weight': 6.797070573513301, 'subsample': 0.5900606889567469, 'colsample_bytree': 0.6115515667790575, 'colsample_bylevel': 0.9399850753272275, 'gamma': 0.2063479360377135, 'reg_alpha': 0.17527682510165127, 'reg_lambda': 0.8917116819767303}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:47:17] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:47:17] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:47:17,874]\u001b[0m Trial 572 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.02443449918916595, 'n_estimators': 84, 'max_depth': 10, 'min_child_weight': 6.708249681528869, 'subsample': 0.6120019702747903, 'colsample_bytree': 0.6342287145566643, 'colsample_bylevel': 0.9253045941596211, 'gamma': 0.04956530928575026, 'reg_alpha': 0.16792511272902713, 'reg_lambda': 0.8961559113404186}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:47:18,037]\u001b[0m Trial 573 finished with value: 0.8285714285714286 and parameters: {'booster': 'gblinear', 'learning_rate': 0.09008377503610313, 'n_estimators': 83, 'max_depth': 5, 'min_child_weight': 6.350543381529318, 'subsample': 0.5289923344509486, 'colsample_bytree': 0.575219494682971, 'colsample_bylevel': 0.9814250306223903, 'gamma': 0.012093877537964516, 'reg_alpha': 0.15003517098783592, 'reg_lambda': 0.5759117453498026}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:47:17] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:47:18] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:47:18,216]\u001b[0m Trial 574 finished with value: 0.8285714285714286 and parameters: {'booster': 'gblinear', 'learning_rate': 0.08813898335238006, 'n_estimators': 87, 'max_depth': 5, 'min_child_weight': 6.292483090095865, 'subsample': 0.6593833775297782, 'colsample_bytree': 0.5617436110259793, 'colsample_bylevel': 0.9797398926654866, 'gamma': 0.2900640305818242, 'reg_alpha': 0.17720695644159257, 'reg_lambda': 0.5570578559319226}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:47:18,404]\u001b[0m Trial 575 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.19469329948680295, 'n_estimators': 100, 'max_depth': 4, 'min_child_weight': 7.052419838419253, 'subsample': 0.8706289080544309, 'colsample_bytree': 0.8101467757916329, 'colsample_bylevel': 0.6468017587050211, 'gamma': 0.08873055586482836, 'reg_alpha': 0.24160593752872653, 'reg_lambda': 0.9978973620577595}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:47:18] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:47:18] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:47:18,590]\u001b[0m Trial 576 finished with value: 0.3333333333333333 and parameters: {'booster': 'gblinear', 'learning_rate': 0.14483404035990735, 'n_estimators': 98, 'max_depth': 7, 'min_child_weight': 1.177371129942955, 'subsample': 0.9015054502104696, 'colsample_bytree': 0.9923317369436792, 'colsample_bylevel': 0.5404442968271381, 'gamma': 0.12830758029346764, 'reg_alpha': 0.0008858015469772329, 'reg_lambda': 0.2468234476671248}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:47:18,755]\u001b[0m Trial 577 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.03397496749258729, 'n_estimators': 84, 'max_depth': 3, 'min_child_weight': 5.788299270741355, 'subsample': 0.6259661837103094, 'colsample_bytree': 0.6163276152495094, 'colsample_bylevel': 0.504944957486328, 'gamma': 0.04267689101182422, 'reg_alpha': 0.19955707196398573, 'reg_lambda': 0.8841594095123064}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:47:18] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:47:18] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:47:18,921]\u001b[0m Trial 578 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.11634803034906718, 'n_estimators': 84, 'max_depth': 6, 'min_child_weight': 6.024447481840376, 'subsample': 0.5068494959732186, 'colsample_bytree': 0.5558369945041317, 'colsample_bylevel': 0.5670385036857695, 'gamma': 0.015614674508623921, 'reg_alpha': 0.3024428201530429, 'reg_lambda': 0.6601365907505388}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:47:19,105]\u001b[0m Trial 579 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.17989368212481252, 'n_estimators': 99, 'max_depth': 4, 'min_child_weight': 1.762361644775849, 'subsample': 0.845878990628771, 'colsample_bytree': 0.5183730752777438, 'colsample_bylevel': 0.6105731228409388, 'gamma': 0.08098911409247236, 'reg_alpha': 0.2722830934572845, 'reg_lambda': 0.9968470171317908}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:47:18] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:47:19,323]\u001b[0m Trial 580 finished with value: 0.8285714285714286 and parameters: {'booster': 'dart', 'learning_rate': 0.17411532416911074, 'n_estimators': 99, 'max_depth': 4, 'min_child_weight': 1.8006354918179353, 'subsample': 0.8450557664973047, 'colsample_bytree': 0.7922918041416921, 'colsample_bylevel': 0.6749436476361599, 'gamma': 0.08284363143947329, 'reg_alpha': 0.2667159883077316, 'reg_lambda': 0.05839283613367145}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:47:19,513]\u001b[0m Trial 581 finished with value: 0.3333333333333333 and parameters: {'booster': 'gbtree', 'learning_rate': 0.16436471109536846, 'n_estimators': 99, 'max_depth': 4, 'min_child_weight': 2.259647660780218, 'subsample': 0.8189554493962005, 'colsample_bytree': 0.7856949114891185, 'colsample_bylevel': 0.6835907551655814, 'gamma': 0.17757191136686065, 'reg_alpha': 0.28165416293641027, 'reg_lambda': 0.9785767752409283}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:47:19,675]\u001b[0m Trial 582 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.12199275339368296, 'n_estimators': 81, 'max_depth': 10, 'min_child_weight': 5.623349581232479, 'subsample': 0.5817540532064752, 'colsample_bytree': 0.7042650758876879, 'colsample_bylevel': 0.5002636625440953, 'gamma': 0.35172435758759857, 'reg_alpha': 0.28322192311562305, 'reg_lambda': 0.8355638317776944}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:47:19] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:47:19] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:47:19,858]\u001b[0m Trial 583 finished with value: 0.8285714285714286 and parameters: {'booster': 'gblinear', 'learning_rate': 0.14713765311080787, 'n_estimators': 98, 'max_depth': 7, 'min_child_weight': 1.0298298801041492, 'subsample': 0.6989164221605573, 'colsample_bytree': 0.9942088976208411, 'colsample_bylevel': 0.9179766067832583, 'gamma': 0.13923376439540794, 'reg_alpha': 0.22755345504143734, 'reg_lambda': 0.22399071582984997}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:47:20,043]\u001b[0m Trial 584 finished with value: 0.3333333333333333 and parameters: {'booster': 'gblinear', 'learning_rate': 0.17064324000264705, 'n_estimators': 100, 'max_depth': 4, 'min_child_weight': 1.9066740849924841, 'subsample': 0.8263643688229015, 'colsample_bytree': 0.6001371084893379, 'colsample_bylevel': 0.693131687798323, 'gamma': 0.17320406961240753, 'reg_alpha': 0.3551218782152119, 'reg_lambda': 0.9999037892323362}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:47:19] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:47:20] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:47:20,206]\u001b[0m Trial 585 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.11765702236120788, 'n_estimators': 81, 'max_depth': 10, 'min_child_weight': 9.60200409928464, 'subsample': 0.5878341161978503, 'colsample_bytree': 0.9797358773683257, 'colsample_bylevel': 0.7817538749515751, 'gamma': 0.32443132266066615, 'reg_alpha': 0.23808228473804224, 'reg_lambda': 0.8325367762107934}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:47:20,388]\u001b[0m Trial 586 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.11876384271698102, 'n_estimators': 97, 'max_depth': 8, 'min_child_weight': 6.187172371972756, 'subsample': 0.536503773844788, 'colsample_bytree': 0.826944023669567, 'colsample_bylevel': 0.5083483755233582, 'gamma': 0.3106428842436634, 'reg_alpha': 0.31758868973758425, 'reg_lambda': 0.6708473796613184}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:47:20] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:47:20] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:47:20,575]\u001b[0m Trial 587 finished with value: 0.3333333333333333 and parameters: {'booster': 'gblinear', 'learning_rate': 0.1772961741897928, 'n_estimators': 100, 'max_depth': 4, 'min_child_weight': 8.899088279755311, 'subsample': 0.8155344018635429, 'colsample_bytree': 0.5992725020621664, 'colsample_bylevel': 0.6883255349020666, 'gamma': 0.49536656874417806, 'reg_alpha': 0.4107408725632173, 'reg_lambda': 0.33902539836698}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:47:20,738]\u001b[0m Trial 588 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.10867172071817131, 'n_estimators': 80, 'max_depth': 10, 'min_child_weight': 9.789114273753624, 'subsample': 0.6210615713601495, 'colsample_bytree': 0.71665984081899, 'colsample_bylevel': 0.7647775787281594, 'gamma': 0.3675813742684849, 'reg_alpha': 0.21280398979330023, 'reg_lambda': 0.7712270153507675}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:47:20] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:47:20] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:47:20,922]\u001b[0m Trial 589 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.1502095039924646, 'n_estimators': 97, 'max_depth': 6, 'min_child_weight': 1.322912291591445, 'subsample': 0.6970769341178734, 'colsample_bytree': 0.9950045442695417, 'colsample_bylevel': 0.923703868755973, 'gamma': 0.15201341997862608, 'reg_alpha': 0.22748030071531755, 'reg_lambda': 0.46119021053814674}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:47:21,106]\u001b[0m Trial 590 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.15045506554275642, 'n_estimators': 97, 'max_depth': 6, 'min_child_weight': 1.2193540703592822, 'subsample': 0.698005855423294, 'colsample_bytree': 0.9998838014250193, 'colsample_bylevel': 0.9294403858113897, 'gamma': 0.12836986415899648, 'reg_alpha': 0.22287037649533728, 'reg_lambda': 0.45597773561285004}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:47:20] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:47:21] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:47:21,270]\u001b[0m Trial 591 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.15605195089190005, 'n_estimators': 81, 'max_depth': 10, 'min_child_weight': 5.619516328696728, 'subsample': 0.5975480258059359, 'colsample_bytree': 0.6882235246135943, 'colsample_bylevel': 0.7314337230328434, 'gamma': 0.06443166052998227, 'reg_alpha': 0.2631484403496442, 'reg_lambda': 0.7779551873985295}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:47:21,434]\u001b[0m Trial 592 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.16297650378515635, 'n_estimators': 81, 'max_depth': 10, 'min_child_weight': 5.55659034174498, 'subsample': 0.9605453206253093, 'colsample_bytree': 0.7099077854289071, 'colsample_bylevel': 0.7283716633459322, 'gamma': 0.06356304795125593, 'reg_alpha': 0.28098975282013394, 'reg_lambda': 0.7742704787667373}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:47:21] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:47:21] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:47:21,609]\u001b[0m Trial 593 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.11568228130203936, 'n_estimators': 89, 'max_depth': 3, 'min_child_weight': 5.973917778571522, 'subsample': 0.6695699619715435, 'colsample_bytree': 0.5258609945461362, 'colsample_bylevel': 0.740005683233998, 'gamma': 0.38979811278971793, 'reg_alpha': 0.28888238420004314, 'reg_lambda': 0.8692819303754021}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:47:21,783]\u001b[0m Trial 594 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.11421948926618661, 'n_estimators': 90, 'max_depth': 3, 'min_child_weight': 6.035439255191304, 'subsample': 0.6726134094005911, 'colsample_bytree': 0.8356798132440206, 'colsample_bylevel': 0.7407609748214178, 'gamma': 0.39100170771219384, 'reg_alpha': 0.29782420575720675, 'reg_lambda': 0.869405269090939}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:47:21] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:47:21] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:47:21,945]\u001b[0m Trial 595 finished with value: 0.6249999999999999 and parameters: {'booster': 'gblinear', 'learning_rate': 0.11007814538427384, 'n_estimators': 81, 'max_depth': 8, 'min_child_weight': 5.696556706784844, 'subsample': 0.9662353916157447, 'colsample_bytree': 0.7259159021508169, 'colsample_bylevel': 0.729706611092125, 'gamma': 0.054817176761773403, 'reg_alpha': 0.35772281693259184, 'reg_lambda': 0.8491543566897896}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:47:22,131]\u001b[0m Trial 596 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.061276285844247, 'n_estimators': 100, 'max_depth': 3, 'min_child_weight': 7.267900419497311, 'subsample': 0.8400389844745612, 'colsample_bytree': 0.5014110554755189, 'colsample_bylevel': 0.5966431650099858, 'gamma': 0.16300235113940523, 'reg_alpha': 0.20581836564430284, 'reg_lambda': 0.7387792837829927}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:47:21] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:47:22] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:47:22,297]\u001b[0m Trial 597 finished with value: 0.8285714285714286 and parameters: {'booster': 'gblinear', 'learning_rate': 0.07435866720877253, 'n_estimators': 83, 'max_depth': 6, 'min_child_weight': 1.599739520226491, 'subsample': 0.7045504567507057, 'colsample_bytree': 0.9839198265899827, 'colsample_bylevel': 0.5530553603640994, 'gamma': 0.16971245721724104, 'reg_alpha': 0.19081072114808903, 'reg_lambda': 0.4223647871584257}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:47:22,483]\u001b[0m Trial 598 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.08378181631879462, 'n_estimators': 100, 'max_depth': 4, 'min_child_weight': 4.11569184816949, 'subsample': 0.862104854409923, 'colsample_bytree': 0.5726766332081303, 'colsample_bylevel': 0.9998497679232483, 'gamma': 0.09397499573667259, 'reg_alpha': 0.246098460181938, 'reg_lambda': 0.8022865946755098}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:47:22] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:47:22] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:47:22,647]\u001b[0m Trial 599 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.0745429805438559, 'n_estimators': 82, 'max_depth': 6, 'min_child_weight': 1.4865356910117105, 'subsample': 0.8904914774166417, 'colsample_bytree': 0.9812201757984648, 'colsample_bylevel': 0.5435987340630053, 'gamma': 0.1420722046511701, 'reg_alpha': 0.2173086087204292, 'reg_lambda': 0.44774271762028034}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:47:22,811]\u001b[0m Trial 600 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.06872508510673132, 'n_estimators': 83, 'max_depth': 6, 'min_child_weight': 1.3980919860048837, 'subsample': 0.7192655124508883, 'colsample_bytree': 0.9848241546556057, 'colsample_bylevel': 0.55235600690681, 'gamma': 0.16050472330039384, 'reg_alpha': 0.22079842996178678, 'reg_lambda': 0.4245134390917861}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:47:22] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:47:22,998]\u001b[0m Trial 601 finished with value: 0.8285714285714286 and parameters: {'booster': 'dart', 'learning_rate': 0.06722827781453199, 'n_estimators': 82, 'max_depth': 6, 'min_child_weight': 1.1589679832866264, 'subsample': 0.7083687336906415, 'colsample_bytree': 0.9879731792186566, 'colsample_bylevel': 0.8865953471058304, 'gamma': 0.15003004169448927, 'reg_alpha': 0.19220019913086933, 'reg_lambda': 0.7535148280189757}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:47:23,172]\u001b[0m Trial 602 finished with value: 0.3333333333333333 and parameters: {'booster': 'gbtree', 'learning_rate': 0.08035811473000833, 'n_estimators': 88, 'max_depth': 8, 'min_child_weight': 4.440660046620955, 'subsample': 0.8048053226219424, 'colsample_bytree': 0.5364745146199958, 'colsample_bylevel': 0.999888493932019, 'gamma': 0.3755085748153236, 'reg_alpha': 0.25026074242876384, 'reg_lambda': 0.546806425813677}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:47:23,348]\u001b[0m Trial 603 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.038600018834607855, 'n_estimators': 92, 'max_depth': 3, 'min_child_weight': 2.536303180989105, 'subsample': 0.9808671999585079, 'colsample_bytree': 0.6323834849020661, 'colsample_bylevel': 0.5228305373791616, 'gamma': 0.05689438991613994, 'reg_alpha': 0.2701441618433563, 'reg_lambda': 0.7482706608741081}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:47:23] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:47:23] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:47:23,527]\u001b[0m Trial 604 finished with value: 0.6249999999999999 and parameters: {'booster': 'gblinear', 'learning_rate': 0.041510865710311984, 'n_estimators': 93, 'max_depth': 3, 'min_child_weight': 2.937350171994111, 'subsample': 0.9699006730609215, 'colsample_bytree': 0.6458096294209397, 'colsample_bylevel': 0.5110927208782172, 'gamma': 0.0475044305429336, 'reg_alpha': 0.3256128676844249, 'reg_lambda': 0.7909095946702727}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:47:23,700]\u001b[0m Trial 605 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.10595516138489373, 'n_estimators': 90, 'max_depth': 10, 'min_child_weight': 6.7338983806498485, 'subsample': 0.6107671007879566, 'colsample_bytree': 0.6153097446436551, 'colsample_bylevel': 0.9339286547926967, 'gamma': 0.19526115036812997, 'reg_alpha': 0.2658580271239397, 'reg_lambda': 0.851241834660094}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:47:23] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:47:23] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:47:23,872]\u001b[0m Trial 606 finished with value: 0.8285714285714286 and parameters: {'booster': 'gblinear', 'learning_rate': 0.07275305984253028, 'n_estimators': 82, 'max_depth': 6, 'min_child_weight': 1.2914733808301004, 'subsample': 0.5425544303248442, 'colsample_bytree': 0.9752739052676553, 'colsample_bylevel': 0.8978231332300253, 'gamma': 0.47226801060920565, 'reg_alpha': 0.14199055215471373, 'reg_lambda': 0.7312106544644976}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:47:24,055]\u001b[0m Trial 607 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.10025157572005011, 'n_estimators': 88, 'max_depth': 8, 'min_child_weight': 4.6257958385524365, 'subsample': 0.8111734772911728, 'colsample_bytree': 0.5860242915603743, 'colsample_bylevel': 0.9907226265778114, 'gamma': 0.3783319912444842, 'reg_alpha': 0.20911426149804302, 'reg_lambda': 0.609556799447145}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:47:23] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:47:24] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:47:24,224]\u001b[0m Trial 608 finished with value: 0.8285714285714286 and parameters: {'booster': 'gblinear', 'learning_rate': 0.07533103000712828, 'n_estimators': 83, 'max_depth': 6, 'min_child_weight': 1.2966713727145625, 'subsample': 0.549404882529933, 'colsample_bytree': 0.9891639644930377, 'colsample_bylevel': 0.8872118985586541, 'gamma': 0.4805699853490052, 'reg_alpha': 0.168714749418254, 'reg_lambda': 0.22427502728575022}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:47:24,413]\u001b[0m Trial 609 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.07735816310931876, 'n_estimators': 100, 'max_depth': 4, 'min_child_weight': 4.232953174631965, 'subsample': 0.8576130411103325, 'colsample_bytree': 0.5887559326714724, 'colsample_bylevel': 0.7689902643049406, 'gamma': 0.45799795144688743, 'reg_alpha': 0.19539004507141294, 'reg_lambda': 0.8322626668875254}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:47:24] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:47:24] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:47:24,593]\u001b[0m Trial 610 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.1572267669151025, 'n_estimators': 91, 'max_depth': 10, 'min_child_weight': 5.372487605051941, 'subsample': 0.9843576136055502, 'colsample_bytree': 0.9592412774700582, 'colsample_bylevel': 0.9675414212024188, 'gamma': 0.3293333451859975, 'reg_alpha': 0.2892231243648641, 'reg_lambda': 0.5061012018758844}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:47:24,757]\u001b[0m Trial 611 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.06807306436646991, 'n_estimators': 82, 'max_depth': 6, 'min_child_weight': 3.1176660956001605, 'subsample': 0.555422754382698, 'colsample_bytree': 0.9935530229190603, 'colsample_bylevel': 0.8869002093816598, 'gamma': 0.22311022957567594, 'reg_alpha': 0.23233138269057219, 'reg_lambda': 0.7271494548708339}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:47:24] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:47:24] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:47:24,930]\u001b[0m Trial 612 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.09358706783916049, 'n_estimators': 83, 'max_depth': 6, 'min_child_weight': 1.0033095730413186, 'subsample': 0.5427530786924681, 'colsample_bytree': 0.9897571551938371, 'colsample_bylevel': 0.9092096353065247, 'gamma': 0.48106781489421374, 'reg_alpha': 0.23246425761696132, 'reg_lambda': 0.4069792135695499}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:47:25,103]\u001b[0m Trial 613 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.08841900606159693, 'n_estimators': 86, 'max_depth': 8, 'min_child_weight': 3.6562619951097743, 'subsample': 0.9253124205409727, 'colsample_bytree': 0.7439095425862619, 'colsample_bylevel': 0.9837535078042161, 'gamma': 0.45903216502939287, 'reg_alpha': 0.3317044338719161, 'reg_lambda': 0.3538557489181775}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:47:24] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:47:25] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:47:25,273]\u001b[0m Trial 614 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.08316408564486663, 'n_estimators': 86, 'max_depth': 8, 'min_child_weight': 3.5842793158662007, 'subsample': 0.927831471493308, 'colsample_bytree': 0.7305777092329997, 'colsample_bylevel': 0.9852070447503632, 'gamma': 0.2659351843390182, 'reg_alpha': 0.3394809827563377, 'reg_lambda': 0.35066892162580404}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:47:25,448]\u001b[0m Trial 615 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.14135868156895562, 'n_estimators': 90, 'max_depth': 10, 'min_child_weight': 3.2283318001154906, 'subsample': 0.9418022452292228, 'colsample_bytree': 0.9720972202088357, 'colsample_bylevel': 0.9517819045561189, 'gamma': 0.33164075537979365, 'reg_alpha': 0.30267406365318317, 'reg_lambda': 0.8127674932429761}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:47:25] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:47:25] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:47:25,624]\u001b[0m Trial 616 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.13869667348220885, 'n_estimators': 89, 'max_depth': 10, 'min_child_weight': 8.451781849725938, 'subsample': 0.9495616318487037, 'colsample_bytree': 0.9709887370632635, 'colsample_bylevel': 0.9573012079273001, 'gamma': 0.40465439558829186, 'reg_alpha': 0.2925720323266231, 'reg_lambda': 0.8020116705131525}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:47:25,786]\u001b[0m Trial 617 finished with value: 0.6249999999999999 and parameters: {'booster': 'gblinear', 'learning_rate': 0.03171652983791426, 'n_estimators': 80, 'max_depth': 3, 'min_child_weight': 6.9070974980210655, 'subsample': 0.6125776054635627, 'colsample_bytree': 0.9628813118084598, 'colsample_bylevel': 0.5000415333295221, 'gamma': 0.07235610541890312, 'reg_alpha': 0.3082467705642971, 'reg_lambda': 0.8174161976900448}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:47:25] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:47:25] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:47:25,949]\u001b[0m Trial 618 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.12098447571690077, 'n_estimators': 81, 'max_depth': 3, 'min_child_weight': 7.546017096794394, 'subsample': 0.6188932293444005, 'colsample_bytree': 0.72000223696598, 'colsample_bylevel': 0.5148946692474404, 'gamma': 0.05679628536761866, 'reg_alpha': 0.32770114212513446, 'reg_lambda': 0.9004546244090117}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:47:26,133]\u001b[0m Trial 619 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.028294278145130705, 'n_estimators': 97, 'max_depth': 6, 'min_child_weight': 1.1337256418567887, 'subsample': 0.5705066705561088, 'colsample_bytree': 0.9990827571792774, 'colsample_bylevel': 0.6289640037312715, 'gamma': 0.1190648629119639, 'reg_alpha': 0.24538181555102528, 'reg_lambda': 0.4387853567371689}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:47:25] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:47:26] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:47:26,318]\u001b[0m Trial 620 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.09501524740985647, 'n_estimators': 97, 'max_depth': 6, 'min_child_weight': 3.1169800014178675, 'subsample': 0.8843780397995636, 'colsample_bytree': 0.978057069350164, 'colsample_bylevel': 0.6136641386172252, 'gamma': 0.12224870608612243, 'reg_alpha': 0.2501256124319171, 'reg_lambda': 0.4235248866966723}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:47:26,502]\u001b[0m Trial 621 finished with value: 0.3333333333333333 and parameters: {'booster': 'gblinear', 'learning_rate': 0.05771830458631623, 'n_estimators': 97, 'max_depth': 9, 'min_child_weight': 1.0324714857106845, 'subsample': 0.5567035426886761, 'colsample_bytree': 0.9999456596842685, 'colsample_bylevel': 0.6295189860698612, 'gamma': 0.1571165440641993, 'reg_alpha': 0.486632756813686, 'reg_lambda': 0.46061117617757674}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:47:26] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:47:26] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:47:26,680]\u001b[0m Trial 622 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.10742285492259554, 'n_estimators': 91, 'max_depth': 10, 'min_child_weight': 4.7043211322787215, 'subsample': 0.9594988788924002, 'colsample_bytree': 0.8709944301173597, 'colsample_bylevel': 0.9358116471699168, 'gamma': 0.07474000729808741, 'reg_alpha': 0.2969590481735001, 'reg_lambda': 0.6312178574805066}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:47:26,862]\u001b[0m Trial 623 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.04171567904359546, 'n_estimators': 96, 'max_depth': 7, 'min_child_weight': 1.6047176669176313, 'subsample': 0.5394250687858498, 'colsample_bytree': 0.9669456116637556, 'colsample_bylevel': 0.6440816602047866, 'gamma': 0.11266881400420398, 'reg_alpha': 0.2707546660528589, 'reg_lambda': 0.4301218809670566}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:47:26] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:47:26] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:47:27,048]\u001b[0m Trial 624 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.03599803916325849, 'n_estimators': 97, 'max_depth': 7, 'min_child_weight': 2.052407147029281, 'subsample': 0.8808911122010348, 'colsample_bytree': 0.6801571156421549, 'colsample_bylevel': 0.6023661208096971, 'gamma': 0.1086416037862428, 'reg_alpha': 0.2582307510666702, 'reg_lambda': 0.47820849676049415}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:47:27,267]\u001b[0m Trial 625 finished with value: 0.3333333333333333 and parameters: {'booster': 'dart', 'learning_rate': 0.1478500877914045, 'n_estimators': 96, 'max_depth': 7, 'min_child_weight': 9.835912020913257, 'subsample': 0.5521778480489592, 'colsample_bytree': 0.965771762363654, 'colsample_bylevel': 0.6105090856878611, 'gamma': 0.12882348999600313, 'reg_alpha': 0.42785198488089815, 'reg_lambda': 0.4652176322683572}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:47:27,434]\u001b[0m Trial 626 finished with value: 0.3333333333333333 and parameters: {'booster': 'gbtree', 'learning_rate': 0.13555802037299347, 'n_estimators': 80, 'max_depth': 10, 'min_child_weight': 2.7534530199869667, 'subsample': 0.9768569684718004, 'colsample_bytree': 0.8951376471298995, 'colsample_bylevel': 0.9269021507443179, 'gamma': 0.034215368049284545, 'reg_alpha': 0.28402098537507353, 'reg_lambda': 0.6340195878160879}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:47:27,612]\u001b[0m Trial 627 finished with value: 0.3333333333333333 and parameters: {'booster': 'gblinear', 'learning_rate': 0.07737746700753588, 'n_estimators': 93, 'max_depth': 8, 'min_child_weight': 4.094264656608066, 'subsample': 0.9405065295579649, 'colsample_bytree': 0.7539527783137223, 'colsample_bylevel': 0.7103395425573593, 'gamma': 0.47027442172655526, 'reg_alpha': 0.3705038135046319, 'reg_lambda': 0.7159302820656878}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:47:27] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:47:27] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:47:27,793]\u001b[0m Trial 628 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.16504413856445468, 'n_estimators': 94, 'max_depth': 8, 'min_child_weight': 4.385790227888197, 'subsample': 0.8071003227053354, 'colsample_bytree': 0.757473071584444, 'colsample_bylevel': 0.7261881920220327, 'gamma': 0.49408887281830544, 'reg_alpha': 0.3536632537972951, 'reg_lambda': 0.3821891546853718}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:47:27,971]\u001b[0m Trial 629 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.08983566907760203, 'n_estimators': 94, 'max_depth': 8, 'min_child_weight': 4.37617830076591, 'subsample': 0.5216688245069026, 'colsample_bytree': 0.9362828450775379, 'colsample_bylevel': 0.5761310364797387, 'gamma': 0.020198902550397406, 'reg_alpha': 0.34301640706360986, 'reg_lambda': 0.3860299746890343}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:47:27] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:47:28] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:47:28,144]\u001b[0m Trial 630 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.08889333577292233, 'n_estimators': 87, 'max_depth': 8, 'min_child_weight': 4.059327788618425, 'subsample': 0.5179278630255304, 'colsample_bytree': 0.9434039289566718, 'colsample_bylevel': 0.5864301390362672, 'gamma': 0.020755167365106186, 'reg_alpha': 0.32306122621377936, 'reg_lambda': 0.6632373810256839}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:47:28,333]\u001b[0m Trial 631 finished with value: 0.3333333333333333 and parameters: {'booster': 'gblinear', 'learning_rate': 0.15251398511991238, 'n_estimators': 95, 'max_depth': 9, 'min_child_weight': 9.329636051819946, 'subsample': 0.7379114079972215, 'colsample_bytree': 0.9691286176239031, 'colsample_bylevel': 0.6109461552041429, 'gamma': 0.16061213458568047, 'reg_alpha': 0.6373337715785058, 'reg_lambda': 0.4821122910456805}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:47:28] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:47:28] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:47:28,498]\u001b[0m Trial 632 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.1386851182640687, 'n_estimators': 80, 'max_depth': 3, 'min_child_weight': 8.335113748360676, 'subsample': 0.9936380016248796, 'colsample_bytree': 0.8638856304730743, 'colsample_bylevel': 0.818891270833619, 'gamma': 0.398571957782457, 'reg_alpha': 0.26465515140388257, 'reg_lambda': 0.9568267435813372}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:47:28,679]\u001b[0m Trial 633 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.16823719399214532, 'n_estimators': 95, 'max_depth': 7, 'min_child_weight': 9.0990879520876, 'subsample': 0.7366839745269859, 'colsample_bytree': 0.9612902310040545, 'colsample_bylevel': 0.6170359908874016, 'gamma': 0.4859319683511615, 'reg_alpha': 0.3477929022729549, 'reg_lambda': 0.4041512307229933}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:47:28] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:47:28] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:47:28,861]\u001b[0m Trial 634 finished with value: 0.3333333333333333 and parameters: {'booster': 'gblinear', 'learning_rate': 0.08320319002866575, 'n_estimators': 96, 'max_depth': 9, 'min_child_weight': 9.092288633081791, 'subsample': 0.7729211754854522, 'colsample_bytree': 0.9753439044331191, 'colsample_bylevel': 0.6275889877757271, 'gamma': 0.48793585774590953, 'reg_alpha': 0.38665001033675084, 'reg_lambda': 0.42521926798094006}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:47:29,029]\u001b[0m Trial 635 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.11570569988471911, 'n_estimators': 86, 'max_depth': 8, 'min_child_weight': 3.976023485539882, 'subsample': 0.6406095391344561, 'colsample_bytree': 0.7330583743432176, 'colsample_bylevel': 0.7152142516011117, 'gamma': 8.252932629497327e-05, 'reg_alpha': 0.36918573320348735, 'reg_lambda': 0.36125855938457296}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:47:28] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:47:29] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:47:29,201]\u001b[0m Trial 636 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.11154509169859858, 'n_estimators': 86, 'max_depth': 8, 'min_child_weight': 3.8593299710773965, 'subsample': 0.5191896388758195, 'colsample_bytree': 0.9504360779578274, 'colsample_bylevel': 0.5755997737374016, 'gamma': 0.014592088672729311, 'reg_alpha': 0.3197501199281404, 'reg_lambda': 0.680463604150614}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:47:29,368]\u001b[0m Trial 637 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.02140258141053458, 'n_estimators': 84, 'max_depth': 7, 'min_child_weight': 7.765506494306485, 'subsample': 0.9937591521785869, 'colsample_bytree': 0.8764641597431653, 'colsample_bylevel': 0.526492850345351, 'gamma': 0.23345860169706883, 'reg_alpha': 0.24214526572904282, 'reg_lambda': 0.9339973515950394}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:47:29] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:47:29] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:47:29,538]\u001b[0m Trial 638 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.02156772040086563, 'n_estimators': 85, 'max_depth': 7, 'min_child_weight': 7.835435873688847, 'subsample': 0.9985238227959857, 'colsample_bytree': 0.8542349657803645, 'colsample_bylevel': 0.5304230573040906, 'gamma': 0.4383608757456949, 'reg_alpha': 0.14712549137519554, 'reg_lambda': 0.9130156158819852}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:47:29,720]\u001b[0m Trial 639 finished with value: 0.6249999999999999 and parameters: {'booster': 'gblinear', 'learning_rate': 0.0439042563371824, 'n_estimators': 95, 'max_depth': 3, 'min_child_weight': 9.503800164186929, 'subsample': 0.5735390234915495, 'colsample_bytree': 0.7751342102651109, 'colsample_bylevel': 0.6389717703100788, 'gamma': 0.1964393109640831, 'reg_alpha': 0.3463594120817123, 'reg_lambda': 0.40858004198562015}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:47:29] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:47:29] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:47:29,888]\u001b[0m Trial 640 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.10476830077865554, 'n_estimators': 85, 'max_depth': 8, 'min_child_weight': 6.992385652785626, 'subsample': 0.6469644485805758, 'colsample_bytree': 0.72065416396099, 'colsample_bylevel': 0.5143632080238076, 'gamma': 0.03731142512886924, 'reg_alpha': 0.14474002482213033, 'reg_lambda': 0.9041305003535541}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:47:30,067]\u001b[0m Trial 641 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.052246091878896804, 'n_estimators': 95, 'max_depth': 9, 'min_child_weight': 9.208098906308111, 'subsample': 0.5704449143854476, 'colsample_bytree': 0.7094855009676441, 'colsample_bylevel': 0.6200020943206409, 'gamma': 0.31980419129384385, 'reg_alpha': 0.3206948208975379, 'reg_lambda': 0.4769162626529406}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:47:29] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:47:30] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:47:30,247]\u001b[0m Trial 642 finished with value: 0.3333333333333333 and parameters: {'booster': 'gblinear', 'learning_rate': 0.049893301664223885, 'n_estimators': 95, 'max_depth': 9, 'min_child_weight': 3.2720154038790246, 'subsample': 0.5602034776442156, 'colsample_bytree': 0.6988341380896238, 'colsample_bylevel': 0.606246792554265, 'gamma': 0.48016233736915837, 'reg_alpha': 0.3934259648684796, 'reg_lambda': 0.4820409168271062}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:47:30,414]\u001b[0m Trial 643 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.13145028888685098, 'n_estimators': 84, 'max_depth': 7, 'min_child_weight': 8.586923037835124, 'subsample': 0.9885706972324578, 'colsample_bytree': 0.7587651281595963, 'colsample_bylevel': 0.5311966323366126, 'gamma': 0.43537580040116886, 'reg_alpha': 0.20297788707407427, 'reg_lambda': 0.9684650101599833}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:47:30] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:47:30] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:47:30,590]\u001b[0m Trial 644 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.046669885160080526, 'n_estimators': 90, 'max_depth': 9, 'min_child_weight': 9.44756766862043, 'subsample': 0.6857802497089642, 'colsample_bytree': 0.7951651875433635, 'colsample_bylevel': 0.5952616688500014, 'gamma': 0.2997218895776687, 'reg_alpha': 0.3105060504482213, 'reg_lambda': 0.48751867207912475}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:47:30,765]\u001b[0m Trial 645 finished with value: 0.6249999999999999 and parameters: {'booster': 'gblinear', 'learning_rate': 0.05252578563942171, 'n_estimators': 90, 'max_depth': 9, 'min_child_weight': 8.772733572104094, 'subsample': 0.5721940619040058, 'colsample_bytree': 0.7183767561138003, 'colsample_bylevel': 0.6304437684771297, 'gamma': 0.3448404180991312, 'reg_alpha': 0.3576797847148375, 'reg_lambda': 0.28315136543281816}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:47:30] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:47:30,962]\u001b[0m Trial 646 finished with value: 0.3333333333333333 and parameters: {'booster': 'dart', 'learning_rate': 0.10238924097004609, 'n_estimators': 85, 'max_depth': 8, 'min_child_weight': 5.782696634078681, 'subsample': 0.505250163747767, 'colsample_bytree': 0.7262917864337042, 'colsample_bylevel': 0.6988492750001918, 'gamma': 0.05399108502252078, 'reg_alpha': 0.3129797556497497, 'reg_lambda': 0.7613277935657822}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:47:31,130]\u001b[0m Trial 647 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.12930481953541775, 'n_estimators': 84, 'max_depth': 7, 'min_child_weight': 8.2844709380879, 'subsample': 0.906869272559392, 'colsample_bytree': 0.921090385988841, 'colsample_bylevel': 0.5654256297180204, 'gamma': 0.42934833059125665, 'reg_alpha': 0.2529161581865247, 'reg_lambda': 0.9974841984256815}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:47:31] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:47:31] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:47:31,310]\u001b[0m Trial 648 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.046605923969662316, 'n_estimators': 94, 'max_depth': 9, 'min_child_weight': 3.438435581099263, 'subsample': 0.56113523437417, 'colsample_bytree': 0.7958603786121592, 'colsample_bylevel': 0.6226621441194472, 'gamma': 0.35935039558912024, 'reg_alpha': 0.3407869967994874, 'reg_lambda': 0.27798592580034626}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:47:31,496]\u001b[0m Trial 649 finished with value: 0.3333333333333333 and parameters: {'booster': 'gbtree', 'learning_rate': 0.12315817324578276, 'n_estimators': 95, 'max_depth': 9, 'min_child_weight': 9.24640197601999, 'subsample': 0.6561389457175987, 'colsample_bytree': 0.8010439411365348, 'colsample_bylevel': 0.6380501386589607, 'gamma': 0.1142643057771035, 'reg_alpha': 0.32202709134225815, 'reg_lambda': 0.3007930301235786}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:47:31,665]\u001b[0m Trial 650 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.12638092941894088, 'n_estimators': 85, 'max_depth': 7, 'min_child_weight': 7.9816850939689115, 'subsample': 0.9153466489545421, 'colsample_bytree': 0.8985654119533139, 'colsample_bylevel': 0.5212131016827504, 'gamma': 0.42273180050281484, 'reg_alpha': 0.23672263411553499, 'reg_lambda': 0.2099268452090369}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:47:31] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:47:31] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:47:31,847]\u001b[0m Trial 651 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.12537771958569507, 'n_estimators': 95, 'max_depth': 9, 'min_child_weight': 3.351945099940475, 'subsample': 0.5792570125169578, 'colsample_bytree': 0.7025749275472044, 'colsample_bylevel': 0.637958474016616, 'gamma': 0.3012832529232026, 'reg_alpha': 0.3346526923074698, 'reg_lambda': 0.3188520168085269}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:47:32,027]\u001b[0m Trial 652 finished with value: 0.3333333333333333 and parameters: {'booster': 'gblinear', 'learning_rate': 0.042082058540684995, 'n_estimators': 95, 'max_depth': 6, 'min_child_weight': 7.520711055745113, 'subsample': 0.5407776471254585, 'colsample_bytree': 0.9495793011762703, 'colsample_bylevel': 0.6135083067796905, 'gamma': 0.31038684980032505, 'reg_alpha': 0.3633662180649996, 'reg_lambda': 0.28751132254157236}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:47:31] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:47:32] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:47:32,197]\u001b[0m Trial 653 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.13504098417464136, 'n_estimators': 85, 'max_depth': 7, 'min_child_weight': 8.227839324222547, 'subsample': 0.9337029608749953, 'colsample_bytree': 0.8994299325063476, 'colsample_bylevel': 0.528354533379612, 'gamma': 0.427495108662376, 'reg_alpha': 0.23358025616780398, 'reg_lambda': 0.2633217178127169}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:47:32,377]\u001b[0m Trial 654 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.12251292862933781, 'n_estimators': 94, 'max_depth': 6, 'min_child_weight': 9.197459969736627, 'subsample': 0.9041850216773243, 'colsample_bytree': 0.7835361361300628, 'colsample_bylevel': 0.6295434573451543, 'gamma': 0.18655807000661506, 'reg_alpha': 0.3399307639685477, 'reg_lambda': 0.3227512187984539}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:47:32] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:47:32] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:47:32,559]\u001b[0m Trial 655 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.1250357487222561, 'n_estimators': 95, 'max_depth': 6, 'min_child_weight': 8.555752741266287, 'subsample': 0.9761687251615865, 'colsample_bytree': 0.8076367665845962, 'colsample_bylevel': 0.6238203245891124, 'gamma': 0.2498795910440835, 'reg_alpha': 0.3729603718419866, 'reg_lambda': 0.38816880526874314}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:47:32,739]\u001b[0m Trial 656 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.12239696271594924, 'n_estimators': 94, 'max_depth': 6, 'min_child_weight': 9.095177465001862, 'subsample': 0.9698297046459066, 'colsample_bytree': 0.7941959986792849, 'colsample_bylevel': 0.8720976548354664, 'gamma': 0.25097924325798, 'reg_alpha': 0.3259508417379441, 'reg_lambda': 0.3254262324480653}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:47:32] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:47:32] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:47:32,921]\u001b[0m Trial 657 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.1265531557534683, 'n_estimators': 95, 'max_depth': 6, 'min_child_weight': 2.8887222414765357, 'subsample': 0.9567379481910978, 'colsample_bytree': 0.8012627025525565, 'colsample_bylevel': 0.6364479941323069, 'gamma': 0.39049198296060517, 'reg_alpha': 0.31337425040103367, 'reg_lambda': 0.28947805747465805}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:47:33,103]\u001b[0m Trial 658 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.11944012014110322, 'n_estimators': 95, 'max_depth': 6, 'min_child_weight': 9.553656251562531, 'subsample': 0.8932916881138133, 'colsample_bytree': 0.7034397946581474, 'colsample_bylevel': 0.6110971148879052, 'gamma': 0.41079672779115295, 'reg_alpha': 0.3476767376371406, 'reg_lambda': 0.36706421603033873}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:47:32] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:47:33] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:47:33,289]\u001b[0m Trial 659 finished with value: 0.8285714285714286 and parameters: {'booster': 'gblinear', 'learning_rate': 0.1458496780451238, 'n_estimators': 98, 'max_depth': 7, 'min_child_weight': 1.2165306243342293, 'subsample': 0.6871490569494803, 'colsample_bytree': 0.9958003388025914, 'colsample_bylevel': 0.9184689471565077, 'gamma': 0.13635250298214488, 'reg_alpha': 0.22235875216087972, 'reg_lambda': 0.23986425304708542}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:47:33,474]\u001b[0m Trial 660 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.15334387720113105, 'n_estimators': 98, 'max_depth': 7, 'min_child_weight': 1.1193493081005021, 'subsample': 0.9153055970059308, 'colsample_bytree': 0.9139408212900054, 'colsample_bylevel': 0.9100115082915616, 'gamma': 0.13625222107350388, 'reg_alpha': 0.2218660127695707, 'reg_lambda': 0.9697738356366874}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:47:33] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:47:33] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:47:33,662]\u001b[0m Trial 661 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.14704790434577547, 'n_estimators': 98, 'max_depth': 7, 'min_child_weight': 1.396575151363315, 'subsample': 0.911537903134524, 'colsample_bytree': 0.9133260169192252, 'colsample_bylevel': 0.8992259754135312, 'gamma': 0.1116292664868972, 'reg_alpha': 0.18041446821739185, 'reg_lambda': 0.9710223397962168}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:47:33,853]\u001b[0m Trial 662 finished with value: 0.8285714285714286 and parameters: {'booster': 'gblinear', 'learning_rate': 0.14935190966194398, 'n_estimators': 98, 'max_depth': 7, 'min_child_weight': 1.357652112840035, 'subsample': 0.693873445365539, 'colsample_bytree': 0.9100156780652459, 'colsample_bylevel': 0.9116600029869143, 'gamma': 0.12776035195195412, 'reg_alpha': 0.11612296558260378, 'reg_lambda': 0.9754710827778713}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:47:33] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:47:34,050]\u001b[0m Trial 663 finished with value: 0.8285714285714286 and parameters: {'booster': 'gblinear', 'learning_rate': 0.15245503732012183, 'n_estimators': 97, 'max_depth': 6, 'min_child_weight': 1.3774375301703674, 'subsample': 0.9107363182054435, 'colsample_bytree': 0.8359099493338419, 'colsample_bylevel': 0.538772000335929, 'gamma': 0.14310927538929966, 'reg_alpha': 0.19463725338393253, 'reg_lambda': 0.41248878955493257}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:47:33] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:47:34] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:47:34,239]\u001b[0m Trial 664 finished with value: 0.8285714285714286 and parameters: {'booster': 'gblinear', 'learning_rate': 0.06226322060965157, 'n_estimators': 99, 'max_depth': 6, 'min_child_weight': 1.6643206862362185, 'subsample': 0.6948965060609149, 'colsample_bytree': 0.5089426241039234, 'colsample_bylevel': 0.654297598920065, 'gamma': 0.14924914990722044, 'reg_alpha': 0.1963433352293376, 'reg_lambda': 0.3864606393001811}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:47:34,405]\u001b[0m Trial 665 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.03062323640515187, 'n_estimators': 81, 'max_depth': 10, 'min_child_weight': 6.6678965588691765, 'subsample': 0.6071730375349721, 'colsample_bytree': 0.6270802935034527, 'colsample_bylevel': 0.9549354437686642, 'gamma': 0.21060587858253071, 'reg_alpha': 0.2758716058857079, 'reg_lambda': 0.8577687969600247}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:47:34] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:47:34] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:47:34,593]\u001b[0m Trial 666 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.1331940437887826, 'n_estimators': 94, 'max_depth': 6, 'min_child_weight': 7.400763124435545, 'subsample': 0.9775887927474545, 'colsample_bytree': 0.9370200307326487, 'colsample_bylevel': 0.8360755275847379, 'gamma': 0.10357226104098918, 'reg_alpha': 0.3175795652188414, 'reg_lambda': 0.9563783508488117}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:47:34,776]\u001b[0m Trial 667 finished with value: 0.3333333333333333 and parameters: {'booster': 'gblinear', 'learning_rate': 0.13167966028723177, 'n_estimators': 94, 'max_depth': 4, 'min_child_weight': 7.5991466251040976, 'subsample': 0.9626215079925593, 'colsample_bytree': 0.8139250969112961, 'colsample_bylevel': 0.8058720761235235, 'gamma': 0.09383837562162087, 'reg_alpha': 0.5221854196727591, 'reg_lambda': 0.9814469360694404}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:47:34] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:47:34] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:47:34,973]\u001b[0m Trial 668 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.08374443677158945, 'n_estimators': 98, 'max_depth': 6, 'min_child_weight': 1.4769260141854332, 'subsample': 0.8964646005197079, 'colsample_bytree': 0.9824953171881868, 'colsample_bylevel': 0.5393297101542411, 'gamma': 0.13917220476175549, 'reg_alpha': 0.2226794789379767, 'reg_lambda': 0.19292589181196268}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:47:35,189]\u001b[0m Trial 669 finished with value: 0.3333333333333333 and parameters: {'booster': 'dart', 'learning_rate': 0.1250061855926323, 'n_estimators': 95, 'max_depth': 6, 'min_child_weight': 4.829086727724669, 'subsample': 0.9560751285915061, 'colsample_bytree': 0.9368115068556386, 'colsample_bylevel': 0.799110378189977, 'gamma': 0.4113922952095789, 'reg_alpha': 0.2995480090388547, 'reg_lambda': 0.07813016010989432}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:47:35,361]\u001b[0m Trial 670 finished with value: 0.8285714285714286 and parameters: {'booster': 'gbtree', 'learning_rate': 0.07098849326290813, 'n_estimators': 83, 'max_depth': 6, 'min_child_weight': 1.5056398203498387, 'subsample': 0.7143221610850743, 'colsample_bytree': 0.9861400371810242, 'colsample_bylevel': 0.5539871934993663, 'gamma': 0.15384699435004728, 'reg_alpha': 0.21593751457636676, 'reg_lambda': 0.4457219761720291}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:47:35,524]\u001b[0m Trial 671 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.10651100376805381, 'n_estimators': 80, 'max_depth': 10, 'min_child_weight': 6.933189673755852, 'subsample': 0.5938977226926249, 'colsample_bytree': 0.5494850365552566, 'colsample_bylevel': 0.506323354311379, 'gamma': 0.20304854793923469, 'reg_alpha': 0.2576799420452053, 'reg_lambda': 0.8423587834185159}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:47:35] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:47:35] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:47:35,707]\u001b[0m Trial 672 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.13418468744187087, 'n_estimators': 95, 'max_depth': 7, 'min_child_weight': 4.643455078650683, 'subsample': 0.9693045043972299, 'colsample_bytree': 0.8246298431462515, 'colsample_bylevel': 0.7948404175264423, 'gamma': 0.11168354813888873, 'reg_alpha': 0.296214406959232, 'reg_lambda': 0.9822230601446057}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:47:35,891]\u001b[0m Trial 673 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.12935744771066135, 'n_estimators': 96, 'max_depth': 7, 'min_child_weight': 4.8596937005749945, 'subsample': 0.9848308385358202, 'colsample_bytree': 0.8254191962820459, 'colsample_bylevel': 0.8233829835592029, 'gamma': 0.10443948712001105, 'reg_alpha': 0.2987717466735588, 'reg_lambda': 0.980525145243445}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:47:35] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:47:35] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:47:36,075]\u001b[0m Trial 674 finished with value: 0.6249999999999999 and parameters: {'booster': 'gblinear', 'learning_rate': 0.09804743499891341, 'n_estimators': 96, 'max_depth': 7, 'min_child_weight': 4.936142561671289, 'subsample': 0.7594380072050502, 'colsample_bytree': 0.8207600512829936, 'colsample_bylevel': 0.8077979248182094, 'gamma': 0.12246996673043449, 'reg_alpha': 0.16436812456746588, 'reg_lambda': 0.019671334855255712}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:47:36,239]\u001b[0m Trial 675 finished with value: 0.3333333333333333 and parameters: {'booster': 'gblinear', 'learning_rate': 0.017396387845671834, 'n_estimators': 80, 'max_depth': 3, 'min_child_weight': 5.143065083088303, 'subsample': 0.9929958378832613, 'colsample_bytree': 0.6544785852451668, 'colsample_bylevel': 0.8452156881878579, 'gamma': 0.06851795777760729, 'reg_alpha': 0.278623657438975, 'reg_lambda': 0.9484926517583191}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:47:36] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:47:36] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:47:36,427]\u001b[0m Trial 676 finished with value: 0.8285714285714286 and parameters: {'booster': 'gblinear', 'learning_rate': 0.14284476463024381, 'n_estimators': 98, 'max_depth': 6, 'min_child_weight': 1.7090556492355937, 'subsample': 0.7835336800760442, 'colsample_bytree': 0.761319118209722, 'colsample_bylevel': 0.9325899829087315, 'gamma': 0.17422008165846217, 'reg_alpha': 0.2078375582158556, 'reg_lambda': 0.44521988446860916}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:47:36,592]\u001b[0m Trial 677 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.026986913039178315, 'n_estimators': 81, 'max_depth': 10, 'min_child_weight': 6.810257383059402, 'subsample': 0.6050962886076894, 'colsample_bytree': 0.5469719391249844, 'colsample_bylevel': 0.7575122382134026, 'gamma': 0.20252484058337644, 'reg_alpha': 0.2619252614446396, 'reg_lambda': 0.8572911515218756}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:47:36] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:47:36] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:47:36,775]\u001b[0m Trial 678 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.09571994741120556, 'n_estimators': 95, 'max_depth': 6, 'min_child_weight': 9.789714616947009, 'subsample': 0.7501333657156426, 'colsample_bytree': 0.6638437282270889, 'colsample_bylevel': 0.8128678147109046, 'gamma': 0.12032762879469244, 'reg_alpha': 0.28085319155675686, 'reg_lambda': 0.988808190629114}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:47:36,958]\u001b[0m Trial 679 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.09661226572244057, 'n_estimators': 95, 'max_depth': 6, 'min_child_weight': 4.571566136858389, 'subsample': 0.756464969618306, 'colsample_bytree': 0.9271010581101169, 'colsample_bylevel': 0.8243499038362002, 'gamma': 0.11533335037957479, 'reg_alpha': 0.29161290314732885, 'reg_lambda': 0.9684508232876337}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:47:36] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:47:37] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:47:37,146]\u001b[0m Trial 680 finished with value: 0.3333333333333333 and parameters: {'booster': 'gblinear', 'learning_rate': 0.12633705093326306, 'n_estimators': 98, 'max_depth': 6, 'min_child_weight': 4.672591100427071, 'subsample': 0.7468541693471237, 'colsample_bytree': 0.9362928189108353, 'colsample_bylevel': 0.6818720768342321, 'gamma': 0.10470551337140142, 'reg_alpha': 0.5741547305922731, 'reg_lambda': 0.10134204951929338}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:47:37,336]\u001b[0m Trial 681 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.09302327868964819, 'n_estimators': 97, 'max_depth': 5, 'min_child_weight': 6.508677414707834, 'subsample': 0.503409130572632, 'colsample_bytree': 0.5622804265169297, 'colsample_bylevel': 0.6508266700401231, 'gamma': 0.026886482262699164, 'reg_alpha': 0.17370782822421277, 'reg_lambda': 0.896543358148144}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:47:37] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:47:37] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:47:37,504]\u001b[0m Trial 682 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.10244190121053223, 'n_estimators': 80, 'max_depth': 10, 'min_child_weight': 9.74497112182782, 'subsample': 0.6077583026530365, 'colsample_bytree': 0.5605250476057088, 'colsample_bylevel': 0.7661851989069346, 'gamma': 0.337796990874816, 'reg_alpha': 0.2421353336896457, 'reg_lambda': 0.8800512899033985}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:47:37,692]\u001b[0m Trial 683 finished with value: 0.8285714285714286 and parameters: {'booster': 'gblinear', 'learning_rate': 0.053789998961418395, 'n_estimators': 95, 'max_depth': 3, 'min_child_weight': 9.907967084815885, 'subsample': 0.7305735798469369, 'colsample_bytree': 0.6845716792326193, 'colsample_bylevel': 0.8149050025040807, 'gamma': 0.18797392838211574, 'reg_alpha': 0.10026563414188418, 'reg_lambda': 0.9674088849794782}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:47:37] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:47:37] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:47:37,858]\u001b[0m Trial 684 finished with value: 0.3333333333333333 and parameters: {'booster': 'gblinear', 'learning_rate': 0.018433537658030723, 'n_estimators': 80, 'max_depth': 10, 'min_child_weight': 9.585561871189434, 'subsample': 0.6303736884622239, 'colsample_bytree': 0.7128129315845626, 'colsample_bylevel': 0.7843719892347492, 'gamma': 0.35915190280714204, 'reg_alpha': 0.6952824206122012, 'reg_lambda': 0.8145474621307872}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:47:38,026]\u001b[0m Trial 685 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.08675734123898446, 'n_estimators': 83, 'max_depth': 7, 'min_child_weight': 3.2866878466618603, 'subsample': 0.7258703257702595, 'colsample_bytree': 0.9988732593973703, 'colsample_bylevel': 0.897204331421589, 'gamma': 0.44875023579835377, 'reg_alpha': 0.22595025344412292, 'reg_lambda': 0.7553537367632024}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:47:37] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:47:38] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:47:38,217]\u001b[0m Trial 686 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.05466600132495346, 'n_estimators': 98, 'max_depth': 3, 'min_child_weight': 4.585693613222603, 'subsample': 0.6746330842571708, 'colsample_bytree': 0.6914723018798303, 'colsample_bylevel': 0.6768143193070466, 'gamma': 0.2164708116575666, 'reg_alpha': 0.1826808638267742, 'reg_lambda': 0.9491732943343887}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:47:38,393]\u001b[0m Trial 687 finished with value: 0.8285714285714286 and parameters: {'booster': 'gblinear', 'learning_rate': 0.06595927666269756, 'n_estimators': 82, 'max_depth': 3, 'min_child_weight': 1.3465409275127662, 'subsample': 0.550737110128968, 'colsample_bytree': 0.6882649526187202, 'colsample_bylevel': 0.9088857572768531, 'gamma': 0.46599879763942015, 'reg_alpha': 0.15317545156575163, 'reg_lambda': 0.7785116946969809}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:47:38] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:47:38] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:47:38,567]\u001b[0m Trial 688 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.09156113278810622, 'n_estimators': 85, 'max_depth': 5, 'min_child_weight': 6.498430973370336, 'subsample': 0.5167812061510038, 'colsample_bytree': 0.9341053630658184, 'colsample_bylevel': 0.5933314760876968, 'gamma': 0.021726465109255733, 'reg_alpha': 0.3121246822648898, 'reg_lambda': 0.6666604279726557}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:47:38,755]\u001b[0m Trial 689 finished with value: 0.8285714285714286 and parameters: {'booster': 'gblinear', 'learning_rate': 0.06374096204722673, 'n_estimators': 99, 'max_depth': 3, 'min_child_weight': 7.436623971177896, 'subsample': 0.7775924211413812, 'colsample_bytree': 0.6843623963217952, 'colsample_bylevel': 0.6883478629136386, 'gamma': 0.2161511528507949, 'reg_alpha': 0.17393770804481268, 'reg_lambda': 0.12703687526461013}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:47:38] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:47:38,943]\u001b[0m Trial 690 finished with value: 0.3333333333333333 and parameters: {'booster': 'dart', 'learning_rate': 0.025055717294003632, 'n_estimators': 81, 'max_depth': 3, 'min_child_weight': 6.500921631507332, 'subsample': 0.6331557361423539, 'colsample_bytree': 0.5420465151964174, 'colsample_bylevel': 0.7795964624450542, 'gamma': 0.3257978432158695, 'reg_alpha': 0.2772207243950686, 'reg_lambda': 0.8240850902604252}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:47:39,129]\u001b[0m Trial 691 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.14316644912083692, 'n_estimators': 97, 'max_depth': 6, 'min_child_weight': 1.0189353719047267, 'subsample': 0.8813146062153384, 'colsample_bytree': 0.9742621319877978, 'colsample_bylevel': 0.6643660592443299, 'gamma': 0.1233759807139835, 'reg_alpha': 0.24572652800890582, 'reg_lambda': 0.4362352836236201}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:47:38] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:47:39] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:47:39,320]\u001b[0m Trial 692 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.056473617336091506, 'n_estimators': 100, 'max_depth': 4, 'min_child_weight': 7.182526878178841, 'subsample': 0.7700058952224579, 'colsample_bytree': 0.5938805687292626, 'colsample_bylevel': 0.6776880163602691, 'gamma': 0.22780672865112772, 'reg_alpha': 0.23843582285762807, 'reg_lambda': 0.9314720388963698}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:47:39,489]\u001b[0m Trial 693 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.14216457985525716, 'n_estimators': 84, 'max_depth': 7, 'min_child_weight': 5.438369037599285, 'subsample': 0.9984880053057991, 'colsample_bytree': 0.8683905735511195, 'colsample_bylevel': 0.5153052578641241, 'gamma': 0.4319050841015494, 'reg_alpha': 0.24325098259983352, 'reg_lambda': 0.9356063133861484}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:47:39] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:47:39,664]\u001b[0m Trial 694 finished with value: 0.3333333333333333 and parameters: {'booster': 'gbtree', 'learning_rate': 0.1384066130388456, 'n_estimators': 84, 'max_depth': 7, 'min_child_weight': 8.10532808779685, 'subsample': 0.9850440262787349, 'colsample_bytree': 0.8649132158315112, 'colsample_bylevel': 0.5208182978787307, 'gamma': 0.43664674878082305, 'reg_alpha': 0.23306533020546435, 'reg_lambda': 0.9266888115110248}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:47:39,849]\u001b[0m Trial 695 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.03548421127991481, 'n_estimators': 97, 'max_depth': 6, 'min_child_weight': 1.896633499460997, 'subsample': 0.5400931512530144, 'colsample_bytree': 0.677715601271228, 'colsample_bylevel': 0.6477152257849929, 'gamma': 0.1110203249022033, 'reg_alpha': 0.25500854136193135, 'reg_lambda': 0.46527540042935744}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:47:39] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:47:39] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:47:40,036]\u001b[0m Trial 696 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.09932297560698981, 'n_estimators': 99, 'max_depth': 4, 'min_child_weight': 7.208511392855644, 'subsample': 0.761232832138576, 'colsample_bytree': 0.6695463414204743, 'colsample_bylevel': 0.7970621299177424, 'gamma': 0.09175210092754522, 'reg_alpha': 0.27874501424876186, 'reg_lambda': 0.9554867374502196}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:47:40,226]\u001b[0m Trial 697 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.06271918610603762, 'n_estimators': 100, 'max_depth': 4, 'min_child_weight': 7.310747425563546, 'subsample': 0.7438986269313095, 'colsample_bytree': 0.6722001412175383, 'colsample_bylevel': 0.6655874899839254, 'gamma': 0.23813611473621055, 'reg_alpha': 0.2898524779858671, 'reg_lambda': 0.9513855118149245}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:47:40] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:47:40] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:47:40,416]\u001b[0m Trial 698 finished with value: 0.3333333333333333 and parameters: {'booster': 'gblinear', 'learning_rate': 0.09807864331764427, 'n_estimators': 100, 'max_depth': 4, 'min_child_weight': 7.2003195473732795, 'subsample': 0.7969113166638749, 'colsample_bytree': 0.601830428781123, 'colsample_bylevel': 0.6600146655189858, 'gamma': 0.2373206361989413, 'reg_alpha': 0.8315712227714454, 'reg_lambda': 0.9334423039762435}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:47:40,605]\u001b[0m Trial 699 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.08002415953863368, 'n_estimators': 99, 'max_depth': 6, 'min_child_weight': 3.157173505901458, 'subsample': 0.8919359114760714, 'colsample_bytree': 0.987416197450669, 'colsample_bylevel': 0.8887650675481042, 'gamma': 0.484603394042165, 'reg_alpha': 0.2578659844873688, 'reg_lambda': 0.39361446448150894}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:47:40] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:47:40] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:47:40,797]\u001b[0m Trial 700 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.0708871612733416, 'n_estimators': 100, 'max_depth': 4, 'min_child_weight': 7.067577230689169, 'subsample': 0.8539471678170442, 'colsample_bytree': 0.5762938825634768, 'colsample_bylevel': 0.6690914559852996, 'gamma': 0.24377098761781732, 'reg_alpha': 0.2731993419140384, 'reg_lambda': 0.9034736844555964}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:47:40,986]\u001b[0m Trial 701 finished with value: 0.8285714285714286 and parameters: {'booster': 'gblinear', 'learning_rate': 0.1870059995372609, 'n_estimators': 100, 'max_depth': 4, 'min_child_weight': 7.074080366781588, 'subsample': 0.8450835095411678, 'colsample_bytree': 0.5849134642207198, 'colsample_bylevel': 0.6732523104330181, 'gamma': 0.22338010327123312, 'reg_alpha': 0.2625640089481679, 'reg_lambda': 0.029131783012714296}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:47:40] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:47:41] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:47:41,159]\u001b[0m Trial 702 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.11303293837597565, 'n_estimators': 85, 'max_depth': 8, 'min_child_weight': 6.213624079008758, 'subsample': 0.5077951985176937, 'colsample_bytree': 0.7487136285420843, 'colsample_bylevel': 0.5111769146839233, 'gamma': 0.28597249774575995, 'reg_alpha': 0.3464370998701917, 'reg_lambda': 0.7165777962516452}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:47:41,349]\u001b[0m Trial 703 finished with value: 0.8285714285714286 and parameters: {'booster': 'gblinear', 'learning_rate': 0.057644288418820015, 'n_estimators': 100, 'max_depth': 4, 'min_child_weight': 7.067618167459545, 'subsample': 0.8668477217826612, 'colsample_bytree': 0.6075140086006124, 'colsample_bylevel': 0.677504579717146, 'gamma': 0.21456029777050287, 'reg_alpha': 0.1322276889290625, 'reg_lambda': 0.9167604635124437}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:47:41] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:47:41] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:47:41,520]\u001b[0m Trial 704 finished with value: 0.3333333333333333 and parameters: {'booster': 'gblinear', 'learning_rate': 0.1601701827886286, 'n_estimators': 85, 'max_depth': 8, 'min_child_weight': 6.087829486199853, 'subsample': 0.5103655710523713, 'colsample_bytree': 0.7409789090763416, 'colsample_bylevel': 0.7067531779104996, 'gamma': 0.45680458214347386, 'reg_alpha': 0.4091115502799297, 'reg_lambda': 0.5061146320207883}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:47:41,706]\u001b[0m Trial 705 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.13991867303077632, 'n_estimators': 96, 'max_depth': 7, 'min_child_weight': 2.8740155843723096, 'subsample': 0.5337532763529536, 'colsample_bytree': 0.9835179822547757, 'colsample_bylevel': 0.8789115436837065, 'gamma': 0.4862490605296466, 'reg_alpha': 0.2659833437858266, 'reg_lambda': 0.4119690805049059}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:47:41] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:47:41] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:47:41,896]\u001b[0m Trial 706 finished with value: 0.8285714285714286 and parameters: {'booster': 'gblinear', 'learning_rate': 0.1845848781811193, 'n_estimators': 100, 'max_depth': 4, 'min_child_weight': 7.30311082341187, 'subsample': 0.8483227304799976, 'colsample_bytree': 0.6727715077382344, 'colsample_bylevel': 0.6819448234495809, 'gamma': 0.19794914797155747, 'reg_alpha': 0.16537314208546242, 'reg_lambda': 0.9237719924366146}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:47:42,070]\u001b[0m Trial 707 finished with value: 0.3333333333333333 and parameters: {'booster': 'gblinear', 'learning_rate': 0.16133120954778735, 'n_estimators': 87, 'max_depth': 7, 'min_child_weight': 5.491603913559928, 'subsample': 0.92753971896218, 'colsample_bytree': 0.8826199407536818, 'colsample_bylevel': 0.7444365529490758, 'gamma': 0.4416854593557567, 'reg_alpha': 0.07011977560043743, 'reg_lambda': 0.26737494218695246}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:47:41] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:47:42] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:47:42,241]\u001b[0m Trial 708 finished with value: 0.8285714285714286 and parameters: {'booster': 'gblinear', 'learning_rate': 0.13703122141753887, 'n_estimators': 85, 'max_depth': 9, 'min_child_weight': 8.727968563448107, 'subsample': 0.9995788504821754, 'colsample_bytree': 0.8449165298171792, 'colsample_bylevel': 0.7209602908132159, 'gamma': 0.4139292120957741, 'reg_alpha': 0.21271788468578895, 'reg_lambda': 0.2518246017503426}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:47:42,419]\u001b[0m Trial 709 finished with value: 0.8285714285714286 and parameters: {'booster': 'gblinear', 'learning_rate': 0.15647072811391938, 'n_estimators': 91, 'max_depth': 5, 'min_child_weight': 5.412337339938344, 'subsample': 0.505116972317914, 'colsample_bytree': 0.75064505734165, 'colsample_bylevel': 0.7138605569409504, 'gamma': 0.34621725338446663, 'reg_alpha': 0.1967305577352288, 'reg_lambda': 0.5304811066150258}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:47:42] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:47:42] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:47:42,599]\u001b[0m Trial 710 finished with value: 0.8285714285714286 and parameters: {'booster': 'gblinear', 'learning_rate': 0.15705934669216032, 'n_estimators': 91, 'max_depth': 5, 'min_child_weight': 5.303305620206228, 'subsample': 0.5186663959700325, 'colsample_bytree': 0.7610131078712689, 'colsample_bylevel': 0.5165877217937473, 'gamma': 0.03645779089968608, 'reg_alpha': 0.18494529902872336, 'reg_lambda': 0.5182085816682559}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:47:42,818]\u001b[0m Trial 711 finished with value: 0.6249999999999999 and parameters: {'booster': 'dart', 'learning_rate': 0.1461821113794486, 'n_estimators': 98, 'max_depth': 7, 'min_child_weight': 1.2151113070405428, 'subsample': 0.9212187338550177, 'colsample_bytree': 0.875998626877612, 'colsample_bylevel': 0.9716682965118751, 'gamma': 0.13393459457504586, 'reg_alpha': 0.22149963424329966, 'reg_lambda': 0.2137694833486692}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:47:42,992]\u001b[0m Trial 712 finished with value: 0.8285714285714286 and parameters: {'booster': 'gblinear', 'learning_rate': 0.1425908952376699, 'n_estimators': 87, 'max_depth': 7, 'min_child_weight': 1.1676437871225067, 'subsample': 0.9077782468268876, 'colsample_bytree': 0.9004074638453343, 'colsample_bylevel': 0.9598778509225584, 'gamma': 0.14615532928327515, 'reg_alpha': 0.20891565706376253, 'reg_lambda': 0.3011380730496295}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:47:42] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:47:43] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:47:43,171]\u001b[0m Trial 713 finished with value: 0.8285714285714286 and parameters: {'booster': 'gblinear', 'learning_rate': 0.15472385344617884, 'n_estimators': 91, 'max_depth': 10, 'min_child_weight': 5.296324426015269, 'subsample': 0.5874255591791923, 'colsample_bytree': 0.9691377018265868, 'colsample_bylevel': 0.9551499225054058, 'gamma': 0.04214214788220429, 'reg_alpha': 0.18867835298807864, 'reg_lambda': 0.5171215463200085}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:47:43,349]\u001b[0m Trial 714 finished with value: 0.8285714285714286 and parameters: {'booster': 'gblinear', 'learning_rate': 0.1599524293095398, 'n_estimators': 91, 'max_depth': 10, 'min_child_weight': 5.2780038173487815, 'subsample': 0.6618330598980544, 'colsample_bytree': 0.95901778821365, 'colsample_bylevel': 0.9470645591687374, 'gamma': 0.04713202036597724, 'reg_alpha': 0.19749131506204246, 'reg_lambda': 0.49718625479454437}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:47:43] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:47:43] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:47:43,527]\u001b[0m Trial 715 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.15847176572190466, 'n_estimators': 90, 'max_depth': 10, 'min_child_weight': 5.793406895655281, 'subsample': 0.6773237149649083, 'colsample_bytree': 0.9786562084628547, 'colsample_bylevel': 0.9438428922851563, 'gamma': 0.3491905344714141, 'reg_alpha': 0.2389518815730354, 'reg_lambda': 0.5314475212293149}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:47:43,720]\u001b[0m Trial 716 finished with value: 0.3333333333333333 and parameters: {'booster': 'gbtree', 'learning_rate': 0.05851834623890863, 'n_estimators': 99, 'max_depth': 3, 'min_child_weight': 6.9775779730554515, 'subsample': 0.7163102118674523, 'colsample_bytree': 0.5807435781232922, 'colsample_bylevel': 0.6707013934277816, 'gamma': 0.2264815639036859, 'reg_alpha': 0.12715364031935233, 'reg_lambda': 0.8934432221594529}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:47:43,918]\u001b[0m Trial 717 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.04394343324680867, 'n_estimators': 96, 'max_depth': 9, 'min_child_weight': 8.946547884389094, 'subsample': 0.564415547729237, 'colsample_bytree': 0.6960378858821098, 'colsample_bylevel': 0.5987408181686265, 'gamma': 0.32130081843440733, 'reg_alpha': 0.30520691466009364, 'reg_lambda': 0.4819293220235791}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:47:43] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:47:44,112]\u001b[0m Trial 718 finished with value: 0.8285714285714286 and parameters: {'booster': 'gblinear', 'learning_rate': 0.060719529225215785, 'n_estimators': 100, 'max_depth': 4, 'min_child_weight': 7.161611639184371, 'subsample': 0.7497051391938729, 'colsample_bytree': 0.6010618196664428, 'colsample_bylevel': 0.6526969727094372, 'gamma': 0.2089755827527301, 'reg_alpha': 0.15571502686541125, 'reg_lambda': 0.9103113695304795}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:47:43] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:47:44] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:47:44,278]\u001b[0m Trial 719 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.10967006748597084, 'n_estimators': 80, 'max_depth': 10, 'min_child_weight': 5.752681109080324, 'subsample': 0.9468354645939525, 'colsample_bytree': 0.9578821717612487, 'colsample_bylevel': 0.7705908635993874, 'gamma': 0.3617700873265535, 'reg_alpha': 0.27934808800405053, 'reg_lambda': 0.5477791276364673}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:47:44,473]\u001b[0m Trial 720 finished with value: 0.8285714285714286 and parameters: {'booster': 'gblinear', 'learning_rate': 0.06562180183708685, 'n_estimators': 99, 'max_depth': 4, 'min_child_weight': 7.004477780892811, 'subsample': 0.6485141892973727, 'colsample_bytree': 0.6157598479269211, 'colsample_bylevel': 0.6863090762342178, 'gamma': 0.18585101197327147, 'reg_alpha': 0.14270822146714446, 'reg_lambda': 0.8793897289975205}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:47:44] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:47:44] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:47:44,656]\u001b[0m Trial 721 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.10926712423765643, 'n_estimators': 90, 'max_depth': 10, 'min_child_weight': 4.998334429587453, 'subsample': 0.9519352039352394, 'colsample_bytree': 0.9901494223490307, 'colsample_bylevel': 0.757937213057515, 'gamma': 0.3424162109471772, 'reg_alpha': 0.28808817884327514, 'reg_lambda': 0.8120262627938228}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:47:44,852]\u001b[0m Trial 722 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.060458590824643485, 'n_estimators': 99, 'max_depth': 4, 'min_child_weight': 6.932216510188064, 'subsample': 0.6511246413514111, 'colsample_bytree': 0.6075313820987736, 'colsample_bylevel': 0.6929189756616371, 'gamma': 0.17896930856533455, 'reg_alpha': 0.17931213358970532, 'reg_lambda': 0.8657241276018884}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:47:44] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:47:45,061]\u001b[0m Trial 723 finished with value: 0.8285714285714286 and parameters: {'booster': 'gblinear', 'learning_rate': 0.06582120517053201, 'n_estimators': 99, 'max_depth': 3, 'min_child_weight': 7.074584046807011, 'subsample': 0.827493152321142, 'colsample_bytree': 0.5991839329929424, 'colsample_bylevel': 0.7503495353255959, 'gamma': 0.19477461095077786, 'reg_alpha': 0.15925285517275153, 'reg_lambda': 0.8371073731633748}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:47:44] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:47:45] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:47:45,242]\u001b[0m Trial 724 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.03175291415738046, 'n_estimators': 89, 'max_depth': 10, 'min_child_weight': 8.017109607974733, 'subsample': 0.9565207323133408, 'colsample_bytree': 0.8883954244852771, 'colsample_bylevel': 0.7681674555459694, 'gamma': 0.4176892093638613, 'reg_alpha': 0.3034307837589136, 'reg_lambda': 0.7907061438512117}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:47:45,413]\u001b[0m Trial 725 finished with value: 0.8285714285714286 and parameters: {'booster': 'gblinear', 'learning_rate': 0.18007908919972276, 'n_estimators': 83, 'max_depth': 3, 'min_child_weight': 6.804270992030487, 'subsample': 0.7659800967814133, 'colsample_bytree': 0.6671532724675209, 'colsample_bylevel': 0.6947108823190381, 'gamma': 0.1709262937840064, 'reg_alpha': 0.14152895432855989, 'reg_lambda': 0.8855925704337652}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:47:45] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:47:45] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:47:45,596]\u001b[0m Trial 726 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.12931285548980231, 'n_estimators': 92, 'max_depth': 10, 'min_child_weight': 8.019698155159281, 'subsample': 0.9679368773963644, 'colsample_bytree': 0.8564520962991918, 'colsample_bylevel': 0.7889429231437285, 'gamma': 0.400711205297784, 'reg_alpha': 0.2698956659108388, 'reg_lambda': 0.6301522902013352}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:47:45,778]\u001b[0m Trial 727 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.13166228188765056, 'n_estimators': 92, 'max_depth': 3, 'min_child_weight': 8.122217856332513, 'subsample': 0.96760317665741, 'colsample_bytree': 0.8875336264986868, 'colsample_bylevel': 0.8762625766443164, 'gamma': 0.40070590009726076, 'reg_alpha': 0.2873452032584184, 'reg_lambda': 0.6241190557625294}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:47:45] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:47:45] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:47:45,960]\u001b[0m Trial 728 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.13192707979250765, 'n_estimators': 92, 'max_depth': 3, 'min_child_weight': 8.469030973572789, 'subsample': 0.9710650854359517, 'colsample_bytree': 0.8921644804444683, 'colsample_bylevel': 0.8796985027351286, 'gamma': 0.4211859430699816, 'reg_alpha': 0.2548411189238667, 'reg_lambda': 0.6188280735530662}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:47:46,143]\u001b[0m Trial 729 finished with value: 0.8285714285714286 and parameters: {'booster': 'gblinear', 'learning_rate': 0.12578312096337693, 'n_estimators': 93, 'max_depth': 9, 'min_child_weight': 9.371732915674269, 'subsample': 0.9793080381852025, 'colsample_bytree': 0.7042407577872343, 'colsample_bylevel': 0.6360074657864012, 'gamma': 0.4243984543889124, 'reg_alpha': 0.38131965665784884, 'reg_lambda': 0.3196451324536785}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:47:46] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:47:46] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:47:46,331]\u001b[0m Trial 730 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.1987390201805942, 'n_estimators': 98, 'max_depth': 6, 'min_child_weight': 1.0479950429639922, 'subsample': 0.7021747173263982, 'colsample_bytree': 0.9167905497545605, 'colsample_bylevel': 0.9277622357822577, 'gamma': 0.1376639755248531, 'reg_alpha': 0.22429400954370066, 'reg_lambda': 0.9997930922080253}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:47:46,523]\u001b[0m Trial 731 finished with value: 0.6249999999999999 and parameters: {'booster': 'gblinear', 'learning_rate': 0.17305249580353138, 'n_estimators': 100, 'max_depth': 6, 'min_child_weight': 4.227983212752037, 'subsample': 0.8210442698030153, 'colsample_bytree': 0.5941736873140018, 'colsample_bylevel': 0.7027141247632657, 'gamma': 0.22014535668745358, 'reg_alpha': 0.12871845815140912, 'reg_lambda': 0.15199576914929952}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:47:46] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:47:46] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:47:46,702]\u001b[0m Trial 732 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.08470944603955563, 'n_estimators': 89, 'max_depth': 6, 'min_child_weight': 1.0476629015260175, 'subsample': 0.7011351465956384, 'colsample_bytree': 0.9676885052695359, 'colsample_bylevel': 0.9408882211581926, 'gamma': 0.1545794739800262, 'reg_alpha': 0.224918018240928, 'reg_lambda': 0.39485415230508275}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:47:46,881]\u001b[0m Trial 733 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.0869813323714951, 'n_estimators': 89, 'max_depth': 6, 'min_child_weight': 1.022321750603328, 'subsample': 0.7080640629065366, 'colsample_bytree': 0.9726209646044567, 'colsample_bylevel': 0.9178836243868784, 'gamma': 0.44941789182049396, 'reg_alpha': 0.2109510057003749, 'reg_lambda': 0.370967040495853}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:47:46] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:47:46] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:47:47,058]\u001b[0m Trial 734 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.08468069996525282, 'n_estimators': 89, 'max_depth': 6, 'min_child_weight': 1.2072176507180656, 'subsample': 0.7142578890764619, 'colsample_bytree': 0.9645735296158111, 'colsample_bylevel': 0.9214347710730739, 'gamma': 0.4437992395212946, 'reg_alpha': 0.20586094614708147, 'reg_lambda': 0.3791683183639132}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:47:47,250]\u001b[0m Trial 735 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.17655975431404955, 'n_estimators': 100, 'max_depth': 4, 'min_child_weight': 6.922373062951193, 'subsample': 0.8160374490719566, 'colsample_bytree': 0.6012252863585531, 'colsample_bylevel': 0.6970053182588898, 'gamma': 0.20739519642184484, 'reg_alpha': 0.3248904696293127, 'reg_lambda': 0.7939648662348234}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:47:47] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:47:47,443]\u001b[0m Trial 736 finished with value: 0.3333333333333333 and parameters: {'booster': 'dart', 'learning_rate': 0.16732385793867902, 'n_estimators': 82, 'max_depth': 4, 'min_child_weight': 7.551079691279271, 'subsample': 0.8233435563606788, 'colsample_bytree': 0.6173922264968594, 'colsample_bylevel': 0.7098611613447183, 'gamma': 0.47016207407047705, 'reg_alpha': 0.35477422906490313, 'reg_lambda': 0.36223515515352533}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:47:47,628]\u001b[0m Trial 737 finished with value: 0.8285714285714286 and parameters: {'booster': 'gblinear', 'learning_rate': 0.12016000100625013, 'n_estimators': 95, 'max_depth': 9, 'min_child_weight': 9.62980609777343, 'subsample': 0.8908952000519609, 'colsample_bytree': 0.7037607052713571, 'colsample_bylevel': 0.6116379679132804, 'gamma': 0.4982779354095379, 'reg_alpha': 0.38372500522201347, 'reg_lambda': 0.33635048446401516}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:47:47] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:47:47] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:47:47,801]\u001b[0m Trial 738 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.10106578214639232, 'n_estimators': 86, 'max_depth': 10, 'min_child_weight': 5.852435953273818, 'subsample': 0.5850877304113893, 'colsample_bytree': 0.6305944234925828, 'colsample_bylevel': 0.97439947838848, 'gamma': 0.25328343583495877, 'reg_alpha': 0.291774138611781, 'reg_lambda': 0.9225880808443117}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:47:47,997]\u001b[0m Trial 739 finished with value: 0.3333333333333333 and parameters: {'booster': 'gbtree', 'learning_rate': 0.17601484389532512, 'n_estimators': 100, 'max_depth': 4, 'min_child_weight': 7.49473111549473, 'subsample': 0.831726416397532, 'colsample_bytree': 0.5929135972536654, 'colsample_bylevel': 0.7055679347093017, 'gamma': 0.24486483238764553, 'reg_alpha': 0.3192107818926461, 'reg_lambda': 0.33944938684592446}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:47:48,181]\u001b[0m Trial 740 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.1199486243961135, 'n_estimators': 95, 'max_depth': 6, 'min_child_weight': 8.854315172147757, 'subsample': 0.8804454902977379, 'colsample_bytree': 0.7911401392734567, 'colsample_bylevel': 0.623189239393772, 'gamma': 0.2596205264216796, 'reg_alpha': 0.345050656685794, 'reg_lambda': 0.49575164940015465}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:47:48] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:47:48] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:47:48,383]\u001b[0m Trial 741 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.17132085286534013, 'n_estimators': 100, 'max_depth': 4, 'min_child_weight': 7.361780563135002, 'subsample': 0.8369331805590513, 'colsample_bytree': 0.5805279987545666, 'colsample_bylevel': 0.7218378906626072, 'gamma': 0.09182926608262687, 'reg_alpha': 0.35881250797695735, 'reg_lambda': 0.35436507491050345}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:47:48,579]\u001b[0m Trial 742 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.1839188057785734, 'n_estimators': 100, 'max_depth': 4, 'min_child_weight': 7.362622508502183, 'subsample': 0.826250939049838, 'colsample_bytree': 0.6055079966829389, 'colsample_bylevel': 0.7195025436907186, 'gamma': 0.07942028269295057, 'reg_alpha': 0.3361210750546455, 'reg_lambda': 0.368849604634089}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:47:48] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:47:48] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:47:48,748]\u001b[0m Trial 743 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.010392354349751826, 'n_estimators': 80, 'max_depth': 10, 'min_child_weight': 2.1780898616068525, 'subsample': 0.9414762705904496, 'colsample_bytree': 0.6381743671376037, 'colsample_bylevel': 0.975110898397175, 'gamma': 0.0810149857442338, 'reg_alpha': 0.08540179116882474, 'reg_lambda': 0.9107288343475309}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:47:48,916]\u001b[0m Trial 744 finished with value: 0.3333333333333333 and parameters: {'booster': 'gblinear', 'learning_rate': 0.013662469282541086, 'n_estimators': 80, 'max_depth': 10, 'min_child_weight': 2.4656549864858004, 'subsample': 0.9358340030015476, 'colsample_bytree': 0.6420697547067762, 'colsample_bylevel': 0.9651551974248525, 'gamma': 0.10160497007221389, 'reg_alpha': 0.263218988434163, 'reg_lambda': 0.9129170561744759}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:47:48] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:47:48] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:47:49,085]\u001b[0m Trial 745 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.10248634295998021, 'n_estimators': 81, 'max_depth': 8, 'min_child_weight': 4.362345064023629, 'subsample': 0.6041225182536518, 'colsample_bytree': 0.6540485980288392, 'colsample_bylevel': 0.7605903006198971, 'gamma': 0.05832589040894787, 'reg_alpha': 0.28078194830202957, 'reg_lambda': 0.8769766371086456}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:47:49,272]\u001b[0m Trial 746 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.12748848457783044, 'n_estimators': 96, 'max_depth': 6, 'min_child_weight': 9.381838598776042, 'subsample': 0.980828315254036, 'colsample_bytree': 0.825289095805913, 'colsample_bylevel': 0.8373670601113897, 'gamma': 0.12632254061247616, 'reg_alpha': 0.3216774195486345, 'reg_lambda': 0.31420116371140183}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:47:49] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:47:49] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:47:49,464]\u001b[0m Trial 747 finished with value: 0.3333333333333333 and parameters: {'booster': 'gblinear', 'learning_rate': 0.11633892026032752, 'n_estimators': 100, 'max_depth': 4, 'min_child_weight': 7.310305429303516, 'subsample': 0.8337731498782928, 'colsample_bytree': 0.5891401931549964, 'colsample_bylevel': 0.7099788485043438, 'gamma': 0.08507829883713824, 'reg_alpha': 0.39677800826106835, 'reg_lambda': 0.3532128186553296}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:47:49,652]\u001b[0m Trial 748 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.1205370312501164, 'n_estimators': 96, 'max_depth': 7, 'min_child_weight': 7.609790400628019, 'subsample': 0.9802013345787616, 'colsample_bytree': 0.806171363482013, 'colsample_bylevel': 0.8216400471802716, 'gamma': 0.11714353800560057, 'reg_alpha': 0.31496487201026263, 'reg_lambda': 0.1830040579517442}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:47:49] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:47:49] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:47:49,844]\u001b[0m Trial 749 finished with value: 0.6249999999999999 and parameters: {'booster': 'gblinear', 'learning_rate': 0.18314974889620972, 'n_estimators': 100, 'max_depth': 4, 'min_child_weight': 7.2333065479067935, 'subsample': 0.843277173888118, 'colsample_bytree': 0.5797139537240716, 'colsample_bylevel': 0.6947578241871868, 'gamma': 0.09019455943748542, 'reg_alpha': 0.3907173343952882, 'reg_lambda': 0.337508849352639}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:47:50,032]\u001b[0m Trial 750 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.16617606644898253, 'n_estimators': 96, 'max_depth': 8, 'min_child_weight': 1.408064467226604, 'subsample': 0.5294399148636694, 'colsample_bytree': 0.9452797356113787, 'colsample_bylevel': 0.5827733232502657, 'gamma': 0.48707189279744845, 'reg_alpha': 0.2534130504357916, 'reg_lambda': 0.6890170420912914}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:47:49] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:47:50] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:47:50,223]\u001b[0m Trial 751 finished with value: 0.3333333333333333 and parameters: {'booster': 'gblinear', 'learning_rate': 0.1788460275137725, 'n_estimators': 100, 'max_depth': 4, 'min_child_weight': 7.7321374663648035, 'subsample': 0.8565750362332556, 'colsample_bytree': 0.594047958830632, 'colsample_bylevel': 0.6897894427836109, 'gamma': 0.08469177183135279, 'reg_alpha': 0.7539422613552691, 'reg_lambda': 0.3378584414764655}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:47:50,408]\u001b[0m Trial 752 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.047356053700896114, 'n_estimators': 95, 'max_depth': 9, 'min_child_weight': 3.4026608072866518, 'subsample': 0.5643753013955658, 'colsample_bytree': 0.8032625956271366, 'colsample_bylevel': 0.8631344310924769, 'gamma': 0.3103320376293037, 'reg_alpha': 0.30242975743667544, 'reg_lambda': 0.9852955153670033}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:47:50] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:47:50] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:47:50,600]\u001b[0m Trial 753 finished with value: 0.3333333333333333 and parameters: {'booster': 'gblinear', 'learning_rate': 0.1713905297091631, 'n_estimators': 100, 'max_depth': 4, 'min_child_weight': 7.450585029891756, 'subsample': 0.8508221979622905, 'colsample_bytree': 0.5715980082828611, 'colsample_bylevel': 0.7336692884484669, 'gamma': 0.07138182024684665, 'reg_alpha': 0.4136094704772305, 'reg_lambda': 0.9357015617267168}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:47:50,787]\u001b[0m Trial 754 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.12719058775425132, 'n_estimators': 95, 'max_depth': 9, 'min_child_weight': 9.696698542391735, 'subsample': 0.6547125252161958, 'colsample_bytree': 0.9460277166932961, 'colsample_bylevel': 0.8436372188460342, 'gamma': 0.3683623675433787, 'reg_alpha': 0.3305259528968241, 'reg_lambda': 0.9907862759493397}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:47:50] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:47:50] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:47:50,972]\u001b[0m Trial 755 finished with value: 0.3333333333333333 and parameters: {'booster': 'gblinear', 'learning_rate': 0.12849974709585252, 'n_estimators': 94, 'max_depth': 9, 'min_child_weight': 9.943080433617098, 'subsample': 0.664060130963484, 'colsample_bytree': 0.9303377235419981, 'colsample_bylevel': 0.8486492742146877, 'gamma': 0.09885569664122994, 'reg_alpha': 0.36499022840249395, 'reg_lambda': 0.9997061582731643}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:47:51,158]\u001b[0m Trial 756 finished with value: 0.6249999999999999 and parameters: {'booster': 'gblinear', 'learning_rate': 0.0503883899125846, 'n_estimators': 94, 'max_depth': 6, 'min_child_weight': 9.72353974153778, 'subsample': 0.878136250327413, 'colsample_bytree': 0.9268041611533109, 'colsample_bylevel': 0.8255292203745908, 'gamma': 0.11649701255098839, 'reg_alpha': 0.356877538891168, 'reg_lambda': 0.2790075396915468}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:47:51] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:47:51,380]\u001b[0m Trial 757 finished with value: 0.8285714285714286 and parameters: {'booster': 'dart', 'learning_rate': 0.020088368648610872, 'n_estimators': 97, 'max_depth': 8, 'min_child_weight': 1.6356882319483241, 'subsample': 0.735623218473228, 'colsample_bytree': 0.9642484824726232, 'colsample_bylevel': 0.582986884808452, 'gamma': 0.47667373637901606, 'reg_alpha': 0.42965373994810596, 'reg_lambda': 0.43494924398705515}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:47:51,550]\u001b[0m Trial 758 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.1913902725796848, 'n_estimators': 81, 'max_depth': 10, 'min_child_weight': 5.093600098687994, 'subsample': 0.5818997956164369, 'colsample_bytree': 0.5078801761160447, 'colsample_bylevel': 0.9780658771673776, 'gamma': 0.31644850734608787, 'reg_alpha': 0.28113748613336015, 'reg_lambda': 0.7775695609909922}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:47:51] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:47:51] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:47:51,727]\u001b[0m Trial 759 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.1632124070057087, 'n_estimators': 87, 'max_depth': 4, 'min_child_weight': 3.8887446012461147, 'subsample': 0.8632953572164955, 'colsample_bytree': 0.5708369196662005, 'colsample_bylevel': 0.7222161883483602, 'gamma': 0.49106704495617165, 'reg_alpha': 0.3826880058424695, 'reg_lambda': 0.07145018138106847}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:47:51,919]\u001b[0m Trial 760 finished with value: 0.3333333333333333 and parameters: {'booster': 'gblinear', 'learning_rate': 0.17880612979639923, 'n_estimators': 100, 'max_depth': 4, 'min_child_weight': 4.244085271529712, 'subsample': 0.8075626184046336, 'colsample_bytree': 0.5839086396832807, 'colsample_bylevel': 0.7309564260942613, 'gamma': 0.07298571106584426, 'reg_alpha': 0.3664555067088782, 'reg_lambda': 0.8239954171023977}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:47:51] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:47:52,114]\u001b[0m Trial 761 finished with value: 0.3333333333333333 and parameters: {'booster': 'gbtree', 'learning_rate': 0.12276540210874401, 'n_estimators': 99, 'max_depth': 4, 'min_child_weight': 4.94909218750867, 'subsample': 0.9011903017358399, 'colsample_bytree': 0.5600146746466401, 'colsample_bylevel': 0.6692442752998723, 'gamma': 0.3835850552243808, 'reg_alpha': 0.3075570848533275, 'reg_lambda': 0.9684799320102289}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:47:52,298]\u001b[0m Trial 762 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.13501795703870736, 'n_estimators': 93, 'max_depth': 4, 'min_child_weight': 9.949011172668925, 'subsample': 0.8902130168126222, 'colsample_bytree': 0.8015883504498791, 'colsample_bylevel': 0.6525636240250426, 'gamma': 0.3770160420733634, 'reg_alpha': 0.3209647924710437, 'reg_lambda': 0.29013737059315314}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:47:52] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:47:52] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:47:52,494]\u001b[0m Trial 763 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.16827403844614294, 'n_estimators': 99, 'max_depth': 4, 'min_child_weight': 4.418914494440053, 'subsample': 0.8717208935447517, 'colsample_bytree': 0.5679748054723288, 'colsample_bylevel': 0.742632601165757, 'gamma': 0.07649315825023624, 'reg_alpha': 0.3960222831587668, 'reg_lambda': 0.10228450436583016}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:47:52,669]\u001b[0m Trial 764 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.1640144348967383, 'n_estimators': 85, 'max_depth': 4, 'min_child_weight': 3.7906146412554746, 'subsample': 0.8598663824217768, 'colsample_bytree': 0.8543427820494772, 'colsample_bylevel': 0.7158354006812255, 'gamma': 0.0798202149515656, 'reg_alpha': 0.3791804774825993, 'reg_lambda': 0.18896319100855505}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:47:52] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:47:52] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:47:52,848]\u001b[0m Trial 765 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.07113478637597856, 'n_estimators': 88, 'max_depth': 4, 'min_child_weight': 4.04016585598103, 'subsample': 0.8676982994214956, 'colsample_bytree': 0.8575447497450903, 'colsample_bylevel': 0.7044718717740099, 'gamma': 0.06974946545609984, 'reg_alpha': 0.29599961515728845, 'reg_lambda': 0.13587443591340176}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:47:53,028]\u001b[0m Trial 766 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.13708462971943006, 'n_estimators': 88, 'max_depth': 4, 'min_child_weight': 3.753147476287351, 'subsample': 0.8687249950092539, 'colsample_bytree': 0.8660700108482738, 'colsample_bylevel': 0.960740787821268, 'gamma': 0.08899323735099549, 'reg_alpha': 0.28995435372241884, 'reg_lambda': 0.15455121489282903}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:47:52] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:47:53] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:47:53,208]\u001b[0m Trial 767 finished with value: 0.3333333333333333 and parameters: {'booster': 'gblinear', 'learning_rate': 0.17493291382892168, 'n_estimators': 88, 'max_depth': 8, 'min_child_weight': 4.530006787199405, 'subsample': 0.7935559597374137, 'colsample_bytree': 0.5553349800445271, 'colsample_bylevel': 0.9990947892233627, 'gamma': 0.0775992623700334, 'reg_alpha': 0.4376048116237686, 'reg_lambda': 0.1748632313118261}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:47:53,398]\u001b[0m Trial 768 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.09395620223633706, 'n_estimators': 97, 'max_depth': 6, 'min_child_weight': 4.779561374262245, 'subsample': 0.9987378322831227, 'colsample_bytree': 0.6725146558598666, 'colsample_bylevel': 0.5689177027268033, 'gamma': 0.1364709089297701, 'reg_alpha': 0.2495625294736056, 'reg_lambda': 0.9715836979896819}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:47:53] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:47:53] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:47:53,575]\u001b[0m Trial 769 finished with value: 0.3333333333333333 and parameters: {'booster': 'gblinear', 'learning_rate': 0.15173574998136452, 'n_estimators': 84, 'max_depth': 7, 'min_child_weight': 5.486773624392335, 'subsample': 0.9994044132591278, 'colsample_bytree': 0.8773499760420282, 'colsample_bylevel': 0.7419267940392534, 'gamma': 0.43082279885543673, 'reg_alpha': 0.04206453725818152, 'reg_lambda': 0.45767037493218693}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:47:53,765]\u001b[0m Trial 770 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.09575704868266435, 'n_estimators': 97, 'max_depth': 6, 'min_child_weight': 4.9656350113581516, 'subsample': 0.7611678793627489, 'colsample_bytree': 0.6661762190602307, 'colsample_bylevel': 0.8030142957661447, 'gamma': 0.12960366132336643, 'reg_alpha': 0.27451066571559785, 'reg_lambda': 0.9563486615547133}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:47:53] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:47:53] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:47:53,970]\u001b[0m Trial 771 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.10336829592010448, 'n_estimators': 98, 'max_depth': 8, 'min_child_weight': 4.527652655371505, 'subsample': 0.729749719741063, 'colsample_bytree': 0.8436313151407292, 'colsample_bylevel': 0.5286543443794263, 'gamma': 0.39577255543659584, 'reg_alpha': 0.371277458775595, 'reg_lambda': 0.23380680186217012}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:47:54,151]\u001b[0m Trial 772 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.11363226646382979, 'n_estimators': 88, 'max_depth': 8, 'min_child_weight': 9.029642419602705, 'subsample': 0.789000016694483, 'colsample_bytree': 0.8514400498726169, 'colsample_bylevel': 0.9999976946402026, 'gamma': 0.38418959553581294, 'reg_alpha': 0.299231192309192, 'reg_lambda': 0.5752185745374992}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:47:54] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:47:54] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:47:54,327]\u001b[0m Trial 773 finished with value: 0.3333333333333333 and parameters: {'booster': 'gblinear', 'learning_rate': 0.10186272871780722, 'n_estimators': 87, 'max_depth': 8, 'min_child_weight': 4.700281942788788, 'subsample': 0.7907584208661818, 'colsample_bytree': 0.8505431069945936, 'colsample_bylevel': 0.9873845730664098, 'gamma': 0.3726863704619555, 'reg_alpha': 0.9390725717494361, 'reg_lambda': 0.21133365459711612}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:47:54,525]\u001b[0m Trial 774 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.0781644066492532, 'n_estimators': 99, 'max_depth': 6, 'min_child_weight': 4.817304747088434, 'subsample': 0.7684776997216745, 'colsample_bytree': 0.8050745395460793, 'colsample_bylevel': 0.8330374673023994, 'gamma': 0.13310463770859152, 'reg_alpha': 0.24069949288389522, 'reg_lambda': 0.9753758464844576}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:47:54] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:47:54,905]\u001b[0m Trial 775 finished with value: 0.3333333333333333 and parameters: {'booster': 'gblinear', 'learning_rate': 0.08053057895135186, 'n_estimators': 87, 'max_depth': 7, 'min_child_weight': 2.6227053656947894, 'subsample': 0.8036505818840934, 'colsample_bytree': 0.846236323457832, 'colsample_bylevel': 0.9994032680850315, 'gamma': 0.1784783533225337, 'reg_alpha': 0.3686768937519485, 'reg_lambda': 0.5884908639344315}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:47:54] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:47:54] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:47:55,103]\u001b[0m Trial 776 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.09563825517822565, 'n_estimators': 99, 'max_depth': 4, 'min_child_weight': 7.145889956262329, 'subsample': 0.8305713704584575, 'colsample_bytree': 0.6721154800237463, 'colsample_bylevel': 0.7996667031772573, 'gamma': 0.1107813488332979, 'reg_alpha': 0.27137408830575366, 'reg_lambda': 0.9763980641548193}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:47:55,296]\u001b[0m Trial 777 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.09651531253297051, 'n_estimators': 99, 'max_depth': 4, 'min_child_weight': 7.259050267566022, 'subsample': 0.7345285497778111, 'colsample_bytree': 0.6740158835773646, 'colsample_bylevel': 0.8018680427167685, 'gamma': 0.10261883311052224, 'reg_alpha': 0.2867962936031885, 'reg_lambda': 0.9598562197077493}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:47:55] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:47:55] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:47:55,489]\u001b[0m Trial 778 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.09268743154030272, 'n_estimators': 99, 'max_depth': 4, 'min_child_weight': 7.286633543886736, 'subsample': 0.7507847571590567, 'colsample_bytree': 0.6567753798636237, 'colsample_bylevel': 0.7896605553883298, 'gamma': 0.10843979747606397, 'reg_alpha': 0.2877426229960325, 'reg_lambda': 0.953595710108681}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:47:55,683]\u001b[0m Trial 779 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.09307608594297734, 'n_estimators': 100, 'max_depth': 4, 'min_child_weight': 7.085137838592773, 'subsample': 0.7567974674625648, 'colsample_bytree': 0.6575757831469873, 'colsample_bylevel': 0.7912802658803615, 'gamma': 0.2395583374753512, 'reg_alpha': 0.26551807833878466, 'reg_lambda': 0.9516292552377792}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:47:55] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:47:55] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:47:55,877]\u001b[0m Trial 780 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.09206771351443173, 'n_estimators': 99, 'max_depth': 4, 'min_child_weight': 7.13323611798957, 'subsample': 0.7554710689248733, 'colsample_bytree': 0.6825664128061276, 'colsample_bylevel': 0.7840677316731086, 'gamma': 0.11834989546196525, 'reg_alpha': 0.2621316297323388, 'reg_lambda': 0.9529260994301467}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:47:56,079]\u001b[0m Trial 781 finished with value: 0.3333333333333333 and parameters: {'booster': 'dart', 'learning_rate': 0.10755906746752532, 'n_estimators': 86, 'max_depth': 8, 'min_child_weight': 3.8305101104514065, 'subsample': 0.7824288780620097, 'colsample_bytree': 0.5504901184414231, 'colsample_bylevel': 0.9902713879977686, 'gamma': 0.48495504879566353, 'reg_alpha': 0.35217962826173604, 'reg_lambda': 0.3408919213930895}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:47:56,275]\u001b[0m Trial 782 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.08949754385461557, 'n_estimators': 99, 'max_depth': 4, 'min_child_weight': 7.003915191069263, 'subsample': 0.7740219995922685, 'colsample_bytree': 0.6830277142836898, 'colsample_bylevel': 0.8134915483571308, 'gamma': 0.1400546012967049, 'reg_alpha': 0.2657303884223891, 'reg_lambda': 0.975766980107448}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:47:56] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:47:56] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:47:56,471]\u001b[0m Trial 783 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.07086061883128093, 'n_estimators': 100, 'max_depth': 4, 'min_child_weight': 6.923718359576059, 'subsample': 0.7813059356600448, 'colsample_bytree': 0.676016327845222, 'colsample_bylevel': 0.6066637967263117, 'gamma': 0.14656702883795833, 'reg_alpha': 0.2997563085493973, 'reg_lambda': 0.9776968786620989}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:47:56,666]\u001b[0m Trial 784 finished with value: 0.3333333333333333 and parameters: {'booster': 'gbtree', 'learning_rate': 0.07313818039447238, 'n_estimators': 98, 'max_depth': 4, 'min_child_weight': 6.925409892397065, 'subsample': 0.7834483108490311, 'colsample_bytree': 0.6784043420221063, 'colsample_bylevel': 0.6050292509885441, 'gamma': 0.1435615206202746, 'reg_alpha': 0.27416511340430777, 'reg_lambda': 0.9998537558749603}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:47:56,846]\u001b[0m Trial 785 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.14980006513019845, 'n_estimators': 89, 'max_depth': 7, 'min_child_weight': 7.8851180892154575, 'subsample': 0.9325727835182968, 'colsample_bytree': 0.885926945992526, 'colsample_bylevel': 0.5484576509838587, 'gamma': 0.4056821419038109, 'reg_alpha': 0.25395626270096516, 'reg_lambda': 0.9632915085318108}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:47:56] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:47:56] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:47:57,019]\u001b[0m Trial 786 finished with value: 0.8285714285714286 and parameters: {'booster': 'gblinear', 'learning_rate': 0.14617090428874885, 'n_estimators': 84, 'max_depth': 7, 'min_child_weight': 1.2666543323738348, 'subsample': 0.9204901573141709, 'colsample_bytree': 0.8936737015451569, 'colsample_bylevel': 0.5496326062328782, 'gamma': 0.1544250630525691, 'reg_alpha': 0.22992762062423178, 'reg_lambda': 0.22459173967013638}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:47:57,195]\u001b[0m Trial 787 finished with value: 0.8285714285714286 and parameters: {'booster': 'gblinear', 'learning_rate': 0.14068002450026718, 'n_estimators': 83, 'max_depth': 7, 'min_child_weight': 1.454788090454747, 'subsample': 0.9124427564105455, 'colsample_bytree': 0.8720576838566265, 'colsample_bylevel': 0.5341471273611115, 'gamma': 0.1712549935364617, 'reg_alpha': 0.22356333109497684, 'reg_lambda': 0.25677299256013597}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:47:57] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:47:57] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:47:57,378]\u001b[0m Trial 788 finished with value: 0.8285714285714286 and parameters: {'booster': 'gblinear', 'learning_rate': 0.14236616045110656, 'n_estimators': 88, 'max_depth': 7, 'min_child_weight': 1.180200739611609, 'subsample': 0.9234377872007506, 'colsample_bytree': 0.9074432345118633, 'colsample_bylevel': 0.9346268275165136, 'gamma': 0.4371463934891622, 'reg_alpha': 0.2096614293275646, 'reg_lambda': 0.261553171267349}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:47:57,557]\u001b[0m Trial 789 finished with value: 0.8285714285714286 and parameters: {'booster': 'gblinear', 'learning_rate': 0.14551779203256535, 'n_estimators': 88, 'max_depth': 7, 'min_child_weight': 8.719979609384586, 'subsample': 0.9284121084317781, 'colsample_bytree': 0.9027829606931347, 'colsample_bylevel': 0.9463475836354344, 'gamma': 0.432562413298925, 'reg_alpha': 0.23475536361184524, 'reg_lambda': 0.25449801188366405}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:47:57] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:47:57] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:47:57,749]\u001b[0m Trial 790 finished with value: 0.8285714285714286 and parameters: {'booster': 'gblinear', 'learning_rate': 0.13749785653990054, 'n_estimators': 98, 'max_depth': 7, 'min_child_weight': 1.247816223963453, 'subsample': 0.9196085054023777, 'colsample_bytree': 0.9999938900252602, 'colsample_bylevel': 0.9298892745801807, 'gamma': 0.45079572336593166, 'reg_alpha': 0.2102083030341826, 'reg_lambda': 0.23704171091631532}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:47:57,934]\u001b[0m Trial 791 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.07507871757854996, 'n_estimators': 93, 'max_depth': 3, 'min_child_weight': 5.011574483096692, 'subsample': 0.959069268473418, 'colsample_bytree': 0.6959427402143066, 'colsample_bylevel': 0.5672426440779218, 'gamma': 0.16630580430450842, 'reg_alpha': 0.3045312528898715, 'reg_lambda': 0.47568437377734585}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:47:57] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:47:57] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:47:58,128]\u001b[0m Trial 792 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.07505224336675048, 'n_estimators': 100, 'max_depth': 3, 'min_child_weight': 5.005595045997749, 'subsample': 0.8307305851835759, 'colsample_bytree': 0.6953792906302741, 'colsample_bylevel': 0.558428003776505, 'gamma': 0.09973796237379215, 'reg_alpha': 0.28074673346871, 'reg_lambda': 0.5416815149195452}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:47:58,304]\u001b[0m Trial 793 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.11209618787679172, 'n_estimators': 86, 'max_depth': 9, 'min_child_weight': 3.7014986175973803, 'subsample': 0.9894266203093617, 'colsample_bytree': 0.5264295382678882, 'colsample_bylevel': 0.9821099418957274, 'gamma': 0.2747182546993813, 'reg_alpha': 0.3330174544626156, 'reg_lambda': 0.3317668106604998}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:47:58] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:47:58] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:47:58,495]\u001b[0m Trial 794 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.10603118444079393, 'n_estimators': 87, 'max_depth': 4, 'min_child_weight': 3.9401435851543645, 'subsample': 0.8719048880924697, 'colsample_bytree': 0.5386659395006483, 'colsample_bylevel': 0.9912789274517222, 'gamma': 0.26172741994099435, 'reg_alpha': 0.3070449941395442, 'reg_lambda': 0.3957860056337919}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:47:58,673]\u001b[0m Trial 795 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.10482726291386678, 'n_estimators': 87, 'max_depth': 4, 'min_child_weight': 9.459747272540765, 'subsample': 0.8602900046737658, 'colsample_bytree': 0.5454475551970097, 'colsample_bylevel': 0.9956388718482401, 'gamma': 0.27493271350893594, 'reg_alpha': 0.3320951793674938, 'reg_lambda': 0.29967465576885416}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:47:58] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:47:58] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:47:58,850]\u001b[0m Trial 796 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.10092075858040912, 'n_estimators': 86, 'max_depth': 8, 'min_child_weight': 2.2872900895788213, 'subsample': 0.7175838774754371, 'colsample_bytree': 0.5311788486884218, 'colsample_bylevel': 0.9846439509441699, 'gamma': 0.29649007972945807, 'reg_alpha': 0.31842052194189696, 'reg_lambda': 0.8981247603612681}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:47:59,043]\u001b[0m Trial 797 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.09870175178274332, 'n_estimators': 97, 'max_depth': 10, 'min_child_weight': 4.815136577279417, 'subsample': 0.6717378322389567, 'colsample_bytree': 0.6214832640413805, 'colsample_bylevel': 0.8119403769526499, 'gamma': 0.33817860235593245, 'reg_alpha': 0.18033806721059575, 'reg_lambda': 0.9848933314805723}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:47:58] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:47:59] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:47:59,235]\u001b[0m Trial 798 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.09894030777361629, 'n_estimators': 97, 'max_depth': 10, 'min_child_weight': 4.767286537718598, 'subsample': 0.7707319827798087, 'colsample_bytree': 0.6938768134518427, 'colsample_bylevel': 0.8011519041390255, 'gamma': 0.12479269845374526, 'reg_alpha': 0.17905814386870672, 'reg_lambda': 0.5481682781462175}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:47:59,425]\u001b[0m Trial 799 finished with value: 0.8285714285714286 and parameters: {'booster': 'gblinear', 'learning_rate': 0.059412856015077495, 'n_estimators': 97, 'max_depth': 10, 'min_child_weight': 8.968093114369914, 'subsample': 0.7686056075310634, 'colsample_bytree': 0.692723114558804, 'colsample_bylevel': 0.7912766712798874, 'gamma': 0.12822496983354323, 'reg_alpha': 0.16407803447275676, 'reg_lambda': 0.539441224389222}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:47:59] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:47:59] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:47:59,599]\u001b[0m Trial 800 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.13481384649088618, 'n_estimators': 84, 'max_depth': 7, 'min_child_weight': 7.797051831141384, 'subsample': 0.9180514377176431, 'colsample_bytree': 0.9059965311062678, 'colsample_bylevel': 0.9416053062684483, 'gamma': 0.10743132101439529, 'reg_alpha': 0.23519799386955453, 'reg_lambda': 0.9300706841230082}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:47:59,775]\u001b[0m Trial 801 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.0902559654560881, 'n_estimators': 86, 'max_depth': 5, 'min_child_weight': 6.3173161019688955, 'subsample': 0.8526039226976642, 'colsample_bytree': 0.5589562395818053, 'colsample_bylevel': 0.6588594624677853, 'gamma': 0.28282159300919, 'reg_alpha': 0.2528401036799225, 'reg_lambda': 0.6101600128011679}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:47:59] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:47:59,978]\u001b[0m Trial 802 finished with value: 0.3333333333333333 and parameters: {'booster': 'dart', 'learning_rate': 0.09956513406900887, 'n_estimators': 86, 'max_depth': 4, 'min_child_weight': 6.343798439685188, 'subsample': 0.8717396390469239, 'colsample_bytree': 0.563719301723467, 'colsample_bylevel': 0.6602214834266973, 'gamma': 0.2903626758163159, 'reg_alpha': 0.2568479172385241, 'reg_lambda': 0.8671159573081314}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:48:00,169]\u001b[0m Trial 803 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.06755730972293653, 'n_estimators': 98, 'max_depth': 10, 'min_child_weight': 8.074511497479644, 'subsample': 0.9522339712676606, 'colsample_bytree': 0.8844599368360624, 'colsample_bylevel': 0.7776920121156413, 'gamma': 0.4119982155102173, 'reg_alpha': 0.2865700264325134, 'reg_lambda': 0.9987951727432558}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:48:00] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:48:00] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:48:00,361]\u001b[0m Trial 804 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.06260645883658858, 'n_estimators': 98, 'max_depth': 10, 'min_child_weight': 8.064814792114614, 'subsample': 0.9644976910890649, 'colsample_bytree': 0.8868819554109838, 'colsample_bylevel': 0.7742957336507829, 'gamma': 0.41513497918798176, 'reg_alpha': 0.2816460291353659, 'reg_lambda': 0.6202586141660836}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:48:00,538]\u001b[0m Trial 805 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.08912725007538358, 'n_estimators': 86, 'max_depth': 5, 'min_child_weight': 6.2478208638625095, 'subsample': 0.9439204456859506, 'colsample_bytree': 0.5457097663183141, 'colsample_bylevel': 0.9891269947536477, 'gamma': 0.2790902167791472, 'reg_alpha': 0.24918955489361355, 'reg_lambda': 0.2085452566598588}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:48:00] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:48:00] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:48:00,715]\u001b[0m Trial 806 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.10992209111311536, 'n_estimators': 86, 'max_depth': 5, 'min_child_weight': 6.142694212528539, 'subsample': 0.939574864118931, 'colsample_bytree': 0.5328328130143487, 'colsample_bylevel': 0.9827448683696078, 'gamma': 0.004290669715864592, 'reg_alpha': 0.2552014696507622, 'reg_lambda': 0.6372916744380361}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:48:00,896]\u001b[0m Trial 807 finished with value: 0.3333333333333333 and parameters: {'booster': 'gbtree', 'learning_rate': 0.10954272457074762, 'n_estimators': 86, 'max_depth': 5, 'min_child_weight': 6.439300592256382, 'subsample': 0.9361610267262833, 'colsample_bytree': 0.539254386223383, 'colsample_bylevel': 0.981012399588411, 'gamma': 0.3793063620209161, 'reg_alpha': 0.24171462086948559, 'reg_lambda': 0.6007318051147714}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:48:01,080]\u001b[0m Trial 808 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.07420978956011988, 'n_estimators': 92, 'max_depth': 3, 'min_child_weight': 8.400271618467166, 'subsample': 0.7025756457458884, 'colsample_bytree': 0.9404350727947096, 'colsample_bylevel': 0.8641946443316145, 'gamma': 0.1531976921557248, 'reg_alpha': 0.22768654615699696, 'reg_lambda': 0.588716162621958}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:48:00] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:48:01] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:48:01,257]\u001b[0m Trial 809 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.11296205041969293, 'n_estimators': 86, 'max_depth': 8, 'min_child_weight': 6.521083664266407, 'subsample': 0.530946510115339, 'colsample_bytree': 0.5236802599995619, 'colsample_bylevel': 0.9903260299215542, 'gamma': 0.016846477884122198, 'reg_alpha': 0.3076111424945015, 'reg_lambda': 0.858782045572717}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:48:01,433]\u001b[0m Trial 810 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.1038442469803962, 'n_estimators': 85, 'max_depth': 5, 'min_child_weight': 6.336488152669856, 'subsample': 0.9456690067001483, 'colsample_bytree': 0.5197355748351306, 'colsample_bylevel': 0.6666874530515986, 'gamma': 0.016282045017590473, 'reg_alpha': 0.32012902249897723, 'reg_lambda': 0.46671628802149095}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:48:01] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:48:01] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:48:01,611]\u001b[0m Trial 811 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.11629107694180704, 'n_estimators': 86, 'max_depth': 8, 'min_child_weight': 6.2158612756535305, 'subsample': 0.5288067826887822, 'colsample_bytree': 0.5231011525063386, 'colsample_bylevel': 0.6608261415618822, 'gamma': 0.028264246684962923, 'reg_alpha': 0.32577396228434624, 'reg_lambda': 0.18961752866183185}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:48:01,786]\u001b[0m Trial 812 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.11050244391742078, 'n_estimators': 85, 'max_depth': 8, 'min_child_weight': 5.997467366183772, 'subsample': 0.5193541309051174, 'colsample_bytree': 0.8309731333267348, 'colsample_bylevel': 0.6682298645391525, 'gamma': 0.028391983604513574, 'reg_alpha': 0.3186019357659309, 'reg_lambda': 0.15889490390246883}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:48:01] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:48:01] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:48:01,963]\u001b[0m Trial 813 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.10661105931190498, 'n_estimators': 86, 'max_depth': 8, 'min_child_weight': 6.394392641147952, 'subsample': 0.54364235615055, 'colsample_bytree': 0.5508374799025643, 'colsample_bylevel': 0.9725817248884214, 'gamma': 0.030751724025482825, 'reg_alpha': 0.30866067747760306, 'reg_lambda': 0.4456268783663015}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:48:02,140]\u001b[0m Trial 814 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.11279946911587126, 'n_estimators': 86, 'max_depth': 8, 'min_child_weight': 6.182186147634023, 'subsample': 0.5521198758814448, 'colsample_bytree': 0.9538654994760243, 'colsample_bylevel': 0.6661320000981233, 'gamma': 0.009697447346411148, 'reg_alpha': 0.26250695384590905, 'reg_lambda': 0.44533086705361724}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:48:02] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:48:02] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:48:02,337]\u001b[0m Trial 815 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.0845562140767787, 'n_estimators': 99, 'max_depth': 8, 'min_child_weight': 6.6276904051380585, 'subsample': 0.8199426396801293, 'colsample_bytree': 0.8424825604689355, 'colsample_bylevel': 0.6838093101246743, 'gamma': 0.027067445992460415, 'reg_alpha': 0.34414238547855125, 'reg_lambda': 0.37964442575572144}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:48:02,533]\u001b[0m Trial 816 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.08781323774235131, 'n_estimators': 99, 'max_depth': 8, 'min_child_weight': 6.676899526874938, 'subsample': 0.8174436504475145, 'colsample_bytree': 0.8143718153909142, 'colsample_bylevel': 0.6788723904713329, 'gamma': 0.01929768797753819, 'reg_alpha': 0.34494438740619143, 'reg_lambda': 0.41146863834843705}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:48:02] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:48:02] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:48:02,729]\u001b[0m Trial 817 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.11608238104065934, 'n_estimators': 99, 'max_depth': 5, 'min_child_weight': 2.0994593410006055, 'subsample': 0.8406086361413999, 'colsample_bytree': 0.8378685999340242, 'colsample_bylevel': 0.6834991128289949, 'gamma': 0.26951603406053826, 'reg_alpha': 0.3325932884643922, 'reg_lambda': 0.3712116419950129}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:48:02,925]\u001b[0m Trial 818 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.08607339959862931, 'n_estimators': 99, 'max_depth': 5, 'min_child_weight': 6.750042867101762, 'subsample': 0.8207554486370928, 'colsample_bytree': 0.9538833503397903, 'colsample_bylevel': 0.6884146961525949, 'gamma': 0.25956154595352615, 'reg_alpha': 0.35879216749461196, 'reg_lambda': 0.45962260785597}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:48:02] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:48:02] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:48:03,118]\u001b[0m Trial 819 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.11790828934775288, 'n_estimators': 99, 'max_depth': 4, 'min_child_weight': 6.6451318960952115, 'subsample': 0.5017476674853784, 'colsample_bytree': 0.7508829133338494, 'colsample_bylevel': 0.678965410810382, 'gamma': 0.4962996296382317, 'reg_alpha': 0.33619158515383557, 'reg_lambda': 0.35543608540384625}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:48:03,312]\u001b[0m Trial 820 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.11683802257403388, 'n_estimators': 99, 'max_depth': 4, 'min_child_weight': 6.470996693947596, 'subsample': 0.8425309521675233, 'colsample_bytree': 0.7576023231784935, 'colsample_bylevel': 0.701407339095536, 'gamma': 0.4746162276281691, 'reg_alpha': 0.33250494764498084, 'reg_lambda': 0.3602768742268626}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:48:03] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:48:03] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:48:03,509]\u001b[0m Trial 821 finished with value: 0.8285714285714286 and parameters: {'booster': 'gblinear', 'learning_rate': 0.19363905219779037, 'n_estimators': 100, 'max_depth': 4, 'min_child_weight': 7.463635070081131, 'subsample': 0.8409984703968497, 'colsample_bytree': 0.6071378401582009, 'colsample_bylevel': 0.723477273577958, 'gamma': 0.19704349995321896, 'reg_alpha': 0.1637043721810903, 'reg_lambda': 0.8415944536285617}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:48:03,701]\u001b[0m Trial 822 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.14796140235441788, 'n_estimators': 97, 'max_depth': 6, 'min_child_weight': 1.5362836401605207, 'subsample': 0.7002471516448939, 'colsample_bytree': 0.9988288511796667, 'colsample_bylevel': 0.9165879054863977, 'gamma': 0.15547351617162064, 'reg_alpha': 0.20244769983559194, 'reg_lambda': 0.7278097708702076}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:48:03] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:48:03] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:48:03,907]\u001b[0m Trial 823 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.10833986965592017, 'n_estimators': 99, 'max_depth': 4, 'min_child_weight': 2.3830753360356822, 'subsample': 0.6598725922919649, 'colsample_bytree': 0.9485564432782022, 'colsample_bylevel': 0.5135564223503232, 'gamma': 0.008017348455902656, 'reg_alpha': 0.24723603957920268, 'reg_lambda': 0.8746260123224106}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:48:04,106]\u001b[0m Trial 824 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.11245816076801099, 'n_estimators': 100, 'max_depth': 4, 'min_child_weight': 6.220844430783287, 'subsample': 0.6679708047444339, 'colsample_bytree': 0.9532131538149531, 'colsample_bylevel': 0.6542866599826003, 'gamma': 0.004100851284128364, 'reg_alpha': 0.21867431296719786, 'reg_lambda': 0.8601784303064323}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:48:03] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:48:04,334]\u001b[0m Trial 825 finished with value: 0.3333333333333333 and parameters: {'booster': 'dart', 'learning_rate': 0.10791051444346945, 'n_estimators': 99, 'max_depth': 4, 'min_child_weight': 6.061418613090513, 'subsample': 0.6488780179641793, 'colsample_bytree': 0.9682382835352896, 'colsample_bylevel': 0.6623032638467834, 'gamma': 0.013226066907654191, 'reg_alpha': 0.24172526377349418, 'reg_lambda': 0.8888134743992102}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:48:04,535]\u001b[0m Trial 826 finished with value: 0.8285714285714286 and parameters: {'booster': 'gblinear', 'learning_rate': 0.18690520357389098, 'n_estimators': 100, 'max_depth': 4, 'min_child_weight': 7.355082378962979, 'subsample': 0.8278873591346178, 'colsample_bytree': 0.6124824970732877, 'colsample_bylevel': 0.7153358676907393, 'gamma': 0.2124862952213925, 'reg_alpha': 0.09865418026520298, 'reg_lambda': 0.899852870589296}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:48:04] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:48:04,725]\u001b[0m Trial 827 finished with value: 0.3333333333333333 and parameters: {'booster': 'gbtree', 'learning_rate': 0.05124227173093476, 'n_estimators': 91, 'max_depth': 9, 'min_child_weight': 3.016021991088, 'subsample': 0.574007853732091, 'colsample_bytree': 0.7705614013962413, 'colsample_bylevel': 0.5888409183988046, 'gamma': 0.30092144145091543, 'reg_alpha': 0.23184697030874982, 'reg_lambda': 0.4700665491437446}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:48:04,907]\u001b[0m Trial 828 finished with value: 0.6249999999999999 and parameters: {'booster': 'gblinear', 'learning_rate': 0.06436776920715676, 'n_estimators': 82, 'max_depth': 3, 'min_child_weight': 6.871180125079884, 'subsample': 0.8322590163855709, 'colsample_bytree': 0.6225181034366101, 'colsample_bylevel': 0.7346555680751606, 'gamma': 0.0673097498292506, 'reg_alpha': 0.05189106225670587, 'reg_lambda': 0.8819367936093506}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:48:04] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:48:04] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:48:05,102]\u001b[0m Trial 829 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.0935882290164807, 'n_estimators': 96, 'max_depth': 6, 'min_child_weight': 5.027505726587335, 'subsample': 0.7443698711026602, 'colsample_bytree': 0.8151399895492819, 'colsample_bylevel': 0.8331546199439406, 'gamma': 0.12001866917164228, 'reg_alpha': 0.2812442667093312, 'reg_lambda': 0.9507746506418273}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:48:05,282]\u001b[0m Trial 830 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.11534916424677477, 'n_estimators': 84, 'max_depth': 4, 'min_child_weight': 5.907840877621423, 'subsample': 0.5194239142051964, 'colsample_bytree': 0.9597635743461561, 'colsample_bylevel': 0.5105344544098531, 'gamma': 0.006365551080194827, 'reg_alpha': 0.2982313382698484, 'reg_lambda': 0.864861595758862}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:48:05] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:48:05] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:48:05,481]\u001b[0m Trial 831 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.11003148340005904, 'n_estimators': 99, 'max_depth': 4, 'min_child_weight': 5.990023999039989, 'subsample': 0.8485819807620003, 'colsample_bytree': 0.9567445533747471, 'colsample_bylevel': 0.511329421554727, 'gamma': 0.0252228033159724, 'reg_alpha': 0.2617249592379974, 'reg_lambda': 0.8614219112344249}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:48:05,666]\u001b[0m Trial 832 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.1115850300046122, 'n_estimators': 90, 'max_depth': 10, 'min_child_weight': 2.7880311857195688, 'subsample': 0.6864176336110579, 'colsample_bytree': 0.8755854619348976, 'colsample_bylevel': 0.5213511239718753, 'gamma': 0.002694996927739524, 'reg_alpha': 0.2699173422351629, 'reg_lambda': 0.8678541100996897}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:48:05] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:48:05] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:48:05,849]\u001b[0m Trial 833 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.03770896020785157, 'n_estimators': 90, 'max_depth': 10, 'min_child_weight': 2.7227332319507025, 'subsample': 0.5003939638552451, 'colsample_bytree': 0.8639078031950568, 'colsample_bylevel': 0.5084418001868127, 'gamma': 0.03655272824695295, 'reg_alpha': 0.2871237761997989, 'reg_lambda': 0.8409108586268265}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:48:06,029]\u001b[0m Trial 834 finished with value: 0.3333333333333333 and parameters: {'booster': 'gblinear', 'learning_rate': 0.08254398700020078, 'n_estimators': 87, 'max_depth': 10, 'min_child_weight': 2.661484798299868, 'subsample': 0.5433033485788358, 'colsample_bytree': 0.8535221145320359, 'colsample_bylevel': 0.500285686990659, 'gamma': 0.02441746469616646, 'reg_alpha': 0.45767840908993657, 'reg_lambda': 0.8450804269820391}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:48:05] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:48:06] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:48:06,222]\u001b[0m Trial 835 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.18149321318045905, 'n_estimators': 96, 'max_depth': 6, 'min_child_weight': 1.4507309901835457, 'subsample': 0.5338675814372056, 'colsample_bytree': 0.8082879647157929, 'colsample_bylevel': 0.6927005489226646, 'gamma': 0.24916405416015913, 'reg_alpha': 0.30466780827202095, 'reg_lambda': 0.17022048061841089}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:48:06,407]\u001b[0m Trial 836 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.08770351199833079, 'n_estimators': 90, 'max_depth': 10, 'min_child_weight': 2.5375707819369815, 'subsample': 0.5579784460857872, 'colsample_bytree': 0.8622920721715969, 'colsample_bylevel': 0.501372229165932, 'gamma': 0.04218525978573403, 'reg_alpha': 0.27198809323840867, 'reg_lambda': 0.8162079386044827}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:48:06] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:48:06] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:48:06,592]\u001b[0m Trial 837 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.03399880461154795, 'n_estimators': 90, 'max_depth': 9, 'min_child_weight': 2.391780410723376, 'subsample': 0.5643469338885019, 'colsample_bytree': 0.8609046102832466, 'colsample_bylevel': 0.5190693953471961, 'gamma': 0.03869255201561796, 'reg_alpha': 0.26737831229746345, 'reg_lambda': 0.8452728146565482}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:48:06,775]\u001b[0m Trial 838 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.08130074061209935, 'n_estimators': 90, 'max_depth': 10, 'min_child_weight': 2.2802847752331394, 'subsample': 0.5542459998276383, 'colsample_bytree': 0.7813239680343999, 'colsample_bylevel': 0.9346370517026072, 'gamma': 0.2975093519777304, 'reg_alpha': 0.26198259541342866, 'reg_lambda': 0.8110913013642469}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:48:06] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:48:06] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:48:06,959]\u001b[0m Trial 839 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.0882216514162428, 'n_estimators': 90, 'max_depth': 10, 'min_child_weight': 6.6679614083970975, 'subsample': 0.6175095476012984, 'colsample_bytree': 0.7694450293145281, 'colsample_bylevel': 0.901842222679068, 'gamma': 0.029698711589325084, 'reg_alpha': 0.2420032117451687, 'reg_lambda': 0.827986861567726}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:48:07,143]\u001b[0m Trial 840 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.10716224478675146, 'n_estimators': 91, 'max_depth': 10, 'min_child_weight': 6.661481074796272, 'subsample': 0.591005895500267, 'colsample_bytree': 0.8455212414716902, 'colsample_bylevel': 0.9075691007746491, 'gamma': 0.022210562297007456, 'reg_alpha': 0.2871060490506402, 'reg_lambda': 0.8045629824499883}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:48:07] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:48:07] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:48:07,318]\u001b[0m Trial 841 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.19355429943021482, 'n_estimators': 82, 'max_depth': 4, 'min_child_weight': 4.139050177453296, 'subsample': 0.8420114481654326, 'colsample_bytree': 0.596497788428682, 'colsample_bylevel': 0.7027932776011219, 'gamma': 0.48643540956367215, 'reg_alpha': 0.3198465942015462, 'reg_lambda': 0.09359132044883438}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:48:07,493]\u001b[0m Trial 842 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.19657611408581735, 'n_estimators': 82, 'max_depth': 4, 'min_child_weight': 4.105180900114787, 'subsample': 0.8546983458234946, 'colsample_bytree': 0.5951317064616188, 'colsample_bylevel': 0.6975308670574896, 'gamma': 0.48857711277422994, 'reg_alpha': 0.3283031501013587, 'reg_lambda': 0.09539974221701077}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:48:07] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:48:07] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:48:07,675]\u001b[0m Trial 843 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.10327240348632664, 'n_estimators': 89, 'max_depth': 10, 'min_child_weight': 6.43420845030612, 'subsample': 0.5908945930113826, 'colsample_bytree': 0.8502896136244309, 'colsample_bylevel': 0.9558745783142306, 'gamma': 0.22556984456807944, 'reg_alpha': 0.21144926835244754, 'reg_lambda': 0.8475729217728016}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:48:07,857]\u001b[0m Trial 844 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.10488645433735576, 'n_estimators': 89, 'max_depth': 10, 'min_child_weight': 6.691409695119893, 'subsample': 0.6060951426261447, 'colsample_bytree': 0.6510477701505659, 'colsample_bylevel': 0.9493614088330942, 'gamma': 0.23270546408997456, 'reg_alpha': 0.20898161202538, 'reg_lambda': 0.5640096250964209}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:48:07] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:48:07] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:48:08,039]\u001b[0m Trial 845 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.10698588288559001, 'n_estimators': 90, 'max_depth': 10, 'min_child_weight': 6.691613475788806, 'subsample': 0.597139361237805, 'colsample_bytree': 0.6368525221232648, 'colsample_bylevel': 0.9379314636066434, 'gamma': 0.2199778602822941, 'reg_alpha': 0.20148254266994123, 'reg_lambda': 0.8381648436191815}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:48:08,220]\u001b[0m Trial 846 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.10441423129544204, 'n_estimators': 89, 'max_depth': 10, 'min_child_weight': 6.7134331936389975, 'subsample': 0.6144980843246379, 'colsample_bytree': 0.6448306220964168, 'colsample_bylevel': 0.9492931512621069, 'gamma': 0.19982356116673095, 'reg_alpha': 0.23150272937504796, 'reg_lambda': 0.8228564005893162}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:48:08] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:48:08] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:48:08,418]\u001b[0m Trial 847 finished with value: 0.3333333333333333 and parameters: {'booster': 'gblinear', 'learning_rate': 0.12824107147705263, 'n_estimators': 94, 'max_depth': 6, 'min_child_weight': 9.194988027581873, 'subsample': 0.8815066381372041, 'colsample_bytree': 0.7937957840873986, 'colsample_bylevel': 0.6189695526908789, 'gamma': 0.4614355959446846, 'reg_alpha': 0.41044227230986846, 'reg_lambda': 0.3878497402593709}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:48:08,622]\u001b[0m Trial 848 finished with value: 0.3333333333333333 and parameters: {'booster': 'dart', 'learning_rate': 0.09078719436398824, 'n_estimators': 82, 'max_depth': 7, 'min_child_weight': 3.2607056920242066, 'subsample': 0.5455626691588153, 'colsample_bytree': 0.9880044616321878, 'colsample_bylevel': 0.8949328701094835, 'gamma': 0.10895200634518104, 'reg_alpha': 0.30040769086008146, 'reg_lambda': 0.42881866837112076}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:48:08,819]\u001b[0m Trial 849 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.16554521571144418, 'n_estimators': 100, 'max_depth': 4, 'min_child_weight': 4.237091274916643, 'subsample': 0.8563582292252316, 'colsample_bytree': 0.5815609217643305, 'colsample_bylevel': 0.7153962132237118, 'gamma': 0.07630085148452606, 'reg_alpha': 0.39299226916657537, 'reg_lambda': 0.11735754297557788}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:48:08] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:48:08] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:48:09,017]\u001b[0m Trial 850 finished with value: 0.6249999999999999 and parameters: {'booster': 'gblinear', 'learning_rate': 0.16742948261519236, 'n_estimators': 100, 'max_depth': 4, 'min_child_weight': 4.297211692844046, 'subsample': 0.8665145187084216, 'colsample_bytree': 0.5769690768810682, 'colsample_bylevel': 0.7146914668968362, 'gamma': 0.08554677659286988, 'reg_alpha': 0.4034195228461113, 'reg_lambda': 0.12049360130671402}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:48:09,210]\u001b[0m Trial 851 finished with value: 0.3333333333333333 and parameters: {'booster': 'gbtree', 'learning_rate': 0.12909669323939907, 'n_estimators': 93, 'max_depth': 3, 'min_child_weight': 4.859872243547516, 'subsample': 0.9592518743700483, 'colsample_bytree': 0.8022799802156257, 'colsample_bylevel': 0.794398977491889, 'gamma': 0.36331536315378543, 'reg_alpha': 0.28891614388472653, 'reg_lambda': 0.5159271754237787}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:48:09,384]\u001b[0m Trial 852 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.16300899100269375, 'n_estimators': 83, 'max_depth': 4, 'min_child_weight': 3.9884140999395936, 'subsample': 0.8568555497697504, 'colsample_bytree': 0.5877281531497275, 'colsample_bylevel': 0.7038046731240041, 'gamma': 0.06919289802607217, 'reg_alpha': 0.36679598957897885, 'reg_lambda': 0.13301055608864618}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:48:09] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:48:09] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:48:09,556]\u001b[0m Trial 853 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.03233831213400081, 'n_estimators': 80, 'max_depth': 10, 'min_child_weight': 6.502603119715131, 'subsample': 0.601951980966534, 'colsample_bytree': 0.6272843294167231, 'colsample_bylevel': 0.9166152796604176, 'gamma': 0.04828642296987658, 'reg_alpha': 0.1869884377409472, 'reg_lambda': 0.8184794833115079}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:48:09,726]\u001b[0m Trial 854 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.02875565474727999, 'n_estimators': 80, 'max_depth': 10, 'min_child_weight': 6.838213386997353, 'subsample': 0.5951991793055167, 'colsample_bytree': 0.6255548227325256, 'colsample_bylevel': 0.5005478957499456, 'gamma': 0.2284369026207944, 'reg_alpha': 0.2201043344046014, 'reg_lambda': 0.8022895037707178}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:48:09] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:48:09] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:48:09,920]\u001b[0m Trial 855 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.13202894586847178, 'n_estimators': 98, 'max_depth': 3, 'min_child_weight': 8.13996165956427, 'subsample': 0.9546232155494533, 'colsample_bytree': 0.9231090933992913, 'colsample_bylevel': 0.8239382427233082, 'gamma': 0.3928021331241557, 'reg_alpha': 0.24982160432566755, 'reg_lambda': 0.9771846533305014}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:48:10,115]\u001b[0m Trial 856 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.1365213089912138, 'n_estimators': 98, 'max_depth': 3, 'min_child_weight': 8.2010999901094, 'subsample': 0.9554356306589009, 'colsample_bytree': 0.9334890665292979, 'colsample_bylevel': 0.8188048273705657, 'gamma': 0.13354689503842293, 'reg_alpha': 0.18338939674701488, 'reg_lambda': 0.996531929593135}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:48:09] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:48:10] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:48:10,312]\u001b[0m Trial 857 finished with value: 0.3333333333333333 and parameters: {'booster': 'gblinear', 'learning_rate': 0.17434845000847393, 'n_estimators': 100, 'max_depth': 8, 'min_child_weight': 4.438310069059612, 'subsample': 0.7629308377704238, 'colsample_bytree': 0.8310315090134047, 'colsample_bylevel': 0.80596589745245, 'gamma': 0.49793718885982435, 'reg_alpha': 0.37199972846049634, 'reg_lambda': 0.9392988362244356}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:48:10,493]\u001b[0m Trial 858 finished with value: 0.3333333333333333 and parameters: {'booster': 'gblinear', 'learning_rate': 0.16999568801853396, 'n_estimators': 88, 'max_depth': 8, 'min_child_weight': 4.56994282846383, 'subsample': 0.7431731235546183, 'colsample_bytree': 0.8407114296440935, 'colsample_bylevel': 0.723733031913526, 'gamma': 0.4751839041451405, 'reg_alpha': 0.4230808343732352, 'reg_lambda': 0.3286160718602012}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:48:10] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:48:10] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:48:10,689]\u001b[0m Trial 859 finished with value: 0.8285714285714286 and parameters: {'booster': 'gblinear', 'learning_rate': 0.07731971035819528, 'n_estimators': 100, 'max_depth': 8, 'min_child_weight': 4.50127160435168, 'subsample': 0.7722475482838332, 'colsample_bytree': 0.8305148978818288, 'colsample_bylevel': 0.5273811571495782, 'gamma': 0.46404784019432527, 'reg_alpha': 0.3679179295303681, 'reg_lambda': 0.3403625100629408}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:48:10,877]\u001b[0m Trial 860 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.03053088213279551, 'n_estimators': 94, 'max_depth': 7, 'min_child_weight': 1.667963637372238, 'subsample': 0.9789120847111124, 'colsample_bytree': 0.7306920227048302, 'colsample_bylevel': 0.6214379455146106, 'gamma': 0.2009300390436794, 'reg_alpha': 0.31744341511184043, 'reg_lambda': 0.41260830472836374}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:48:10] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:48:10] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:48:11,049]\u001b[0m Trial 861 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.027668291711015158, 'n_estimators': 80, 'max_depth': 10, 'min_child_weight': 6.539298367993683, 'subsample': 0.6226031228163049, 'colsample_bytree': 0.7200675766401505, 'colsample_bylevel': 0.9275581270098043, 'gamma': 0.3163048972314099, 'reg_alpha': 0.2725969707563183, 'reg_lambda': 0.8067281480566071}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:48:11,222]\u001b[0m Trial 862 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.02744021418816865, 'n_estimators': 81, 'max_depth': 10, 'min_child_weight': 6.510029960475696, 'subsample': 0.5808797507587629, 'colsample_bytree': 0.7617921175989331, 'colsample_bylevel': 0.9374958249360389, 'gamma': 0.33192645473926735, 'reg_alpha': 0.2768626149864682, 'reg_lambda': 0.8338138348760398}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:48:11] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:48:11] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:48:11,396]\u001b[0m Trial 863 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.02586301868412671, 'n_estimators': 81, 'max_depth': 10, 'min_child_weight': 7.717436045571851, 'subsample': 0.6132838728250236, 'colsample_bytree': 0.6360662809502674, 'colsample_bylevel': 0.9291733224193786, 'gamma': 0.3386150104855052, 'reg_alpha': 0.2579974020691854, 'reg_lambda': 0.8357022993711039}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:48:11,569]\u001b[0m Trial 864 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.033665157050894186, 'n_estimators': 81, 'max_depth': 10, 'min_child_weight': 5.7931317576045425, 'subsample': 0.600780788963196, 'colsample_bytree': 0.7055817990561435, 'colsample_bylevel': 0.9437225368553803, 'gamma': 0.32858143470641793, 'reg_alpha': 0.26229144085000233, 'reg_lambda': 0.8502159335279487}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:48:11] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:48:11] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:48:11,740]\u001b[0m Trial 865 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.10273772796389365, 'n_estimators': 80, 'max_depth': 10, 'min_child_weight': 6.766491596725087, 'subsample': 0.5873879809767854, 'colsample_bytree': 0.7192738855930745, 'colsample_bylevel': 0.9229905924620889, 'gamma': 0.33437503768635024, 'reg_alpha': 0.2751363998765361, 'reg_lambda': 0.8310154333229789}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:48:11,913]\u001b[0m Trial 866 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.1003750987884891, 'n_estimators': 81, 'max_depth': 10, 'min_child_weight': 5.563487158864007, 'subsample': 0.5962250814863118, 'colsample_bytree': 0.6489895658538168, 'colsample_bylevel': 0.9350119666763422, 'gamma': 0.056232471933315424, 'reg_alpha': 0.24484390087014032, 'reg_lambda': 0.821988409873125}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:48:11] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:48:11] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:48:12,108]\u001b[0m Trial 867 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.1010757647903403, 'n_estimators': 98, 'max_depth': 10, 'min_child_weight': 7.104312044895326, 'subsample': 0.7293643964521851, 'colsample_bytree': 0.6609998457918709, 'colsample_bylevel': 0.6755937235797485, 'gamma': 0.2120544591168146, 'reg_alpha': 0.2336698753064409, 'reg_lambda': 0.9291530475003641}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:48:12,485]\u001b[0m Trial 868 finished with value: 0.8285714285714286 and parameters: {'booster': 'gblinear', 'learning_rate': 0.09748533452606348, 'n_estimators': 98, 'max_depth': 10, 'min_child_weight': 1.1228715706330403, 'subsample': 0.7241505878360922, 'colsample_bytree': 0.6630624210459535, 'colsample_bylevel': 0.6741488898008278, 'gamma': 0.09327261950998411, 'reg_alpha': 0.23465523617366182, 'reg_lambda': 0.0010061501371914638}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:48:12] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:48:12,709]\u001b[0m Trial 869 finished with value: 0.3333333333333333 and parameters: {'booster': 'dart', 'learning_rate': 0.1212960985541289, 'n_estimators': 95, 'max_depth': 7, 'min_child_weight': 3.424081336577127, 'subsample': 0.727004768444694, 'colsample_bytree': 0.8208568137349203, 'colsample_bylevel': 0.8270902766018539, 'gamma': 0.31732555226083475, 'reg_alpha': 0.14196765699632546, 'reg_lambda': 0.9542792147189539}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:48:12,890]\u001b[0m Trial 870 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.15975185792427962, 'n_estimators': 88, 'max_depth': 8, 'min_child_weight': 3.8341607545922662, 'subsample': 0.7792452277775657, 'colsample_bytree': 0.8716368370640624, 'colsample_bylevel': 0.9689771916993523, 'gamma': 0.4752127758998833, 'reg_alpha': 0.301696223586313, 'reg_lambda': 0.198667226432256}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:48:12] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:48:12] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:48:13,061]\u001b[0m Trial 871 finished with value: 0.3333333333333333 and parameters: {'booster': 'gblinear', 'learning_rate': 0.033092735746124084, 'n_estimators': 80, 'max_depth': 10, 'min_child_weight': 6.826732487629855, 'subsample': 0.6185384729020511, 'colsample_bytree': 0.7163298335848721, 'colsample_bylevel': 0.7512960975563737, 'gamma': 0.3520802292520398, 'reg_alpha': 0.9966029355562571, 'reg_lambda': 0.8007511806913301}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:48:13,245]\u001b[0m Trial 872 finished with value: 0.3333333333333333 and parameters: {'booster': 'gbtree', 'learning_rate': 0.0759357746638692, 'n_estimators': 87, 'max_depth': 7, 'min_child_weight': 4.525652357323071, 'subsample': 0.7973721998842656, 'colsample_bytree': 0.5163986218118433, 'colsample_bylevel': 0.5310005038523822, 'gamma': 0.4637708610159253, 'reg_alpha': 0.3816564950210386, 'reg_lambda': 0.2841342597608383}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:48:13,427]\u001b[0m Trial 873 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.154123962114321, 'n_estimators': 88, 'max_depth': 7, 'min_child_weight': 4.43893539416534, 'subsample': 0.8061510314657901, 'colsample_bytree': 0.570064871276071, 'colsample_bylevel': 0.5274073802364699, 'gamma': 0.3686781989694523, 'reg_alpha': 0.3420274291669064, 'reg_lambda': 0.5628667184628212}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:48:13] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:48:13] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:48:13,602]\u001b[0m Trial 874 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.025345582543022712, 'n_estimators': 81, 'max_depth': 10, 'min_child_weight': 6.869954999008344, 'subsample': 0.6093829135574, 'colsample_bytree': 0.7249448782803962, 'colsample_bylevel': 0.7539035544453628, 'gamma': 0.3472481542560257, 'reg_alpha': 0.2573602863440824, 'reg_lambda': 0.797623969097139}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:48:13,772]\u001b[0m Trial 875 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.03152136902186647, 'n_estimators': 80, 'max_depth': 10, 'min_child_weight': 6.68882420323048, 'subsample': 0.6267156109297016, 'colsample_bytree': 0.7307599923690093, 'colsample_bylevel': 0.9643017988706353, 'gamma': 0.33988641424073657, 'reg_alpha': 0.26283920044897857, 'reg_lambda': 0.8227610070015198}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:48:13] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:48:13] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:48:13,962]\u001b[0m Trial 876 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.09687245846652415, 'n_estimators': 81, 'max_depth': 10, 'min_child_weight': 6.9045548645090475, 'subsample': 0.6068606860604464, 'colsample_bytree': 0.7170408438730665, 'colsample_bylevel': 0.7417552751303995, 'gamma': 0.047528457383278766, 'reg_alpha': 0.25375316159703526, 'reg_lambda': 0.7754594285238667}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:48:14,155]\u001b[0m Trial 877 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.12548645754000395, 'n_estimators': 94, 'max_depth': 6, 'min_child_weight': 9.848573216758028, 'subsample': 0.7421321413952414, 'colsample_bytree': 0.7098455413620737, 'colsample_bylevel': 0.8856382582354065, 'gamma': 0.4507353717625506, 'reg_alpha': 0.18035625868166494, 'reg_lambda': 0.7622897147133683}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:48:14] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:48:14] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:48:14,346]\u001b[0m Trial 878 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.05511196869033931, 'n_estimators': 94, 'max_depth': 6, 'min_child_weight': 9.890465427924726, 'subsample': 0.6833160792438712, 'colsample_bytree': 0.7371043543582091, 'colsample_bylevel': 0.8498279885103769, 'gamma': 0.4589625674780063, 'reg_alpha': 0.18046326454479208, 'reg_lambda': 0.9187550160077412}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:48:14,526]\u001b[0m Trial 879 finished with value: 0.3333333333333333 and parameters: {'booster': 'gblinear', 'learning_rate': 0.10426761096976174, 'n_estimators': 81, 'max_depth': 10, 'min_child_weight': 6.935524098617132, 'subsample': 0.6160133909851535, 'colsample_bytree': 0.7139316006460391, 'colsample_bylevel': 0.9550565191630314, 'gamma': 0.269746319400872, 'reg_alpha': 0.6705609535328114, 'reg_lambda': 0.7937023168723801}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:48:14] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:48:14] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:48:14,709]\u001b[0m Trial 880 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.10658804090924366, 'n_estimators': 87, 'max_depth': 9, 'min_child_weight': 4.005551792535102, 'subsample': 0.8152026182678023, 'colsample_bytree': 0.5295948215857432, 'colsample_bylevel': 0.9968289672376667, 'gamma': 0.28433582151756315, 'reg_alpha': 0.34997548384433164, 'reg_lambda': 0.3180085713861006}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:48:14,887]\u001b[0m Trial 881 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.09308422278260829, 'n_estimators': 80, 'max_depth': 10, 'min_child_weight': 5.199804326510628, 'subsample': 0.5011133044720899, 'colsample_bytree': 0.5578078643974326, 'colsample_bylevel': 0.7351082129757133, 'gamma': 0.2653257712460253, 'reg_alpha': 0.24580034944028653, 'reg_lambda': 0.7705083098542979}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:48:14] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:48:14] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:48:15,067]\u001b[0m Trial 882 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.09453555549612842, 'n_estimators': 81, 'max_depth': 10, 'min_child_weight': 5.3670951371196205, 'subsample': 0.5940354791563959, 'colsample_bytree': 0.5566127632228972, 'colsample_bylevel': 0.7443898128626556, 'gamma': 0.27363795644511296, 'reg_alpha': 0.2819062779266034, 'reg_lambda': 0.7481173794101731}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:48:15,249]\u001b[0m Trial 883 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.10097064479846325, 'n_estimators': 87, 'max_depth': 7, 'min_child_weight': 9.190292098464585, 'subsample': 0.7160048214784989, 'colsample_bytree': 0.542916191275844, 'colsample_bylevel': 0.9767714325559343, 'gamma': 0.29210391816817427, 'reg_alpha': 0.36065031273303094, 'reg_lambda': 0.309521236611152}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:48:15] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:48:15] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:48:15,429]\u001b[0m Trial 884 finished with value: 0.6249999999999999 and parameters: {'booster': 'gblinear', 'learning_rate': 0.09960739538143144, 'n_estimators': 87, 'max_depth': 7, 'min_child_weight': 9.098298599187407, 'subsample': 0.791763031090909, 'colsample_bytree': 0.5440735669825675, 'colsample_bylevel': 0.9738091400592959, 'gamma': 0.29521101271688777, 'reg_alpha': 0.3891486347349563, 'reg_lambda': 0.3378743117137273}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:48:15,606]\u001b[0m Trial 885 finished with value: 0.8285714285714286 and parameters: {'booster': 'gblinear', 'learning_rate': 0.05610142419348081, 'n_estimators': 83, 'max_depth': 3, 'min_child_weight': 7.101472896993782, 'subsample': 0.6358879613712156, 'colsample_bytree': 0.686126854826986, 'colsample_bylevel': 0.5887728418213943, 'gamma': 0.18534118401210228, 'reg_alpha': 0.16409887625098396, 'reg_lambda': 0.1425279007273959}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:48:15] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:48:15] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:48:15,797]\u001b[0m Trial 886 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.047057294372603004, 'n_estimators': 95, 'max_depth': 6, 'min_child_weight': 8.846314769252498, 'subsample': 0.8853276084862222, 'colsample_bytree': 0.9424119221836701, 'colsample_bylevel': 0.6011846566175382, 'gamma': 0.25489951109112624, 'reg_alpha': 0.3424826390007578, 'reg_lambda': 0.30044359469261206}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:48:15,977]\u001b[0m Trial 887 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.11590594183330624, 'n_estimators': 87, 'max_depth': 7, 'min_child_weight': 3.6830254702533067, 'subsample': 0.7946594827037204, 'colsample_bytree': 0.5524070082782128, 'colsample_bylevel': 0.9909394100516912, 'gamma': 0.3950555097545836, 'reg_alpha': 0.3441590735083046, 'reg_lambda': 0.6087277605772765}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:48:15] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:48:16] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:48:16,160]\u001b[0m Trial 888 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.1142137751601753, 'n_estimators': 87, 'max_depth': 7, 'min_child_weight': 3.560791737443907, 'subsample': 0.8503970490391731, 'colsample_bytree': 0.5541458083370493, 'colsample_bylevel': 0.9896995741276662, 'gamma': 0.3826285426433838, 'reg_alpha': 0.3052716366481455, 'reg_lambda': 0.5980773379326292}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:48:16,342]\u001b[0m Trial 889 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.11802755046160306, 'n_estimators': 87, 'max_depth': 7, 'min_child_weight': 3.7689785906266007, 'subsample': 0.8441876528615863, 'colsample_bytree': 0.5623091595740297, 'colsample_bylevel': 0.9824615336163469, 'gamma': 0.37468397735728776, 'reg_alpha': 0.31083421129861205, 'reg_lambda': 0.6135949355492637}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:48:16] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:48:16] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:48:16,522]\u001b[0m Trial 890 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.11923047056542722, 'n_estimators': 86, 'max_depth': 7, 'min_child_weight': 3.807989984231333, 'subsample': 0.8477284593754436, 'colsample_bytree': 0.567811187792535, 'colsample_bylevel': 0.9825724594275792, 'gamma': 0.3894290620557753, 'reg_alpha': 0.33101906042609464, 'reg_lambda': 0.5810228385962236}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:48:16,702]\u001b[0m Trial 891 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.06607114657850476, 'n_estimators': 86, 'max_depth': 7, 'min_child_weight': 7.718910394556687, 'subsample': 0.9655070953239969, 'colsample_bytree': 0.5673913825031314, 'colsample_bylevel': 0.9818617195107253, 'gamma': 0.3932554427972765, 'reg_alpha': 0.30585411671191925, 'reg_lambda': 0.5800664108293282}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:48:16] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:48:16] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:48:16,881]\u001b[0m Trial 892 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.06734703043402698, 'n_estimators': 86, 'max_depth': 7, 'min_child_weight': 7.93090675212291, 'subsample': 0.9482310367944001, 'colsample_bytree': 0.5343211017591448, 'colsample_bylevel': 0.9915671551714215, 'gamma': 0.3808355225361384, 'reg_alpha': 0.29742397321456115, 'reg_lambda': 0.21207277669396435}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:48:17,076]\u001b[0m Trial 893 finished with value: 0.3333333333333333 and parameters: {'booster': 'dart', 'learning_rate': 0.11438002119774722, 'n_estimators': 80, 'max_depth': 10, 'min_child_weight': 5.444662961199704, 'subsample': 0.5138782263896436, 'colsample_bytree': 0.7273684229710683, 'colsample_bylevel': 0.5054234777877941, 'gamma': 0.32886897018482003, 'reg_alpha': 0.29101669571575733, 'reg_lambda': 0.7590466192154937}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:48:17,264]\u001b[0m Trial 894 finished with value: 0.3333333333333333 and parameters: {'booster': 'gbtree', 'learning_rate': 0.10962948828698392, 'n_estimators': 86, 'max_depth': 7, 'min_child_weight': 6.236502150598192, 'subsample': 0.9428561064134682, 'colsample_bytree': 0.5371100467560994, 'colsample_bylevel': 0.99833692323254, 'gamma': 0.28178691865913097, 'reg_alpha': 0.2891551630976493, 'reg_lambda': 0.6394439040060179}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:48:17,446]\u001b[0m Trial 895 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.08108046521116628, 'n_estimators': 86, 'max_depth': 5, 'min_child_weight': 7.653171468521337, 'subsample': 0.9396101167016044, 'colsample_bytree': 0.5352778522151274, 'colsample_bylevel': 0.9858079576631297, 'gamma': 0.39679638373037646, 'reg_alpha': 0.27205287869996486, 'reg_lambda': 0.23111456358592009}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:48:17] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:48:17] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:48:17,619]\u001b[0m Trial 896 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.11697261129577102, 'n_estimators': 80, 'max_depth': 10, 'min_child_weight': 5.124864185680437, 'subsample': 0.6398188035048005, 'colsample_bytree': 0.9771236289557127, 'colsample_bylevel': 0.7480763315326908, 'gamma': 0.32447005425415315, 'reg_alpha': 0.2797673130979339, 'reg_lambda': 0.7004612440226271}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:48:17,792]\u001b[0m Trial 897 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.15514597309452974, 'n_estimators': 80, 'max_depth': 10, 'min_child_weight': 5.207686174058003, 'subsample': 0.631802534295182, 'colsample_bytree': 0.9848085293272514, 'colsample_bylevel': 0.7594564223247279, 'gamma': 0.3527634968148879, 'reg_alpha': 0.3159088745200489, 'reg_lambda': 0.7608663543484184}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:48:17] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:48:17] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:48:17,964]\u001b[0m Trial 898 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.14845148252516743, 'n_estimators': 80, 'max_depth': 10, 'min_child_weight': 5.570092456825774, 'subsample': 0.5114416602319308, 'colsample_bytree': 0.9880257657032239, 'colsample_bylevel': 0.7369947852892146, 'gamma': 0.3331598851223735, 'reg_alpha': 0.243949263517137, 'reg_lambda': 0.7217611527066633}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:48:18,137]\u001b[0m Trial 899 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.11744128505064999, 'n_estimators': 81, 'max_depth': 10, 'min_child_weight': 5.2625327487857, 'subsample': 0.6366933237393755, 'colsample_bytree': 0.9844133978886559, 'colsample_bylevel': 0.7606443817371636, 'gamma': 0.35685549280977397, 'reg_alpha': 0.31661513818087783, 'reg_lambda': 0.6928783282709543}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:48:18] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:48:18] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:48:18,320]\u001b[0m Trial 900 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.11523089916044914, 'n_estimators': 88, 'max_depth': 10, 'min_child_weight': 5.238564251040585, 'subsample': 0.9735985805731321, 'colsample_bytree': 0.9766047736174498, 'colsample_bylevel': 0.7506161554806321, 'gamma': 0.34745900725824846, 'reg_alpha': 0.3191837395020007, 'reg_lambda': 0.7474411328075782}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:48:18,511]\u001b[0m Trial 901 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.11393675543667588, 'n_estimators': 89, 'max_depth': 10, 'min_child_weight': 6.07267725188536, 'subsample': 0.96957152278841, 'colsample_bytree': 0.9929137807084141, 'colsample_bylevel': 0.7690282230515493, 'gamma': 0.34135712120683875, 'reg_alpha': 0.33151680450218723, 'reg_lambda': 0.7367931253945488}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:48:18] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:48:18] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:48:18,698]\u001b[0m Trial 902 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.11308060304999337, 'n_estimators': 89, 'max_depth': 10, 'min_child_weight': 5.912693199430836, 'subsample': 0.9675608027962195, 'colsample_bytree': 0.9831945791575509, 'colsample_bylevel': 0.7599418816357132, 'gamma': 0.32372094477527913, 'reg_alpha': 0.31511099133384796, 'reg_lambda': 0.7469096432084414}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:48:18,879]\u001b[0m Trial 903 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.11130077125322066, 'n_estimators': 86, 'max_depth': 8, 'min_child_weight': 6.1494011445013, 'subsample': 0.5504776630300707, 'colsample_bytree': 0.5456662589486465, 'colsample_bylevel': 0.6826389411696451, 'gamma': 0.2633882362041212, 'reg_alpha': 0.34544356283298866, 'reg_lambda': 0.43730532465562244}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:48:18] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:48:18] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:48:19,077]\u001b[0m Trial 904 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.08597282946322192, 'n_estimators': 99, 'max_depth': 8, 'min_child_weight': 6.332718094225261, 'subsample': 0.8164732240297161, 'colsample_bytree': 0.5509799196357004, 'colsample_bylevel': 0.6693694712844018, 'gamma': 0.009704728594656542, 'reg_alpha': 0.3357327390803379, 'reg_lambda': 0.4207101976561956}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:48:19,261]\u001b[0m Trial 905 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.11081384738778383, 'n_estimators': 89, 'max_depth': 10, 'min_child_weight': 6.0664039749133405, 'subsample': 0.6424974343835363, 'colsample_bytree': 0.7343384687269382, 'colsample_bylevel': 0.7719279609006087, 'gamma': 0.0014063647594918308, 'reg_alpha': 0.29799827414498387, 'reg_lambda': 0.7767677927124716}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:48:19] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:48:19] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:48:19,446]\u001b[0m Trial 906 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.11171416890189571, 'n_estimators': 90, 'max_depth': 10, 'min_child_weight': 6.116795303945082, 'subsample': 0.6685906530471775, 'colsample_bytree': 0.8936660573946844, 'colsample_bylevel': 0.7509844629656588, 'gamma': 0.0008252065022553746, 'reg_alpha': 0.3117388012300031, 'reg_lambda': 0.6393862221458056}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:48:19,633]\u001b[0m Trial 907 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.1565961449284555, 'n_estimators': 91, 'max_depth': 10, 'min_child_weight': 6.286895730906445, 'subsample': 0.9752811440164945, 'colsample_bytree': 0.8953383676506005, 'colsample_bylevel': 0.9625613699753564, 'gamma': 0.3100283694849569, 'reg_alpha': 0.29529320319356195, 'reg_lambda': 0.6581263106910143}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:48:19] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:48:19] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:48:19,819]\u001b[0m Trial 908 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.15314817010052936, 'n_estimators': 91, 'max_depth': 10, 'min_child_weight': 5.9390987077619055, 'subsample': 0.9855647376102961, 'colsample_bytree': 0.9051298370007226, 'colsample_bylevel': 0.7573366171968835, 'gamma': 0.2967583925987429, 'reg_alpha': 0.2937308207011785, 'reg_lambda': 0.6479252810392074}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:48:20,005]\u001b[0m Trial 909 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.12312601606935653, 'n_estimators': 90, 'max_depth': 6, 'min_child_weight': 4.929774803649713, 'subsample': 0.6561512292451589, 'colsample_bytree': 0.9056968354463237, 'colsample_bylevel': 0.5769528220704397, 'gamma': 0.1805407027252924, 'reg_alpha': 0.20874599313218523, 'reg_lambda': 0.9784609065623006}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:48:19] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:48:20] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:48:20,192]\u001b[0m Trial 910 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.12308461876989164, 'n_estimators': 90, 'max_depth': 6, 'min_child_weight': 4.936572589710999, 'subsample': 0.653060512261318, 'colsample_bytree': 0.7922683123136524, 'colsample_bylevel': 0.5808550250412685, 'gamma': 0.12582997004408086, 'reg_alpha': 0.22162378616853198, 'reg_lambda': 0.9802984999692664}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:48:20,378]\u001b[0m Trial 911 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.13456305279930866, 'n_estimators': 91, 'max_depth': 10, 'min_child_weight': 2.984681027538912, 'subsample': 0.9884658650297893, 'colsample_bytree': 0.8757824108031033, 'colsample_bylevel': 0.7412896105213703, 'gamma': 0.40883183208540436, 'reg_alpha': 0.2617333571263895, 'reg_lambda': 0.6242345912464702}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:48:20] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:48:20] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:48:20,566]\u001b[0m Trial 912 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.13365752529019015, 'n_estimators': 91, 'max_depth': 9, 'min_child_weight': 8.593725763028088, 'subsample': 0.9907354596168749, 'colsample_bytree': 0.8920419799817811, 'colsample_bylevel': 0.9097963585071719, 'gamma': 0.4156104715540071, 'reg_alpha': 0.2717153545930437, 'reg_lambda': 0.6672251347303364}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:48:20,752]\u001b[0m Trial 913 finished with value: 0.3333333333333333 and parameters: {'booster': 'gblinear', 'learning_rate': 0.13715191528536402, 'n_estimators': 89, 'max_depth': 9, 'min_child_weight': 2.557893768263558, 'subsample': 0.9736514340411044, 'colsample_bytree': 0.8591248442404718, 'colsample_bylevel': 0.8955859980905905, 'gamma': 0.42168106472632816, 'reg_alpha': 0.4826832172463611, 'reg_lambda': 0.6137034601354114}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:48:20] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:48:20,960]\u001b[0m Trial 914 finished with value: 0.3333333333333333 and parameters: {'booster': 'dart', 'learning_rate': 0.10861394963215623, 'n_estimators': 86, 'max_depth': 4, 'min_child_weight': 6.346100657237526, 'subsample': 0.6766065402134603, 'colsample_bytree': 0.9517359977635405, 'colsample_bylevel': 0.6504461862405618, 'gamma': 0.007896116282318602, 'reg_alpha': 0.22348978041304224, 'reg_lambda': 0.8712268705140688}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:48:21,157]\u001b[0m Trial 915 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.10737635531407679, 'n_estimators': 99, 'max_depth': 4, 'min_child_weight': 1.9693799410208686, 'subsample': 0.928067329731687, 'colsample_bytree': 0.9542364521694818, 'colsample_bylevel': 0.6573859512573256, 'gamma': 0.013542433808918071, 'reg_alpha': 0.24124206134892306, 'reg_lambda': 0.8563926768888117}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:48:21] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:48:21] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:48:21,344]\u001b[0m Trial 916 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.14250836029885613, 'n_estimators': 90, 'max_depth': 9, 'min_child_weight': 5.618944715948439, 'subsample': 0.9355043497043739, 'colsample_bytree': 0.7709352861343792, 'colsample_bylevel': 0.9067031753375179, 'gamma': 0.40682808252635533, 'reg_alpha': 0.23858193060301888, 'reg_lambda': 0.7064820473034552}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:48:21,540]\u001b[0m Trial 917 finished with value: 0.3333333333333333 and parameters: {'booster': 'gbtree', 'learning_rate': 0.13580192203776897, 'n_estimators': 91, 'max_depth': 9, 'min_child_weight': 8.233272589411726, 'subsample': 0.9603591024234849, 'colsample_bytree': 0.8910244339831812, 'colsample_bylevel': 0.9665676621863332, 'gamma': 0.40985548349208634, 'reg_alpha': 0.29216555041640635, 'reg_lambda': 0.6434131734744467}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:48:21,736]\u001b[0m Trial 918 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.11050272461320726, 'n_estimators': 96, 'max_depth': 5, 'min_child_weight': 5.963769772594556, 'subsample': 0.5176089632952308, 'colsample_bytree': 0.9620230045997147, 'colsample_bylevel': 0.9976561477264136, 'gamma': 0.024858809846525207, 'reg_alpha': 0.26222023152554685, 'reg_lambda': 0.4622592273201399}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:48:21] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:48:21] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:48:21,924]\u001b[0m Trial 919 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.1445965673879515, 'n_estimators': 91, 'max_depth': 9, 'min_child_weight': 8.390428160604268, 'subsample': 0.9805367248970306, 'colsample_bytree': 0.8618682134363539, 'colsample_bylevel': 0.9012180014070592, 'gamma': 0.4023934123925976, 'reg_alpha': 0.2485559854971624, 'reg_lambda': 0.690372574690756}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:48:22,103]\u001b[0m Trial 920 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.1444016710265368, 'n_estimators': 85, 'max_depth': 9, 'min_child_weight': 5.773544665376908, 'subsample': 0.9466959827057716, 'colsample_bytree': 0.8695176517482511, 'colsample_bylevel': 0.9021476018340624, 'gamma': 0.4252121382594624, 'reg_alpha': 0.22661783078528716, 'reg_lambda': 0.7099867555695603}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:48:21] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:48:22] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:48:22,281]\u001b[0m Trial 921 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.13776114010532084, 'n_estimators': 85, 'max_depth': 9, 'min_child_weight': 5.304970253427943, 'subsample': 0.9490166920644966, 'colsample_bytree': 0.8516089723753172, 'colsample_bylevel': 0.8927360061823358, 'gamma': 0.4100942698342807, 'reg_alpha': 0.23788119517279802, 'reg_lambda': 0.657411297241592}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:48:22,468]\u001b[0m Trial 922 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.13790610820102078, 'n_estimators': 91, 'max_depth': 9, 'min_child_weight': 8.298506745649401, 'subsample': 0.9655712498436582, 'colsample_bytree': 0.8685696867623238, 'colsample_bylevel': 0.8883131795254022, 'gamma': 0.42171338485939464, 'reg_alpha': 0.27270523830422716, 'reg_lambda': 0.7027558671645935}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:48:22] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:48:22] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:48:22,654]\u001b[0m Trial 923 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.13172126252118804, 'n_estimators': 89, 'max_depth': 9, 'min_child_weight': 5.899391054115414, 'subsample': 0.9390366198635859, 'colsample_bytree': 0.8747432714969222, 'colsample_bylevel': 0.9493180479770829, 'gamma': 0.4164415488343896, 'reg_alpha': 0.22626911844981265, 'reg_lambda': 0.5929999101948711}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:48:22,838]\u001b[0m Trial 924 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.10288474006363801, 'n_estimators': 89, 'max_depth': 9, 'min_child_weight': 8.495938487870438, 'subsample': 0.9435978214231664, 'colsample_bytree': 0.8415698619599139, 'colsample_bylevel': 0.9588086192677469, 'gamma': 0.40169043902452556, 'reg_alpha': 0.23847030292959306, 'reg_lambda': 0.6769697713086427}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:48:22] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:48:22] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:48:23,015]\u001b[0m Trial 925 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.028718205634388635, 'n_estimators': 82, 'max_depth': 9, 'min_child_weight': 6.5132546933123, 'subsample': 0.9742632806623859, 'colsample_bytree': 0.8823251389578102, 'colsample_bylevel': 0.9596630385584656, 'gamma': 0.3877251851676631, 'reg_alpha': 0.2529559379659579, 'reg_lambda': 0.6906213874812445}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:48:23,191]\u001b[0m Trial 926 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.03465179108692133, 'n_estimators': 82, 'max_depth': 9, 'min_child_weight': 8.304533599290341, 'subsample': 0.9501026016128489, 'colsample_bytree': 0.8388059916409608, 'colsample_bylevel': 0.9642324436975198, 'gamma': 0.4022904824929807, 'reg_alpha': 0.2112076956705502, 'reg_lambda': 0.6768951780318463}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:48:23] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:48:23] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:48:23,368]\u001b[0m Trial 927 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.030878935030375417, 'n_estimators': 83, 'max_depth': 9, 'min_child_weight': 8.608052238348064, 'subsample': 0.5992466724639188, 'colsample_bytree': 0.8368455300217372, 'colsample_bylevel': 0.920686771069883, 'gamma': 0.3700086884160977, 'reg_alpha': 0.20013634941925262, 'reg_lambda': 0.9044093208401285}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:48:23,546]\u001b[0m Trial 928 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.02240300246620661, 'n_estimators': 84, 'max_depth': 9, 'min_child_weight': 8.681721905636998, 'subsample': 0.5917665064663095, 'colsample_bytree': 0.8299189693619654, 'colsample_bylevel': 0.9216262458905309, 'gamma': 0.41832747011664384, 'reg_alpha': 0.20514929102254126, 'reg_lambda': 0.8972876682973346}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:48:23] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:48:23] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:48:23,734]\u001b[0m Trial 929 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.08983200758125365, 'n_estimators': 90, 'max_depth': 9, 'min_child_weight': 2.213916003775072, 'subsample': 0.6314996860866521, 'colsample_bytree': 0.7723768336151933, 'colsample_bylevel': 0.5150491229152083, 'gamma': 0.014124022157817932, 'reg_alpha': 0.20416918097309425, 'reg_lambda': 0.8881421032928515}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:48:23,933]\u001b[0m Trial 930 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.08994051279066191, 'n_estimators': 90, 'max_depth': 9, 'min_child_weight': 2.306905669884456, 'subsample': 0.5266431827671388, 'colsample_bytree': 0.7684996485870991, 'colsample_bylevel': 0.5176849530961616, 'gamma': 0.0220117540549878, 'reg_alpha': 0.18948399026070162, 'reg_lambda': 0.8768633123875711}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:48:23] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:48:24,118]\u001b[0m Trial 931 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.03954390699439897, 'n_estimators': 83, 'max_depth': 3, 'min_child_weight': 8.050750408909753, 'subsample': 0.6107680021234618, 'colsample_bytree': 0.8381352849324057, 'colsample_bylevel': 0.9167840523839266, 'gamma': 0.4024238547826567, 'reg_alpha': 0.2189631773030181, 'reg_lambda': 0.9193114609228301}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:48:24] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:48:24] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:48:24,300]\u001b[0m Trial 932 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.024002704253714986, 'n_estimators': 84, 'max_depth': 3, 'min_child_weight': 7.97954408986049, 'subsample': 0.6219946654946813, 'colsample_bytree': 0.8217464310749193, 'colsample_bylevel': 0.8754672734267966, 'gamma': 0.4181749921986708, 'reg_alpha': 0.19797820539151578, 'reg_lambda': 0.9138913786928077}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:48:24,484]\u001b[0m Trial 933 finished with value: 0.3333333333333333 and parameters: {'booster': 'gblinear', 'learning_rate': 0.024622989727100794, 'n_estimators': 84, 'max_depth': 3, 'min_child_weight': 5.909513175906973, 'subsample': 0.9495480840195231, 'colsample_bytree': 0.856869788556309, 'colsample_bylevel': 0.9087136762257709, 'gamma': 0.3994648872145962, 'reg_alpha': 0.5379440766785704, 'reg_lambda': 0.6501445762794633}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:48:24] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:48:24] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:48:24,675]\u001b[0m Trial 934 finished with value: 0.3333333333333333 and parameters: {'booster': 'gblinear', 'learning_rate': 0.19416569606304815, 'n_estimators': 87, 'max_depth': 4, 'min_child_weight': 2.152814143150919, 'subsample': 0.5336337565334119, 'colsample_bytree': 0.5803528107643405, 'colsample_bylevel': 0.5183276436998424, 'gamma': 0.02898326413851713, 'reg_alpha': 0.885498291146161, 'reg_lambda': 0.0789615842288432}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:48:24,865]\u001b[0m Trial 935 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.1978362132922398, 'n_estimators': 86, 'max_depth': 5, 'min_child_weight': 6.438734728566393, 'subsample': 0.8443844095018153, 'colsample_bytree': 0.5721144449999024, 'colsample_bylevel': 0.5213035245433569, 'gamma': 0.03547438426454619, 'reg_alpha': 0.31052830670513815, 'reg_lambda': 0.07966572967930259}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:48:24] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:48:24] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:48:25,050]\u001b[0m Trial 936 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.03887874053768581, 'n_estimators': 83, 'max_depth': 3, 'min_child_weight': 5.750763120073871, 'subsample': 0.6208810061751049, 'colsample_bytree': 0.8679567849002888, 'colsample_bylevel': 0.9113545116529524, 'gamma': 0.42573559102808056, 'reg_alpha': 0.2714190914175162, 'reg_lambda': 0.9081577522576523}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:48:25,254]\u001b[0m Trial 937 finished with value: 0.3333333333333333 and parameters: {'booster': 'dart', 'learning_rate': 0.034588024556195286, 'n_estimators': 83, 'max_depth': 3, 'min_child_weight': 6.5218659329637365, 'subsample': 0.9301879216861418, 'colsample_bytree': 0.8468823765669642, 'colsample_bylevel': 0.9146790378156305, 'gamma': 0.04409139285859695, 'reg_alpha': 0.2220102877734184, 'reg_lambda': 0.8970748174158806}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:48:25,446]\u001b[0m Trial 938 finished with value: 0.3333333333333333 and parameters: {'booster': 'gbtree', 'learning_rate': 0.10572747773584013, 'n_estimators': 90, 'max_depth': 10, 'min_child_weight': 6.748855188236257, 'subsample': 0.5733248498589374, 'colsample_bytree': 0.6534266379107485, 'colsample_bylevel': 0.5013844454067636, 'gamma': 0.2400373354533056, 'reg_alpha': 0.2912310851826782, 'reg_lambda': 0.8535097064656971}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:48:25,645]\u001b[0m Trial 939 finished with value: 0.8285714285714286 and parameters: {'booster': 'gblinear', 'learning_rate': 0.07289668931223892, 'n_estimators': 98, 'max_depth': 3, 'min_child_weight': 7.219532151438906, 'subsample': 0.7614204764098397, 'colsample_bytree': 0.6874417056296774, 'colsample_bylevel': 0.6373631324034973, 'gamma': 0.21561968059931239, 'reg_alpha': 0.11979816815519478, 'reg_lambda': 0.9217750467059045}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:48:25] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:48:25] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:48:25,823]\u001b[0m Trial 940 finished with value: 0.8285714285714286 and parameters: {'booster': 'gblinear', 'learning_rate': 0.07100121603561807, 'n_estimators': 83, 'max_depth': 3, 'min_child_weight': 7.147287208968302, 'subsample': 0.7510130193129223, 'colsample_bytree': 0.6905755985711316, 'colsample_bylevel': 0.6716946738224533, 'gamma': 0.23034856435825243, 'reg_alpha': 0.14880131180650286, 'reg_lambda': 0.9334478334096115}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:48:26,019]\u001b[0m Trial 941 finished with value: 0.3333333333333333 and parameters: {'booster': 'gblinear', 'learning_rate': 0.09610324833506133, 'n_estimators': 96, 'max_depth': 4, 'min_child_weight': 7.530668364703268, 'subsample': 0.7406509506459272, 'colsample_bytree': 0.6172833792536899, 'colsample_bylevel': 0.599121041185232, 'gamma': 0.10011993597657425, 'reg_alpha': 0.025142975129205936, 'reg_lambda': 0.9441836365511606}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:48:25] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:48:26] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:48:26,207]\u001b[0m Trial 942 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.08314639269567171, 'n_estimators': 90, 'max_depth': 10, 'min_child_weight': 2.0494308247339013, 'subsample': 0.5700482661748326, 'colsample_bytree': 0.7539941195345076, 'colsample_bylevel': 0.5134671660296035, 'gamma': 0.03969037392036893, 'reg_alpha': 0.285743495475883, 'reg_lambda': 0.04013678929735576}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:48:26,388]\u001b[0m Trial 943 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.02718071211482654, 'n_estimators': 84, 'max_depth': 9, 'min_child_weight': 5.6788626117135825, 'subsample': 0.5994960139712772, 'colsample_bytree': 0.8818688496150845, 'colsample_bylevel': 0.9017622530378967, 'gamma': 0.3869965544592336, 'reg_alpha': 0.2513736044921172, 'reg_lambda': 0.6565363336342135}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:48:26] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:48:26] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:48:26,568]\u001b[0m Trial 944 finished with value: 0.3333333333333333 and parameters: {'booster': 'gblinear', 'learning_rate': 0.029306188147779688, 'n_estimators': 84, 'max_depth': 9, 'min_child_weight': 8.000578254132016, 'subsample': 0.5827936424008915, 'colsample_bytree': 0.8826353497032489, 'colsample_bylevel': 0.783600607268867, 'gamma': 0.37782096450796626, 'reg_alpha': 0.5034406820495733, 'reg_lambda': 0.6810305698136321}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:48:26,757]\u001b[0m Trial 945 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.10899126372923927, 'n_estimators': 89, 'max_depth': 10, 'min_child_weight': 6.111566776392816, 'subsample': 0.5595773912255674, 'colsample_bytree': 0.5132394323742091, 'colsample_bylevel': 0.9244675797514155, 'gamma': 0.04955881030424361, 'reg_alpha': 0.2790209699804089, 'reg_lambda': 0.4481989198319595}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:48:26] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:48:26] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:48:26,934]\u001b[0m Trial 946 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.12241513241928509, 'n_estimators': 82, 'max_depth': 9, 'min_child_weight': 5.715278035865134, 'subsample': 0.5851676016911148, 'colsample_bytree': 0.7419843894332467, 'colsample_bylevel': 0.9046909958529673, 'gamma': 0.34579251330707544, 'reg_alpha': 0.19919175456227403, 'reg_lambda': 0.6730386633544796}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:48:27,113]\u001b[0m Trial 947 finished with value: 0.6249999999999999 and parameters: {'booster': 'gblinear', 'learning_rate': 0.11966134182058935, 'n_estimators': 82, 'max_depth': 9, 'min_child_weight': 5.860613149861149, 'subsample': 0.5826516361666788, 'colsample_bytree': 0.8317836949215827, 'colsample_bylevel': 0.785304549566205, 'gamma': 0.31947716077582744, 'reg_alpha': 0.15907775715406303, 'reg_lambda': 0.6594069090299152}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:48:26] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:48:27] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:48:27,299]\u001b[0m Trial 948 finished with value: 0.6249999999999999 and parameters: {'booster': 'gblinear', 'learning_rate': 0.07071253178365605, 'n_estimators': 87, 'max_depth': 8, 'min_child_weight': 3.525825280176871, 'subsample': 0.805050094869058, 'colsample_bytree': 0.5463835715277893, 'colsample_bylevel': 0.97454603141775, 'gamma': 0.48476114720708136, 'reg_alpha': 0.36720878582027827, 'reg_lambda': 0.3390169019659633}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:48:27,484]\u001b[0m Trial 949 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.1265096533231087, 'n_estimators': 86, 'max_depth': 5, 'min_child_weight': 3.9091216009643093, 'subsample': 0.8228441611650259, 'colsample_bytree': 0.5334276080109589, 'colsample_bylevel': 0.9899350186604216, 'gamma': 0.28484111631000225, 'reg_alpha': 0.3722411638608017, 'reg_lambda': 0.2297599157834547}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:48:27] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:48:27] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:48:27,663]\u001b[0m Trial 950 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.12178367605324367, 'n_estimators': 82, 'max_depth': 9, 'min_child_weight': 9.697704720916011, 'subsample': 0.611452785309912, 'colsample_bytree': 0.7346280104814328, 'colsample_bylevel': 0.9198824058197637, 'gamma': 0.30524817165881507, 'reg_alpha': 0.16994012225386695, 'reg_lambda': 0.8802260227306258}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:48:27,850]\u001b[0m Trial 951 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.10516296871822632, 'n_estimators': 90, 'max_depth': 10, 'min_child_weight': 6.412849156762503, 'subsample': 0.5693459905560092, 'colsample_bytree': 0.7584997847675345, 'colsample_bylevel': 0.9374323534269778, 'gamma': 0.20599456689289536, 'reg_alpha': 0.1699517769143499, 'reg_lambda': 0.838379591563212}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:48:27] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:48:27] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:48:28,039]\u001b[0m Trial 952 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.10627269615381922, 'n_estimators': 90, 'max_depth': 10, 'min_child_weight': 6.628977489883438, 'subsample': 0.5664007957041396, 'colsample_bytree': 0.7612859160896985, 'colsample_bylevel': 0.9369758388378274, 'gamma': 0.19632756365203272, 'reg_alpha': 0.18890016283419359, 'reg_lambda': 0.847034811970236}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:48:28,217]\u001b[0m Trial 953 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.19072839990242413, 'n_estimators': 82, 'max_depth': 9, 'min_child_weight': 5.724422810377512, 'subsample': 0.5983217560516919, 'colsample_bytree': 0.6144557860004216, 'colsample_bylevel': 0.9228228284543438, 'gamma': 0.03801686531542184, 'reg_alpha': 0.21501912906039972, 'reg_lambda': 0.9378604223830361}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:48:28] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:48:28] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:48:28,395]\u001b[0m Trial 954 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.02119991746289307, 'n_estimators': 82, 'max_depth': 10, 'min_child_weight': 5.654433275113995, 'subsample': 0.5857874512278435, 'colsample_bytree': 0.7124921279370261, 'colsample_bylevel': 0.884155629564312, 'gamma': 0.06709485918349886, 'reg_alpha': 0.2229788904143879, 'reg_lambda': 0.9111913604280998}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:48:28,581]\u001b[0m Trial 955 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.18993502658878444, 'n_estimators': 81, 'max_depth': 9, 'min_child_weight': 5.854647966458525, 'subsample': 0.5999569107266558, 'colsample_bytree': 0.7081606130904771, 'colsample_bylevel': 0.9196155735673872, 'gamma': 0.36895986398472347, 'reg_alpha': 0.20631822992075816, 'reg_lambda': 0.7059519123554064}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:48:28] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:48:28] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:48:28,760]\u001b[0m Trial 956 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.15112378875335175, 'n_estimators': 81, 'max_depth': 9, 'min_child_weight': 5.687974317581007, 'subsample': 0.587807655903887, 'colsample_bytree': 0.5163913126453156, 'colsample_bylevel': 0.9266472976196978, 'gamma': 0.06411968137338893, 'reg_alpha': 0.22134664740429766, 'reg_lambda': 0.884715542347843}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:48:28,936]\u001b[0m Trial 957 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.189679503559139, 'n_estimators': 81, 'max_depth': 3, 'min_child_weight': 5.979407979859612, 'subsample': 0.6027063352775182, 'colsample_bytree': 0.8095685287809007, 'colsample_bylevel': 0.8556469928787609, 'gamma': 0.3654219856863744, 'reg_alpha': 0.19036035639960552, 'reg_lambda': 0.8886785018433043}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:48:28] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:48:28] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:48:29,114]\u001b[0m Trial 958 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.15993286274580612, 'n_estimators': 82, 'max_depth': 3, 'min_child_weight': 5.842288160864761, 'subsample': 0.5003315598783413, 'colsample_bytree': 0.7022379134973646, 'colsample_bylevel': 0.729031920470584, 'gamma': 0.059249345835398814, 'reg_alpha': 0.2321611282839587, 'reg_lambda': 0.8914504886649586}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:48:29,321]\u001b[0m Trial 959 finished with value: 0.3333333333333333 and parameters: {'booster': 'dart', 'learning_rate': 0.08051054845696644, 'n_estimators': 86, 'max_depth': 8, 'min_child_weight': 6.255234865682911, 'subsample': 0.872167003979697, 'colsample_bytree': 0.5516630766099516, 'colsample_bylevel': 0.9996049692413247, 'gamma': 0.27113095527337416, 'reg_alpha': 0.26045025234721353, 'reg_lambda': 0.4044995588433598}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:48:29,498]\u001b[0m Trial 960 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.10329229846434154, 'n_estimators': 81, 'max_depth': 10, 'min_child_weight': 6.755006618541563, 'subsample': 0.6075417759388996, 'colsample_bytree': 0.6495828921029255, 'colsample_bylevel': 0.9431303408638746, 'gamma': 0.21600550846668473, 'reg_alpha': 0.23117279055792084, 'reg_lambda': 0.8305387877554979}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:48:29] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:48:29] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:48:29,677]\u001b[0m Trial 961 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.1632582451551284, 'n_estimators': 81, 'max_depth': 3, 'min_child_weight': 5.585223902539626, 'subsample': 0.6904855795450728, 'colsample_bytree': 0.5004525968921585, 'colsample_bylevel': 0.7359935604220516, 'gamma': 0.0613091262330932, 'reg_alpha': 0.17992871087983656, 'reg_lambda': 0.7357219765503141}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:48:29,853]\u001b[0m Trial 962 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.15830672749105135, 'n_estimators': 81, 'max_depth': 3, 'min_child_weight': 5.959180753395821, 'subsample': 0.6251431950680044, 'colsample_bytree': 0.500882440659079, 'colsample_bylevel': 0.6293885005934812, 'gamma': 0.054149623125271176, 'reg_alpha': 0.20252403106805292, 'reg_lambda': 0.7403054341993328}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:48:29] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:48:30,243]\u001b[0m Trial 963 finished with value: 0.3333333333333333 and parameters: {'booster': 'gbtree', 'learning_rate': 0.09365881702245929, 'n_estimators': 99, 'max_depth': 5, 'min_child_weight': 6.3113082663378615, 'subsample': 0.8297443780353154, 'colsample_bytree': 0.5123952742026834, 'colsample_bylevel': 0.6817033996721403, 'gamma': 0.2670503025868105, 'reg_alpha': 0.34199637924320786, 'reg_lambda': 0.37532975740334323}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:48:30,432]\u001b[0m Trial 964 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.03790947311965584, 'n_estimators': 89, 'max_depth': 10, 'min_child_weight': 6.833046650960697, 'subsample': 0.6165307668778165, 'colsample_bytree': 0.6367198973815736, 'colsample_bylevel': 0.9506001835991582, 'gamma': 0.22220590459574424, 'reg_alpha': 0.23248399038691542, 'reg_lambda': 0.195158725073765}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:48:30] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:48:30] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:48:30,621]\u001b[0m Trial 965 finished with value: 0.3333333333333333 and parameters: {'booster': 'gblinear', 'learning_rate': 0.06937005704539274, 'n_estimators': 88, 'max_depth': 4, 'min_child_weight': 4.078847901617485, 'subsample': 0.8669885154674282, 'colsample_bytree': 0.5897293460289202, 'colsample_bylevel': 0.7114372947155304, 'gamma': 0.08225551034334708, 'reg_alpha': 0.5729551468006564, 'reg_lambda': 0.1664020134097354}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:48:30,812]\u001b[0m Trial 966 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.025132937919712925, 'n_estimators': 89, 'max_depth': 10, 'min_child_weight': 6.817173346151135, 'subsample': 0.5806144502651632, 'colsample_bytree': 0.6286115075173407, 'colsample_bylevel': 0.9518877658595319, 'gamma': 0.23605625105315114, 'reg_alpha': 0.25376190495098055, 'reg_lambda': 0.8046705334296398}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:48:30] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:48:30] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:48:30,993]\u001b[0m Trial 967 finished with value: 0.8285714285714286 and parameters: {'booster': 'gblinear', 'learning_rate': 0.1808428372884547, 'n_estimators': 82, 'max_depth': 6, 'min_child_weight': 1.6037212701316474, 'subsample': 0.8913442407025931, 'colsample_bytree': 0.9998575029146914, 'colsample_bylevel': 0.5591416025173414, 'gamma': 0.08944159165348028, 'reg_alpha': 0.17456506039439676, 'reg_lambda': 0.7392730497055258}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:48:31,194]\u001b[0m Trial 968 finished with value: 0.6249999999999999 and parameters: {'booster': 'gblinear', 'learning_rate': 0.1711724465156725, 'n_estimators': 97, 'max_depth': 4, 'min_child_weight': 9.877480153030188, 'subsample': 0.8652881888766751, 'colsample_bytree': 0.6088874653262276, 'colsample_bylevel': 0.6981728614286814, 'gamma': 0.07509284605829386, 'reg_alpha': 0.41102669683221116, 'reg_lambda': 0.1481103677668993}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:48:31] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:48:31] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:48:31,396]\u001b[0m Trial 969 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.10092823908109239, 'n_estimators': 99, 'max_depth': 4, 'min_child_weight': 6.342943815060449, 'subsample': 0.680269760046207, 'colsample_bytree': 0.5305540976762328, 'colsample_bylevel': 0.9817852147498165, 'gamma': 0.47499054985444555, 'reg_alpha': 0.32673568148192117, 'reg_lambda': 0.5917783398827452}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:48:31,577]\u001b[0m Trial 970 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.06430562296808882, 'n_estimators': 83, 'max_depth': 3, 'min_child_weight': 5.850804400023568, 'subsample': 0.6882378316058216, 'colsample_bytree': 0.5156757562057742, 'colsample_bylevel': 0.5009003740404354, 'gamma': 0.09843151413159419, 'reg_alpha': 0.20946979760055243, 'reg_lambda': 0.7126973440683482}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:48:31] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:48:31] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:48:31,756]\u001b[0m Trial 971 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.11814856959793583, 'n_estimators': 83, 'max_depth': 3, 'min_child_weight': 5.683657938259872, 'subsample': 0.6953629983752198, 'colsample_bytree': 0.510572351384269, 'colsample_bylevel': 0.5645412278843923, 'gamma': 0.15417720459365866, 'reg_alpha': 0.1613938621272294, 'reg_lambda': 0.7626934263458861}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:48:31,934]\u001b[0m Trial 972 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.18581217659229757, 'n_estimators': 83, 'max_depth': 3, 'min_child_weight': 1.8509462800766778, 'subsample': 0.698434910770843, 'colsample_bytree': 0.5189505245437644, 'colsample_bylevel': 0.5379240580544543, 'gamma': 0.16758748045795563, 'reg_alpha': 0.1919839796674274, 'reg_lambda': 0.7771879324733982}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:48:31] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:48:31] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:48:32,113]\u001b[0m Trial 973 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.19824347342521445, 'n_estimators': 83, 'max_depth': 3, 'min_child_weight': 1.5449741401258623, 'subsample': 0.6898297528715589, 'colsample_bytree': 0.9959215952605538, 'colsample_bylevel': 0.5519745018228962, 'gamma': 0.16402709011728533, 'reg_alpha': 0.20627437791487996, 'reg_lambda': 0.7597716794359952}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:48:32,290]\u001b[0m Trial 974 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.06799355824978769, 'n_estimators': 82, 'max_depth': 3, 'min_child_weight': 1.7696694231168992, 'subsample': 0.5900241995222708, 'colsample_bytree': 0.6110347758355074, 'colsample_bylevel': 0.5468331298206348, 'gamma': 0.15965567881969747, 'reg_alpha': 0.21611791578035575, 'reg_lambda': 0.7868911558231642}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:48:32] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:48:32] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:48:32,487]\u001b[0m Trial 975 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.09050952023828682, 'n_estimators': 96, 'max_depth': 5, 'min_child_weight': 1.8806791611894078, 'subsample': 0.51402345458006, 'colsample_bytree': 0.820455949888927, 'colsample_bylevel': 0.5103830459724681, 'gamma': 0.01841161268526166, 'reg_alpha': 0.27806464420054255, 'reg_lambda': 0.8553280168957218}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:48:32,682]\u001b[0m Trial 976 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.09209757468811902, 'n_estimators': 95, 'max_depth': 5, 'min_child_weight': 6.125169397564982, 'subsample': 0.5229823644457124, 'colsample_bytree': 0.8147503179389396, 'colsample_bylevel': 0.5119416576179998, 'gamma': 0.0008044073008825739, 'reg_alpha': 0.2746287301492612, 'reg_lambda': 0.8624153794709906}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:48:32] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:48:32] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:48:32,883]\u001b[0m Trial 977 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.09682824386349408, 'n_estimators': 99, 'max_depth': 4, 'min_child_weight': 4.60093252027325, 'subsample': 0.7576004466727544, 'colsample_bytree': 0.6596149024971544, 'colsample_bylevel': 0.7975267665852336, 'gamma': 0.10889727522177774, 'reg_alpha': 0.28514488152663753, 'reg_lambda': 0.9525278475525035}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:48:33,085]\u001b[0m Trial 978 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.09669437289166904, 'n_estimators': 100, 'max_depth': 4, 'min_child_weight': 7.351867354054155, 'subsample': 0.7706589112415674, 'colsample_bytree': 0.6505096047491395, 'colsample_bylevel': 0.7917530302249585, 'gamma': 0.10044822744000345, 'reg_alpha': 0.28817806103456906, 'reg_lambda': 0.9547752026032484}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:48:32] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:48:33,287]\u001b[0m Trial 979 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.0972534108551092, 'n_estimators': 100, 'max_depth': 4, 'min_child_weight': 7.360028223433169, 'subsample': 0.7797642075130983, 'colsample_bytree': 0.6648879208205489, 'colsample_bylevel': 0.7926398567375132, 'gamma': 0.09873935152461871, 'reg_alpha': 0.2941283312490683, 'reg_lambda': 0.9586177331936784}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:48:33] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:48:33,488]\u001b[0m Trial 980 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.17633498990785157, 'n_estimators': 100, 'max_depth': 4, 'min_child_weight': 7.278536800226298, 'subsample': 0.8376749454716765, 'colsample_bytree': 0.6682688868127051, 'colsample_bylevel': 0.7840362968896081, 'gamma': 0.10373396394001284, 'reg_alpha': 0.308497865550679, 'reg_lambda': 0.9553565595282284}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:48:33] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:48:33,692]\u001b[0m Trial 981 finished with value: 0.8285714285714286 and parameters: {'booster': 'dart', 'learning_rate': 0.15718504877521622, 'n_estimators': 82, 'max_depth': 6, 'min_child_weight': 1.4555706614644501, 'subsample': 0.5401411093049434, 'colsample_bytree': 0.9897574142718416, 'colsample_bylevel': 0.5385643597311784, 'gamma': 0.36886603921984573, 'reg_alpha': 0.21527398403644926, 'reg_lambda': 0.7449747853799976}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:48:33,905]\u001b[0m Trial 982 finished with value: 0.3333333333333333 and parameters: {'booster': 'gblinear', 'learning_rate': 0.1719194208843952, 'n_estimators': 100, 'max_depth': 4, 'min_child_weight': 7.172272507401423, 'subsample': 0.7520470828293597, 'colsample_bytree': 0.6806228968185839, 'colsample_bylevel': 0.7812771858832259, 'gamma': 0.0873919428554023, 'reg_alpha': 0.38683547702518517, 'reg_lambda': 0.9588081167558915}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:48:33] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:48:34,088]\u001b[0m Trial 983 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.11601108735565391, 'n_estimators': 80, 'max_depth': 10, 'min_child_weight': 3.5417896792146237, 'subsample': 0.6107382198208364, 'colsample_bytree': 0.7219252342292143, 'colsample_bylevel': 0.969466885318309, 'gamma': 0.3617100880009006, 'reg_alpha': 0.3101143702394885, 'reg_lambda': 0.8169694693605278}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:48:33] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:48:34,270]\u001b[0m Trial 984 finished with value: 0.3333333333333333 and parameters: {'booster': 'gbtree', 'learning_rate': 0.07406206685842222, 'n_estimators': 82, 'max_depth': 6, 'min_child_weight': 1.4292547551123813, 'subsample': 0.5422933329598616, 'colsample_bytree': 0.9906384249283019, 'colsample_bylevel': 0.7450108398796145, 'gamma': 0.35899956335119027, 'reg_alpha': 0.2281797654350145, 'reg_lambda': 0.758492147801034}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:48:34,448]\u001b[0m Trial 985 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.10387794340697759, 'n_estimators': 80, 'max_depth': 10, 'min_child_weight': 7.800369422810993, 'subsample': 0.6035852939614875, 'colsample_bytree': 0.5601688469102413, 'colsample_bylevel': 0.9768078181561505, 'gamma': 0.3785713696968403, 'reg_alpha': 0.3041522437986787, 'reg_lambda': 0.5593568122921716}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:48:34] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:48:34] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:48:34,627]\u001b[0m Trial 986 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.10497769060728306, 'n_estimators': 80, 'max_depth': 10, 'min_child_weight': 7.846158232216259, 'subsample': 0.6069605708536535, 'colsample_bytree': 0.5515134057496514, 'colsample_bylevel': 0.9733735237390867, 'gamma': 0.3773241967623055, 'reg_alpha': 0.30458886585223727, 'reg_lambda': 0.5546481613608814}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:48:34,807]\u001b[0m Trial 987 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.08314564184419723, 'n_estimators': 81, 'max_depth': 3, 'min_child_weight': 1.3222943145121988, 'subsample': 0.7047576598298864, 'colsample_bytree': 0.9923238651102007, 'colsample_bylevel': 0.7334816659329287, 'gamma': 0.16316545164749943, 'reg_alpha': 0.21265665068405357, 'reg_lambda': 0.780724803698997}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:48:34] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:48:34] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:48:34,991]\u001b[0m Trial 988 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.07668390375102037, 'n_estimators': 81, 'max_depth': 3, 'min_child_weight': 1.7466118103904473, 'subsample': 0.6851919501400471, 'colsample_bytree': 0.9882177620623838, 'colsample_bylevel': 0.7460341384115012, 'gamma': 0.36528514440244875, 'reg_alpha': 0.21582046397411947, 'reg_lambda': 0.7798706077600579}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:48:35,170]\u001b[0m Trial 989 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.0777011907794684, 'n_estimators': 80, 'max_depth': 10, 'min_child_weight': 7.779157872996902, 'subsample': 0.6252221934340794, 'colsample_bytree': 0.7116179868547865, 'colsample_bylevel': 0.7520492672337908, 'gamma': 0.36157794452894726, 'reg_alpha': 0.2649613099354438, 'reg_lambda': 0.792427673281186}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:48:35] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:48:35] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:48:35,346]\u001b[0m Trial 990 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.08109410072903751, 'n_estimators': 80, 'max_depth': 10, 'min_child_weight': 7.81852518666237, 'subsample': 0.6293470949221763, 'colsample_bytree': 0.7144888855584629, 'colsample_bylevel': 0.7584002219982682, 'gamma': 0.3459529606913672, 'reg_alpha': 0.2665238849143834, 'reg_lambda': 0.7987712394466934}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:48:35,532]\u001b[0m Trial 991 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.16040629865274428, 'n_estimators': 89, 'max_depth': 6, 'min_child_weight': 3.1393662503673236, 'subsample': 0.7147966036210931, 'colsample_bytree': 0.97966260507281, 'colsample_bylevel': 0.7282857866257197, 'gamma': 0.1585393796030313, 'reg_alpha': 0.20186623799943273, 'reg_lambda': 0.760149883643237}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:48:35] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:48:35] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:48:35,720]\u001b[0m Trial 992 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.152722330673565, 'n_estimators': 89, 'max_depth': 6, 'min_child_weight': 3.3119125578577493, 'subsample': 0.55205678593708, 'colsample_bytree': 0.9762489247087488, 'colsample_bylevel': 0.7349561862538829, 'gamma': 0.3204481910914873, 'reg_alpha': 0.22873834671241594, 'reg_lambda': 0.7728731857990369}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:48:35,908]\u001b[0m Trial 993 finished with value: 0.3333333333333333 and parameters: {'booster': 'gblinear', 'learning_rate': 0.15484881324259667, 'n_estimators': 90, 'max_depth': 3, 'min_child_weight': 3.147397100913141, 'subsample': 0.6956596913916475, 'colsample_bytree': 0.9996052658760646, 'colsample_bylevel': 0.5270626625831476, 'gamma': 0.39143286582330433, 'reg_alpha': 0.7248732196880152, 'reg_lambda': 0.7326184614525048}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:48:35] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:48:35] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:48:36,087]\u001b[0m Trial 994 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.11628058493023863, 'n_estimators': 80, 'max_depth': 10, 'min_child_weight': 5.214693222714293, 'subsample': 0.6333698027763119, 'colsample_bytree': 0.9837458414203957, 'colsample_bylevel': 0.7513522220724865, 'gamma': 0.34772995234345916, 'reg_alpha': 0.3223530842650183, 'reg_lambda': 0.7218085747812147}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:48:36,275]\u001b[0m Trial 995 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.07872470744500833, 'n_estimators': 90, 'max_depth': 6, 'min_child_weight': 3.135674628244571, 'subsample': 0.6773119467531706, 'colsample_bytree': 0.9985266076535328, 'colsample_bylevel': 0.9549919636743276, 'gamma': 0.15112000285285523, 'reg_alpha': 0.24038155266095929, 'reg_lambda': 0.43089018405301677}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:48:36] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:48:36] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:48:36,465]\u001b[0m Trial 996 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.1505516131531731, 'n_estimators': 90, 'max_depth': 6, 'min_child_weight': 1.0371736172021897, 'subsample': 0.685567736153194, 'colsample_bytree': 0.9768193843126566, 'colsample_bylevel': 0.9703139916355144, 'gamma': 0.15074820433329142, 'reg_alpha': 0.2417956836878569, 'reg_lambda': 0.4528812627665648}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:48:36,644]\u001b[0m Trial 997 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.15818268952120332, 'n_estimators': 83, 'max_depth': 6, 'min_child_weight': 3.232634925385911, 'subsample': 0.67281590575493, 'colsample_bytree': 0.9990510111707874, 'colsample_bylevel': 0.9299134234285066, 'gamma': 0.13858872225630703, 'reg_alpha': 0.24282273917471714, 'reg_lambda': 0.4250624506915582}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:48:36] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:48:36] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-12 13:48:36,824]\u001b[0m Trial 998 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.15982323061947779, 'n_estimators': 82, 'max_depth': 6, 'min_child_weight': 2.9855788911149146, 'subsample': 0.6771140950807136, 'colsample_bytree': 0.9791458207080889, 'colsample_bylevel': 0.9420297662294351, 'gamma': 0.14419925068311562, 'reg_alpha': 0.23722018785452859, 'reg_lambda': 0.42889123455708855}. Best is trial 15 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2023-08-12 13:48:37,015]\u001b[0m Trial 999 finished with value: 1.0 and parameters: {'booster': 'gblinear', 'learning_rate': 0.15547844260536942, 'n_estimators': 91, 'max_depth': 6, 'min_child_weight': 2.786852711609016, 'subsample': 0.5754774232960655, 'colsample_bytree': 0.9774119286841424, 'colsample_bylevel': 0.9332468399107907, 'gamma': 0.058160596794077056, 'reg_alpha': 0.2512502543435816, 'reg_lambda': 0.42158543328939135}. Best is trial 15 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:48:36] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "def objective(trial):\n",
    "    param = {\n",
    "        'booster': trial.suggest_categorical('booster', ['gbtree', 'gblinear', 'dart']),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.2),\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 80, 100),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 10),\n",
    "        'min_child_weight': trial.suggest_float('min_child_weight', 1, 10),\n",
    "        'subsample': trial.suggest_float('subsample', 0.5, 1),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1),\n",
    "        'colsample_bylevel': trial.suggest_float('colsample_bylevel', 0.5, 1),\n",
    "        'gamma': trial.suggest_float('gamma', 0, 0.5),\n",
    "        'reg_alpha': trial.suggest_float('reg_alpha', 0, 1),\n",
    "        'reg_lambda': trial.suggest_float('reg_lambda', 0, 1),\n",
    "    }\n",
    "    model = xgb.XGBClassifier(random_state=SEED, n_jobs=n_jobs, **param)\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_discovery, y_discovery, test_size=0.2, random_state=SEED)\n",
    "    \n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "    \n",
    "    return f1\n",
    "    \n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "c2e4c884-380b-486e-81c9-70ffc0a88de6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'booster': 'gblinear',\n",
       " 'learning_rate': 0.0715559596102712,\n",
       " 'n_estimators': 94,\n",
       " 'max_depth': 5,\n",
       " 'min_child_weight': 4.891009166379371,\n",
       " 'subsample': 0.8750588706212519,\n",
       " 'colsample_bytree': 0.7386371120688078,\n",
       " 'colsample_bylevel': 0.7492967578298728,\n",
       " 'gamma': 0.12511841166144674,\n",
       " 'reg_alpha': 0.24493238214913976,\n",
       " 'reg_lambda': 0.290664497662243}"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "study.best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "1d2e8c0b-0d2a-4c65-a90a-fce8457926c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = xgb.XGBClassifier(random_state=SEED, n_jobs=n_jobs, **study.best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "f3f12587-86c6-41f8-a3b5-369999bc7f94",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_39636/990433516.py\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_discovery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_discovery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstratify\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my_discovery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.15\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mSEED\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_set\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mearly_stopping_rounds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstudy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_params\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'n_estimators'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/venvs/default/lib/python3.9/site-packages/xgboost/core.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venvs/default/lib/python3.9/site-packages/xgboost/sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, base_margin, eval_set, eval_metric, early_stopping_rounds, verbose, xgb_model, sample_weight_eval_set, base_margin_eval_set, feature_weights, callbacks)\u001b[0m\n\u001b[1;32m   1380\u001b[0m             \u001b[0mxgb_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_metric\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mearly_stopping_rounds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1381\u001b[0m         )\n\u001b[0;32m-> 1382\u001b[0;31m         train_dmatrix, evals = _wrap_evaluation_matrices(\n\u001b[0m\u001b[1;32m   1383\u001b[0m             \u001b[0mmissing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmissing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1384\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venvs/default/lib/python3.9/site-packages/xgboost/sklearn.py\u001b[0m in \u001b[0;36m_wrap_evaluation_matrices\u001b[0;34m(missing, X, y, group, qid, sample_weight, base_margin, feature_weights, eval_set, sample_weight_eval_set, base_margin_eval_set, eval_group, eval_qid, create_dmatrix, enable_categorical)\u001b[0m\n\u001b[1;32m    434\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    435\u001b[0m         \u001b[0mevals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 436\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mvalid_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_y\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meval_set\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    437\u001b[0m             \u001b[0;31m# Skip the duplicated entry.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    438\u001b[0m             if all(\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_discovery, y_discovery, stratify=y_discovery, test_size=0.15, random_state=SEED)\n",
    "model.fit(X_train, y_train, eval_set=(X_test,y_test),early_stopping_rounds=round(study.best_params['n_estimators']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "090ae024-2266-4013-b35c-6a447924d9ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x453.6 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "explainer = shap.TreeExplainer(model)\n",
    "shap_values = explainer.shap_values(X)\n",
    "sns.set_style('white')\n",
    "shap.summary_plot(shap_values[1], X,max_display=20, plot_type='dot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "2195b88c-8592-406a-a4bc-aee3c131cc55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CD4+ NV maturity and HLA-DR+ expression</th>\n",
       "      <th>CD4+ NV maturity and PD-1+TIGIT+ expression</th>\n",
       "      <th>CD4+ NV maturity and PD1+ expression</th>\n",
       "      <th>CD4+ None maturity and None expression</th>\n",
       "      <th>CD4+ CM maturity and CD226+ expression</th>\n",
       "      <th>CD8+ EM maturity and None expression</th>\n",
       "      <th>CD4+ CM maturity and PD-1+TIGIT- expression</th>\n",
       "      <th>CD4+ NV maturity and PD-1+TIGIT- expression</th>\n",
       "      <th>CD4+ CM maturity and None expression</th>\n",
       "      <th>CD4+ NV maturity and TIGIT+ expression</th>\n",
       "      <th>Th22 CM maturity and None expression</th>\n",
       "      <th>CD8+ EMTM maturity and PD-1+TIGIT+ expression</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>45_90</th>\n",
       "      <td>3.253369</td>\n",
       "      <td>0.816087</td>\n",
       "      <td>1.077758</td>\n",
       "      <td>8.639425</td>\n",
       "      <td>6.978063</td>\n",
       "      <td>8.075357</td>\n",
       "      <td>4.457562</td>\n",
       "      <td>0.433097</td>\n",
       "      <td>6.918507</td>\n",
       "      <td>2.663674</td>\n",
       "      <td>0.750708</td>\n",
       "      <td>6.234018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20_90</th>\n",
       "      <td>1.279961</td>\n",
       "      <td>0.566767</td>\n",
       "      <td>1.166722</td>\n",
       "      <td>6.419449</td>\n",
       "      <td>5.435524</td>\n",
       "      <td>7.970050</td>\n",
       "      <td>3.585743</td>\n",
       "      <td>0.818695</td>\n",
       "      <td>5.576912</td>\n",
       "      <td>1.289008</td>\n",
       "      <td>1.907014</td>\n",
       "      <td>7.343317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1_90</th>\n",
       "      <td>3.410649</td>\n",
       "      <td>1.942530</td>\n",
       "      <td>2.943277</td>\n",
       "      <td>6.595505</td>\n",
       "      <td>5.557021</td>\n",
       "      <td>0.544861</td>\n",
       "      <td>3.495338</td>\n",
       "      <td>2.277322</td>\n",
       "      <td>5.590927</td>\n",
       "      <td>2.261040</td>\n",
       "      <td>0.395206</td>\n",
       "      <td>1.590633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12_90</th>\n",
       "      <td>0.855045</td>\n",
       "      <td>0.532925</td>\n",
       "      <td>1.103282</td>\n",
       "      <td>3.902334</td>\n",
       "      <td>3.515843</td>\n",
       "      <td>3.625189</td>\n",
       "      <td>2.269227</td>\n",
       "      <td>0.766866</td>\n",
       "      <td>3.955223</td>\n",
       "      <td>0.716752</td>\n",
       "      <td>0.125539</td>\n",
       "      <td>1.209929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21_90</th>\n",
       "      <td>1.123628</td>\n",
       "      <td>0.836701</td>\n",
       "      <td>1.085039</td>\n",
       "      <td>4.676005</td>\n",
       "      <td>4.109025</td>\n",
       "      <td>4.904480</td>\n",
       "      <td>2.519778</td>\n",
       "      <td>0.417349</td>\n",
       "      <td>4.629348</td>\n",
       "      <td>1.018338</td>\n",
       "      <td>0.765531</td>\n",
       "      <td>3.941586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28_90</th>\n",
       "      <td>1.250624</td>\n",
       "      <td>0.256150</td>\n",
       "      <td>0.687407</td>\n",
       "      <td>5.252894</td>\n",
       "      <td>4.410355</td>\n",
       "      <td>4.247289</td>\n",
       "      <td>2.622920</td>\n",
       "      <td>0.501923</td>\n",
       "      <td>4.694759</td>\n",
       "      <td>0.542581</td>\n",
       "      <td>0.842808</td>\n",
       "      <td>3.379009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25_90</th>\n",
       "      <td>0.828065</td>\n",
       "      <td>0.516745</td>\n",
       "      <td>0.731568</td>\n",
       "      <td>7.044523</td>\n",
       "      <td>5.439212</td>\n",
       "      <td>3.674965</td>\n",
       "      <td>3.225216</td>\n",
       "      <td>0.298329</td>\n",
       "      <td>5.785610</td>\n",
       "      <td>0.718071</td>\n",
       "      <td>1.444452</td>\n",
       "      <td>2.959369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69_90</th>\n",
       "      <td>0.502712</td>\n",
       "      <td>0.461841</td>\n",
       "      <td>0.764846</td>\n",
       "      <td>5.279298</td>\n",
       "      <td>4.812743</td>\n",
       "      <td>1.414310</td>\n",
       "      <td>2.647623</td>\n",
       "      <td>0.402603</td>\n",
       "      <td>4.878402</td>\n",
       "      <td>0.998075</td>\n",
       "      <td>0.400476</td>\n",
       "      <td>1.141858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54_90</th>\n",
       "      <td>1.777890</td>\n",
       "      <td>1.035724</td>\n",
       "      <td>1.943648</td>\n",
       "      <td>4.460682</td>\n",
       "      <td>5.023118</td>\n",
       "      <td>7.156520</td>\n",
       "      <td>3.826163</td>\n",
       "      <td>1.483688</td>\n",
       "      <td>5.191335</td>\n",
       "      <td>1.211536</td>\n",
       "      <td>0.793901</td>\n",
       "      <td>6.232746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57_90</th>\n",
       "      <td>0.475158</td>\n",
       "      <td>0.126830</td>\n",
       "      <td>0.479045</td>\n",
       "      <td>5.487521</td>\n",
       "      <td>4.172622</td>\n",
       "      <td>2.536713</td>\n",
       "      <td>2.241854</td>\n",
       "      <td>0.380651</td>\n",
       "      <td>4.351655</td>\n",
       "      <td>0.569333</td>\n",
       "      <td>0.534019</td>\n",
       "      <td>1.744961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29_90</th>\n",
       "      <td>1.931924</td>\n",
       "      <td>1.146924</td>\n",
       "      <td>1.766314</td>\n",
       "      <td>4.992294</td>\n",
       "      <td>4.643359</td>\n",
       "      <td>10.083559</td>\n",
       "      <td>3.621469</td>\n",
       "      <td>1.129234</td>\n",
       "      <td>5.491072</td>\n",
       "      <td>1.379191</td>\n",
       "      <td>1.245811</td>\n",
       "      <td>8.642603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14_90</th>\n",
       "      <td>0.610545</td>\n",
       "      <td>0.209266</td>\n",
       "      <td>0.391987</td>\n",
       "      <td>5.907475</td>\n",
       "      <td>4.384943</td>\n",
       "      <td>4.204570</td>\n",
       "      <td>1.736275</td>\n",
       "      <td>0.209266</td>\n",
       "      <td>4.854820</td>\n",
       "      <td>0.699905</td>\n",
       "      <td>0.099416</td>\n",
       "      <td>3.455362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40_90</th>\n",
       "      <td>1.494335</td>\n",
       "      <td>0.544415</td>\n",
       "      <td>1.980772</td>\n",
       "      <td>2.724157</td>\n",
       "      <td>4.932435</td>\n",
       "      <td>7.653666</td>\n",
       "      <td>4.305913</td>\n",
       "      <td>1.802654</td>\n",
       "      <td>5.292847</td>\n",
       "      <td>0.592202</td>\n",
       "      <td>1.452001</td>\n",
       "      <td>6.433929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44_90</th>\n",
       "      <td>2.153241</td>\n",
       "      <td>0.799895</td>\n",
       "      <td>2.546066</td>\n",
       "      <td>6.881467</td>\n",
       "      <td>5.130796</td>\n",
       "      <td>1.218489</td>\n",
       "      <td>3.841404</td>\n",
       "      <td>2.350333</td>\n",
       "      <td>5.206412</td>\n",
       "      <td>1.784192</td>\n",
       "      <td>0.150192</td>\n",
       "      <td>2.574576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36_90</th>\n",
       "      <td>0.891971</td>\n",
       "      <td>0.359883</td>\n",
       "      <td>1.033612</td>\n",
       "      <td>5.176522</td>\n",
       "      <td>1.516387</td>\n",
       "      <td>1.031684</td>\n",
       "      <td>0.387593</td>\n",
       "      <td>0.818704</td>\n",
       "      <td>2.754781</td>\n",
       "      <td>0.824954</td>\n",
       "      <td>0.117166</td>\n",
       "      <td>0.630171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56_90</th>\n",
       "      <td>0.318195</td>\n",
       "      <td>0.073068</td>\n",
       "      <td>0.461161</td>\n",
       "      <td>5.757045</td>\n",
       "      <td>3.322016</td>\n",
       "      <td>0.495635</td>\n",
       "      <td>1.804745</td>\n",
       "      <td>0.405663</td>\n",
       "      <td>3.844096</td>\n",
       "      <td>0.256791</td>\n",
       "      <td>0.284585</td>\n",
       "      <td>0.504575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3_90</th>\n",
       "      <td>0.710730</td>\n",
       "      <td>0.542227</td>\n",
       "      <td>1.540968</td>\n",
       "      <td>6.365023</td>\n",
       "      <td>4.042038</td>\n",
       "      <td>0.921524</td>\n",
       "      <td>2.852853</td>\n",
       "      <td>1.294946</td>\n",
       "      <td>4.653615</td>\n",
       "      <td>0.974001</td>\n",
       "      <td>0.064305</td>\n",
       "      <td>0.981566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13_90</th>\n",
       "      <td>0.117152</td>\n",
       "      <td>0.053245</td>\n",
       "      <td>0.190282</td>\n",
       "      <td>3.585809</td>\n",
       "      <td>4.233529</td>\n",
       "      <td>6.737907</td>\n",
       "      <td>3.559514</td>\n",
       "      <td>0.141943</td>\n",
       "      <td>4.666428</td>\n",
       "      <td>0.053245</td>\n",
       "      <td>0.217804</td>\n",
       "      <td>5.608378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62_90</th>\n",
       "      <td>2.333862</td>\n",
       "      <td>0.175346</td>\n",
       "      <td>1.459718</td>\n",
       "      <td>1.913150</td>\n",
       "      <td>1.655352</td>\n",
       "      <td>1.954355</td>\n",
       "      <td>1.258346</td>\n",
       "      <td>1.390289</td>\n",
       "      <td>2.667638</td>\n",
       "      <td>0.391041</td>\n",
       "      <td>0.019302</td>\n",
       "      <td>1.727371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6_90</th>\n",
       "      <td>0.690855</td>\n",
       "      <td>0.202001</td>\n",
       "      <td>0.316952</td>\n",
       "      <td>6.151546</td>\n",
       "      <td>3.008879</td>\n",
       "      <td>4.567745</td>\n",
       "      <td>0.560130</td>\n",
       "      <td>0.131462</td>\n",
       "      <td>4.069697</td>\n",
       "      <td>0.925349</td>\n",
       "      <td>0.103577</td>\n",
       "      <td>3.366667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52_90</th>\n",
       "      <td>2.228695</td>\n",
       "      <td>0.698657</td>\n",
       "      <td>2.629326</td>\n",
       "      <td>4.226325</td>\n",
       "      <td>4.775817</td>\n",
       "      <td>7.504838</td>\n",
       "      <td>3.924953</td>\n",
       "      <td>2.476220</td>\n",
       "      <td>4.510494</td>\n",
       "      <td>0.908640</td>\n",
       "      <td>0.109107</td>\n",
       "      <td>7.417362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24_90</th>\n",
       "      <td>1.424661</td>\n",
       "      <td>1.034915</td>\n",
       "      <td>1.500800</td>\n",
       "      <td>7.839187</td>\n",
       "      <td>4.428150</td>\n",
       "      <td>8.641090</td>\n",
       "      <td>3.036526</td>\n",
       "      <td>0.832689</td>\n",
       "      <td>4.820381</td>\n",
       "      <td>1.207473</td>\n",
       "      <td>0.152848</td>\n",
       "      <td>6.822212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8_90</th>\n",
       "      <td>2.013756</td>\n",
       "      <td>0.852270</td>\n",
       "      <td>1.500963</td>\n",
       "      <td>6.362247</td>\n",
       "      <td>5.986539</td>\n",
       "      <td>4.884773</td>\n",
       "      <td>4.039471</td>\n",
       "      <td>1.017905</td>\n",
       "      <td>6.485300</td>\n",
       "      <td>1.161230</td>\n",
       "      <td>1.064408</td>\n",
       "      <td>4.300965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32_90</th>\n",
       "      <td>1.155353</td>\n",
       "      <td>0.423246</td>\n",
       "      <td>1.924373</td>\n",
       "      <td>5.868175</td>\n",
       "      <td>6.018745</td>\n",
       "      <td>6.516363</td>\n",
       "      <td>4.667646</td>\n",
       "      <td>1.788593</td>\n",
       "      <td>6.015756</td>\n",
       "      <td>0.554632</td>\n",
       "      <td>1.099598</td>\n",
       "      <td>5.312683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48_90</th>\n",
       "      <td>0.759384</td>\n",
       "      <td>0.468194</td>\n",
       "      <td>1.510771</td>\n",
       "      <td>4.659357</td>\n",
       "      <td>5.093613</td>\n",
       "      <td>6.438854</td>\n",
       "      <td>3.994419</td>\n",
       "      <td>1.302317</td>\n",
       "      <td>5.000288</td>\n",
       "      <td>0.859043</td>\n",
       "      <td>0.829312</td>\n",
       "      <td>4.703549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16_90</th>\n",
       "      <td>0.782981</td>\n",
       "      <td>0.517229</td>\n",
       "      <td>1.623932</td>\n",
       "      <td>6.269076</td>\n",
       "      <td>4.469730</td>\n",
       "      <td>3.067922</td>\n",
       "      <td>3.609431</td>\n",
       "      <td>1.406502</td>\n",
       "      <td>3.946836</td>\n",
       "      <td>0.589841</td>\n",
       "      <td>0.124920</td>\n",
       "      <td>2.918688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43_90</th>\n",
       "      <td>4.192239</td>\n",
       "      <td>1.104379</td>\n",
       "      <td>1.640389</td>\n",
       "      <td>6.411233</td>\n",
       "      <td>5.243027</td>\n",
       "      <td>4.341182</td>\n",
       "      <td>3.393215</td>\n",
       "      <td>0.976316</td>\n",
       "      <td>5.309750</td>\n",
       "      <td>1.589241</td>\n",
       "      <td>2.102640</td>\n",
       "      <td>2.001241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33_90</th>\n",
       "      <td>0.585137</td>\n",
       "      <td>0.242442</td>\n",
       "      <td>0.745948</td>\n",
       "      <td>6.113829</td>\n",
       "      <td>5.299384</td>\n",
       "      <td>6.072987</td>\n",
       "      <td>3.831525</td>\n",
       "      <td>0.579259</td>\n",
       "      <td>5.652498</td>\n",
       "      <td>0.342999</td>\n",
       "      <td>0.554139</td>\n",
       "      <td>4.369445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51_90</th>\n",
       "      <td>2.078475</td>\n",
       "      <td>0.207173</td>\n",
       "      <td>1.072233</td>\n",
       "      <td>6.105971</td>\n",
       "      <td>5.216498</td>\n",
       "      <td>2.842161</td>\n",
       "      <td>4.180805</td>\n",
       "      <td>0.962187</td>\n",
       "      <td>5.614786</td>\n",
       "      <td>0.353302</td>\n",
       "      <td>1.488106</td>\n",
       "      <td>1.691766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35_90</th>\n",
       "      <td>3.477255</td>\n",
       "      <td>0.649367</td>\n",
       "      <td>1.125306</td>\n",
       "      <td>5.111683</td>\n",
       "      <td>5.283467</td>\n",
       "      <td>6.070404</td>\n",
       "      <td>2.992539</td>\n",
       "      <td>0.689746</td>\n",
       "      <td>5.661069</td>\n",
       "      <td>1.363784</td>\n",
       "      <td>1.677669</td>\n",
       "      <td>4.869467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23_90</th>\n",
       "      <td>0.203423</td>\n",
       "      <td>0.044007</td>\n",
       "      <td>0.315390</td>\n",
       "      <td>6.047823</td>\n",
       "      <td>3.425778</td>\n",
       "      <td>0.858494</td>\n",
       "      <td>1.668337</td>\n",
       "      <td>0.279024</td>\n",
       "      <td>3.124903</td>\n",
       "      <td>0.207728</td>\n",
       "      <td>0.122302</td>\n",
       "      <td>1.661396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5_90</th>\n",
       "      <td>0.910888</td>\n",
       "      <td>0.332144</td>\n",
       "      <td>1.031188</td>\n",
       "      <td>6.374137</td>\n",
       "      <td>5.722422</td>\n",
       "      <td>2.024931</td>\n",
       "      <td>3.834479</td>\n",
       "      <td>0.835782</td>\n",
       "      <td>5.877363</td>\n",
       "      <td>0.902499</td>\n",
       "      <td>0.696906</td>\n",
       "      <td>3.154634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4_90</th>\n",
       "      <td>0.602707</td>\n",
       "      <td>0.159641</td>\n",
       "      <td>0.500649</td>\n",
       "      <td>7.067608</td>\n",
       "      <td>5.956769</td>\n",
       "      <td>4.514472</td>\n",
       "      <td>3.229911</td>\n",
       "      <td>0.376114</td>\n",
       "      <td>6.351472</td>\n",
       "      <td>0.459391</td>\n",
       "      <td>1.882990</td>\n",
       "      <td>3.632937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55_90</th>\n",
       "      <td>2.320034</td>\n",
       "      <td>1.412087</td>\n",
       "      <td>2.124832</td>\n",
       "      <td>6.070148</td>\n",
       "      <td>5.773805</td>\n",
       "      <td>6.692558</td>\n",
       "      <td>4.639699</td>\n",
       "      <td>1.433123</td>\n",
       "      <td>5.964837</td>\n",
       "      <td>1.559668</td>\n",
       "      <td>0.263900</td>\n",
       "      <td>3.842778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58_90</th>\n",
       "      <td>1.286814</td>\n",
       "      <td>0.962520</td>\n",
       "      <td>1.590628</td>\n",
       "      <td>7.166425</td>\n",
       "      <td>5.108281</td>\n",
       "      <td>3.532285</td>\n",
       "      <td>3.037890</td>\n",
       "      <td>1.044809</td>\n",
       "      <td>5.533309</td>\n",
       "      <td>1.905044</td>\n",
       "      <td>0.341947</td>\n",
       "      <td>2.439343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53_90</th>\n",
       "      <td>1.421329</td>\n",
       "      <td>0.236733</td>\n",
       "      <td>1.753018</td>\n",
       "      <td>3.954196</td>\n",
       "      <td>3.980639</td>\n",
       "      <td>5.552259</td>\n",
       "      <td>3.019416</td>\n",
       "      <td>1.674600</td>\n",
       "      <td>3.076036</td>\n",
       "      <td>0.584963</td>\n",
       "      <td>0.035591</td>\n",
       "      <td>5.110121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67_90</th>\n",
       "      <td>0.425933</td>\n",
       "      <td>0.282757</td>\n",
       "      <td>0.503961</td>\n",
       "      <td>5.538973</td>\n",
       "      <td>3.900312</td>\n",
       "      <td>4.872421</td>\n",
       "      <td>2.590887</td>\n",
       "      <td>0.264939</td>\n",
       "      <td>4.257046</td>\n",
       "      <td>0.511537</td>\n",
       "      <td>0.793829</td>\n",
       "      <td>2.531616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37_90</th>\n",
       "      <td>0.826858</td>\n",
       "      <td>0.250724</td>\n",
       "      <td>0.616275</td>\n",
       "      <td>5.834799</td>\n",
       "      <td>5.203637</td>\n",
       "      <td>4.685435</td>\n",
       "      <td>3.082033</td>\n",
       "      <td>0.425575</td>\n",
       "      <td>5.016784</td>\n",
       "      <td>0.595515</td>\n",
       "      <td>0.596998</td>\n",
       "      <td>3.670380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19_90</th>\n",
       "      <td>0.639659</td>\n",
       "      <td>0.074879</td>\n",
       "      <td>0.608161</td>\n",
       "      <td>6.408398</td>\n",
       "      <td>4.877191</td>\n",
       "      <td>2.117341</td>\n",
       "      <td>3.184127</td>\n",
       "      <td>0.556839</td>\n",
       "      <td>5.610417</td>\n",
       "      <td>0.213898</td>\n",
       "      <td>0.768194</td>\n",
       "      <td>2.949699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63_90</th>\n",
       "      <td>1.269291</td>\n",
       "      <td>1.028049</td>\n",
       "      <td>1.419835</td>\n",
       "      <td>6.576289</td>\n",
       "      <td>5.731318</td>\n",
       "      <td>3.547912</td>\n",
       "      <td>4.001471</td>\n",
       "      <td>0.710424</td>\n",
       "      <td>6.160604</td>\n",
       "      <td>1.246903</td>\n",
       "      <td>1.591935</td>\n",
       "      <td>4.163275</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       CD4+ NV maturity and HLA-DR+ expression  \\\n",
       "45_90                                 3.253369   \n",
       "20_90                                 1.279961   \n",
       "1_90                                  3.410649   \n",
       "12_90                                 0.855045   \n",
       "21_90                                 1.123628   \n",
       "28_90                                 1.250624   \n",
       "25_90                                 0.828065   \n",
       "69_90                                 0.502712   \n",
       "54_90                                 1.777890   \n",
       "57_90                                 0.475158   \n",
       "29_90                                 1.931924   \n",
       "14_90                                 0.610545   \n",
       "40_90                                 1.494335   \n",
       "44_90                                 2.153241   \n",
       "36_90                                 0.891971   \n",
       "56_90                                 0.318195   \n",
       "3_90                                  0.710730   \n",
       "13_90                                 0.117152   \n",
       "62_90                                 2.333862   \n",
       "6_90                                  0.690855   \n",
       "52_90                                 2.228695   \n",
       "24_90                                 1.424661   \n",
       "8_90                                  2.013756   \n",
       "32_90                                 1.155353   \n",
       "48_90                                 0.759384   \n",
       "16_90                                 0.782981   \n",
       "43_90                                 4.192239   \n",
       "33_90                                 0.585137   \n",
       "51_90                                 2.078475   \n",
       "35_90                                 3.477255   \n",
       "23_90                                 0.203423   \n",
       "5_90                                  0.910888   \n",
       "4_90                                  0.602707   \n",
       "55_90                                 2.320034   \n",
       "58_90                                 1.286814   \n",
       "53_90                                 1.421329   \n",
       "67_90                                 0.425933   \n",
       "37_90                                 0.826858   \n",
       "19_90                                 0.639659   \n",
       "63_90                                 1.269291   \n",
       "\n",
       "       CD4+ NV maturity and PD-1+TIGIT+ expression  \\\n",
       "45_90                                     0.816087   \n",
       "20_90                                     0.566767   \n",
       "1_90                                      1.942530   \n",
       "12_90                                     0.532925   \n",
       "21_90                                     0.836701   \n",
       "28_90                                     0.256150   \n",
       "25_90                                     0.516745   \n",
       "69_90                                     0.461841   \n",
       "54_90                                     1.035724   \n",
       "57_90                                     0.126830   \n",
       "29_90                                     1.146924   \n",
       "14_90                                     0.209266   \n",
       "40_90                                     0.544415   \n",
       "44_90                                     0.799895   \n",
       "36_90                                     0.359883   \n",
       "56_90                                     0.073068   \n",
       "3_90                                      0.542227   \n",
       "13_90                                     0.053245   \n",
       "62_90                                     0.175346   \n",
       "6_90                                      0.202001   \n",
       "52_90                                     0.698657   \n",
       "24_90                                     1.034915   \n",
       "8_90                                      0.852270   \n",
       "32_90                                     0.423246   \n",
       "48_90                                     0.468194   \n",
       "16_90                                     0.517229   \n",
       "43_90                                     1.104379   \n",
       "33_90                                     0.242442   \n",
       "51_90                                     0.207173   \n",
       "35_90                                     0.649367   \n",
       "23_90                                     0.044007   \n",
       "5_90                                      0.332144   \n",
       "4_90                                      0.159641   \n",
       "55_90                                     1.412087   \n",
       "58_90                                     0.962520   \n",
       "53_90                                     0.236733   \n",
       "67_90                                     0.282757   \n",
       "37_90                                     0.250724   \n",
       "19_90                                     0.074879   \n",
       "63_90                                     1.028049   \n",
       "\n",
       "       CD4+ NV maturity and PD1+ expression  \\\n",
       "45_90                              1.077758   \n",
       "20_90                              1.166722   \n",
       "1_90                               2.943277   \n",
       "12_90                              1.103282   \n",
       "21_90                              1.085039   \n",
       "28_90                              0.687407   \n",
       "25_90                              0.731568   \n",
       "69_90                              0.764846   \n",
       "54_90                              1.943648   \n",
       "57_90                              0.479045   \n",
       "29_90                              1.766314   \n",
       "14_90                              0.391987   \n",
       "40_90                              1.980772   \n",
       "44_90                              2.546066   \n",
       "36_90                              1.033612   \n",
       "56_90                              0.461161   \n",
       "3_90                               1.540968   \n",
       "13_90                              0.190282   \n",
       "62_90                              1.459718   \n",
       "6_90                               0.316952   \n",
       "52_90                              2.629326   \n",
       "24_90                              1.500800   \n",
       "8_90                               1.500963   \n",
       "32_90                              1.924373   \n",
       "48_90                              1.510771   \n",
       "16_90                              1.623932   \n",
       "43_90                              1.640389   \n",
       "33_90                              0.745948   \n",
       "51_90                              1.072233   \n",
       "35_90                              1.125306   \n",
       "23_90                              0.315390   \n",
       "5_90                               1.031188   \n",
       "4_90                               0.500649   \n",
       "55_90                              2.124832   \n",
       "58_90                              1.590628   \n",
       "53_90                              1.753018   \n",
       "67_90                              0.503961   \n",
       "37_90                              0.616275   \n",
       "19_90                              0.608161   \n",
       "63_90                              1.419835   \n",
       "\n",
       "       CD4+ None maturity and None expression  \\\n",
       "45_90                                8.639425   \n",
       "20_90                                6.419449   \n",
       "1_90                                 6.595505   \n",
       "12_90                                3.902334   \n",
       "21_90                                4.676005   \n",
       "28_90                                5.252894   \n",
       "25_90                                7.044523   \n",
       "69_90                                5.279298   \n",
       "54_90                                4.460682   \n",
       "57_90                                5.487521   \n",
       "29_90                                4.992294   \n",
       "14_90                                5.907475   \n",
       "40_90                                2.724157   \n",
       "44_90                                6.881467   \n",
       "36_90                                5.176522   \n",
       "56_90                                5.757045   \n",
       "3_90                                 6.365023   \n",
       "13_90                                3.585809   \n",
       "62_90                                1.913150   \n",
       "6_90                                 6.151546   \n",
       "52_90                                4.226325   \n",
       "24_90                                7.839187   \n",
       "8_90                                 6.362247   \n",
       "32_90                                5.868175   \n",
       "48_90                                4.659357   \n",
       "16_90                                6.269076   \n",
       "43_90                                6.411233   \n",
       "33_90                                6.113829   \n",
       "51_90                                6.105971   \n",
       "35_90                                5.111683   \n",
       "23_90                                6.047823   \n",
       "5_90                                 6.374137   \n",
       "4_90                                 7.067608   \n",
       "55_90                                6.070148   \n",
       "58_90                                7.166425   \n",
       "53_90                                3.954196   \n",
       "67_90                                5.538973   \n",
       "37_90                                5.834799   \n",
       "19_90                                6.408398   \n",
       "63_90                                6.576289   \n",
       "\n",
       "       CD4+ CM maturity and CD226+ expression  \\\n",
       "45_90                                6.978063   \n",
       "20_90                                5.435524   \n",
       "1_90                                 5.557021   \n",
       "12_90                                3.515843   \n",
       "21_90                                4.109025   \n",
       "28_90                                4.410355   \n",
       "25_90                                5.439212   \n",
       "69_90                                4.812743   \n",
       "54_90                                5.023118   \n",
       "57_90                                4.172622   \n",
       "29_90                                4.643359   \n",
       "14_90                                4.384943   \n",
       "40_90                                4.932435   \n",
       "44_90                                5.130796   \n",
       "36_90                                1.516387   \n",
       "56_90                                3.322016   \n",
       "3_90                                 4.042038   \n",
       "13_90                                4.233529   \n",
       "62_90                                1.655352   \n",
       "6_90                                 3.008879   \n",
       "52_90                                4.775817   \n",
       "24_90                                4.428150   \n",
       "8_90                                 5.986539   \n",
       "32_90                                6.018745   \n",
       "48_90                                5.093613   \n",
       "16_90                                4.469730   \n",
       "43_90                                5.243027   \n",
       "33_90                                5.299384   \n",
       "51_90                                5.216498   \n",
       "35_90                                5.283467   \n",
       "23_90                                3.425778   \n",
       "5_90                                 5.722422   \n",
       "4_90                                 5.956769   \n",
       "55_90                                5.773805   \n",
       "58_90                                5.108281   \n",
       "53_90                                3.980639   \n",
       "67_90                                3.900312   \n",
       "37_90                                5.203637   \n",
       "19_90                                4.877191   \n",
       "63_90                                5.731318   \n",
       "\n",
       "       CD8+ EM maturity and None expression  \\\n",
       "45_90                              8.075357   \n",
       "20_90                              7.970050   \n",
       "1_90                               0.544861   \n",
       "12_90                              3.625189   \n",
       "21_90                              4.904480   \n",
       "28_90                              4.247289   \n",
       "25_90                              3.674965   \n",
       "69_90                              1.414310   \n",
       "54_90                              7.156520   \n",
       "57_90                              2.536713   \n",
       "29_90                             10.083559   \n",
       "14_90                              4.204570   \n",
       "40_90                              7.653666   \n",
       "44_90                              1.218489   \n",
       "36_90                              1.031684   \n",
       "56_90                              0.495635   \n",
       "3_90                               0.921524   \n",
       "13_90                              6.737907   \n",
       "62_90                              1.954355   \n",
       "6_90                               4.567745   \n",
       "52_90                              7.504838   \n",
       "24_90                              8.641090   \n",
       "8_90                               4.884773   \n",
       "32_90                              6.516363   \n",
       "48_90                              6.438854   \n",
       "16_90                              3.067922   \n",
       "43_90                              4.341182   \n",
       "33_90                              6.072987   \n",
       "51_90                              2.842161   \n",
       "35_90                              6.070404   \n",
       "23_90                              0.858494   \n",
       "5_90                               2.024931   \n",
       "4_90                               4.514472   \n",
       "55_90                              6.692558   \n",
       "58_90                              3.532285   \n",
       "53_90                              5.552259   \n",
       "67_90                              4.872421   \n",
       "37_90                              4.685435   \n",
       "19_90                              2.117341   \n",
       "63_90                              3.547912   \n",
       "\n",
       "       CD4+ CM maturity and PD-1+TIGIT- expression  \\\n",
       "45_90                                     4.457562   \n",
       "20_90                                     3.585743   \n",
       "1_90                                      3.495338   \n",
       "12_90                                     2.269227   \n",
       "21_90                                     2.519778   \n",
       "28_90                                     2.622920   \n",
       "25_90                                     3.225216   \n",
       "69_90                                     2.647623   \n",
       "54_90                                     3.826163   \n",
       "57_90                                     2.241854   \n",
       "29_90                                     3.621469   \n",
       "14_90                                     1.736275   \n",
       "40_90                                     4.305913   \n",
       "44_90                                     3.841404   \n",
       "36_90                                     0.387593   \n",
       "56_90                                     1.804745   \n",
       "3_90                                      2.852853   \n",
       "13_90                                     3.559514   \n",
       "62_90                                     1.258346   \n",
       "6_90                                      0.560130   \n",
       "52_90                                     3.924953   \n",
       "24_90                                     3.036526   \n",
       "8_90                                      4.039471   \n",
       "32_90                                     4.667646   \n",
       "48_90                                     3.994419   \n",
       "16_90                                     3.609431   \n",
       "43_90                                     3.393215   \n",
       "33_90                                     3.831525   \n",
       "51_90                                     4.180805   \n",
       "35_90                                     2.992539   \n",
       "23_90                                     1.668337   \n",
       "5_90                                      3.834479   \n",
       "4_90                                      3.229911   \n",
       "55_90                                     4.639699   \n",
       "58_90                                     3.037890   \n",
       "53_90                                     3.019416   \n",
       "67_90                                     2.590887   \n",
       "37_90                                     3.082033   \n",
       "19_90                                     3.184127   \n",
       "63_90                                     4.001471   \n",
       "\n",
       "       CD4+ NV maturity and PD-1+TIGIT- expression  \\\n",
       "45_90                                     0.433097   \n",
       "20_90                                     0.818695   \n",
       "1_90                                      2.277322   \n",
       "12_90                                     0.766866   \n",
       "21_90                                     0.417349   \n",
       "28_90                                     0.501923   \n",
       "25_90                                     0.298329   \n",
       "69_90                                     0.402603   \n",
       "54_90                                     1.483688   \n",
       "57_90                                     0.380651   \n",
       "29_90                                     1.129234   \n",
       "14_90                                     0.209266   \n",
       "40_90                                     1.802654   \n",
       "44_90                                     2.350333   \n",
       "36_90                                     0.818704   \n",
       "56_90                                     0.405663   \n",
       "3_90                                      1.294946   \n",
       "13_90                                     0.141943   \n",
       "62_90                                     1.390289   \n",
       "6_90                                      0.131462   \n",
       "52_90                                     2.476220   \n",
       "24_90                                     0.832689   \n",
       "8_90                                      1.017905   \n",
       "32_90                                     1.788593   \n",
       "48_90                                     1.302317   \n",
       "16_90                                     1.406502   \n",
       "43_90                                     0.976316   \n",
       "33_90                                     0.579259   \n",
       "51_90                                     0.962187   \n",
       "35_90                                     0.689746   \n",
       "23_90                                     0.279024   \n",
       "5_90                                      0.835782   \n",
       "4_90                                      0.376114   \n",
       "55_90                                     1.433123   \n",
       "58_90                                     1.044809   \n",
       "53_90                                     1.674600   \n",
       "67_90                                     0.264939   \n",
       "37_90                                     0.425575   \n",
       "19_90                                     0.556839   \n",
       "63_90                                     0.710424   \n",
       "\n",
       "       CD4+ CM maturity and None expression  \\\n",
       "45_90                              6.918507   \n",
       "20_90                              5.576912   \n",
       "1_90                               5.590927   \n",
       "12_90                              3.955223   \n",
       "21_90                              4.629348   \n",
       "28_90                              4.694759   \n",
       "25_90                              5.785610   \n",
       "69_90                              4.878402   \n",
       "54_90                              5.191335   \n",
       "57_90                              4.351655   \n",
       "29_90                              5.491072   \n",
       "14_90                              4.854820   \n",
       "40_90                              5.292847   \n",
       "44_90                              5.206412   \n",
       "36_90                              2.754781   \n",
       "56_90                              3.844096   \n",
       "3_90                               4.653615   \n",
       "13_90                              4.666428   \n",
       "62_90                              2.667638   \n",
       "6_90                               4.069697   \n",
       "52_90                              4.510494   \n",
       "24_90                              4.820381   \n",
       "8_90                               6.485300   \n",
       "32_90                              6.015756   \n",
       "48_90                              5.000288   \n",
       "16_90                              3.946836   \n",
       "43_90                              5.309750   \n",
       "33_90                              5.652498   \n",
       "51_90                              5.614786   \n",
       "35_90                              5.661069   \n",
       "23_90                              3.124903   \n",
       "5_90                               5.877363   \n",
       "4_90                               6.351472   \n",
       "55_90                              5.964837   \n",
       "58_90                              5.533309   \n",
       "53_90                              3.076036   \n",
       "67_90                              4.257046   \n",
       "37_90                              5.016784   \n",
       "19_90                              5.610417   \n",
       "63_90                              6.160604   \n",
       "\n",
       "       CD4+ NV maturity and TIGIT+ expression  \\\n",
       "45_90                                2.663674   \n",
       "20_90                                1.289008   \n",
       "1_90                                 2.261040   \n",
       "12_90                                0.716752   \n",
       "21_90                                1.018338   \n",
       "28_90                                0.542581   \n",
       "25_90                                0.718071   \n",
       "69_90                                0.998075   \n",
       "54_90                                1.211536   \n",
       "57_90                                0.569333   \n",
       "29_90                                1.379191   \n",
       "14_90                                0.699905   \n",
       "40_90                                0.592202   \n",
       "44_90                                1.784192   \n",
       "36_90                                0.824954   \n",
       "56_90                                0.256791   \n",
       "3_90                                 0.974001   \n",
       "13_90                                0.053245   \n",
       "62_90                                0.391041   \n",
       "6_90                                 0.925349   \n",
       "52_90                                0.908640   \n",
       "24_90                                1.207473   \n",
       "8_90                                 1.161230   \n",
       "32_90                                0.554632   \n",
       "48_90                                0.859043   \n",
       "16_90                                0.589841   \n",
       "43_90                                1.589241   \n",
       "33_90                                0.342999   \n",
       "51_90                                0.353302   \n",
       "35_90                                1.363784   \n",
       "23_90                                0.207728   \n",
       "5_90                                 0.902499   \n",
       "4_90                                 0.459391   \n",
       "55_90                                1.559668   \n",
       "58_90                                1.905044   \n",
       "53_90                                0.584963   \n",
       "67_90                                0.511537   \n",
       "37_90                                0.595515   \n",
       "19_90                                0.213898   \n",
       "63_90                                1.246903   \n",
       "\n",
       "       Th22 CM maturity and None expression  \\\n",
       "45_90                              0.750708   \n",
       "20_90                              1.907014   \n",
       "1_90                               0.395206   \n",
       "12_90                              0.125539   \n",
       "21_90                              0.765531   \n",
       "28_90                              0.842808   \n",
       "25_90                              1.444452   \n",
       "69_90                              0.400476   \n",
       "54_90                              0.793901   \n",
       "57_90                              0.534019   \n",
       "29_90                              1.245811   \n",
       "14_90                              0.099416   \n",
       "40_90                              1.452001   \n",
       "44_90                              0.150192   \n",
       "36_90                              0.117166   \n",
       "56_90                              0.284585   \n",
       "3_90                               0.064305   \n",
       "13_90                              0.217804   \n",
       "62_90                              0.019302   \n",
       "6_90                               0.103577   \n",
       "52_90                              0.109107   \n",
       "24_90                              0.152848   \n",
       "8_90                               1.064408   \n",
       "32_90                              1.099598   \n",
       "48_90                              0.829312   \n",
       "16_90                              0.124920   \n",
       "43_90                              2.102640   \n",
       "33_90                              0.554139   \n",
       "51_90                              1.488106   \n",
       "35_90                              1.677669   \n",
       "23_90                              0.122302   \n",
       "5_90                               0.696906   \n",
       "4_90                               1.882990   \n",
       "55_90                              0.263900   \n",
       "58_90                              0.341947   \n",
       "53_90                              0.035591   \n",
       "67_90                              0.793829   \n",
       "37_90                              0.596998   \n",
       "19_90                              0.768194   \n",
       "63_90                              1.591935   \n",
       "\n",
       "       CD8+ EMTM maturity and PD-1+TIGIT+ expression  \n",
       "45_90                                       6.234018  \n",
       "20_90                                       7.343317  \n",
       "1_90                                        1.590633  \n",
       "12_90                                       1.209929  \n",
       "21_90                                       3.941586  \n",
       "28_90                                       3.379009  \n",
       "25_90                                       2.959369  \n",
       "69_90                                       1.141858  \n",
       "54_90                                       6.232746  \n",
       "57_90                                       1.744961  \n",
       "29_90                                       8.642603  \n",
       "14_90                                       3.455362  \n",
       "40_90                                       6.433929  \n",
       "44_90                                       2.574576  \n",
       "36_90                                       0.630171  \n",
       "56_90                                       0.504575  \n",
       "3_90                                        0.981566  \n",
       "13_90                                       5.608378  \n",
       "62_90                                       1.727371  \n",
       "6_90                                        3.366667  \n",
       "52_90                                       7.417362  \n",
       "24_90                                       6.822212  \n",
       "8_90                                        4.300965  \n",
       "32_90                                       5.312683  \n",
       "48_90                                       4.703549  \n",
       "16_90                                       2.918688  \n",
       "43_90                                       2.001241  \n",
       "33_90                                       4.369445  \n",
       "51_90                                       1.691766  \n",
       "35_90                                       4.869467  \n",
       "23_90                                       1.661396  \n",
       "5_90                                        3.154634  \n",
       "4_90                                        3.632937  \n",
       "55_90                                       3.842778  \n",
       "58_90                                       2.439343  \n",
       "53_90                                       5.110121  \n",
       "67_90                                       2.531616  \n",
       "37_90                                       3.670380  \n",
       "19_90                                       2.949699  \n",
       "63_90                                       4.163275  "
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "e60bf057-225e-4a3d-a2fd-5a4d4b1b1eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# estimation is always with kfolds\n",
    "cv = KFold(n_splits=3)\n",
    "name = 'GVHD_prediction'\n",
    "metrics_array = {}\n",
    "X_estim = X_val\n",
    "y_estim = y_val\n",
    "thr = 0.5\n",
    "for i, (train, test) in enumerate(cv.split(X_estim, y_estim)):\n",
    "    X_fold_test, y_fold_test = X_estim.iloc[test],y_estim.iloc[test]\n",
    "    metrics_array[i] = calculate_metrics(model,X_fold_test, y_fold_test,verbose=False,ret=True,\n",
    "                                plot_curves=False, plot_correlations=False,plot_confusion_matrices=False,classifier= True,\n",
    "                               name = f\"{name}_{i}\",thr=thr)\n",
    "\n",
    "\n",
    "metrics_array = pd.DataFrame(metrics_array).T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "2f386fd2-0a55-49fd-8fc5-24580a09100f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 0.98, 'RF model predicting GVHD development (90 days)')"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axes = plt.subplots(1,2,figsize=(10,5))\n",
    "plt_mean_auc_curve(axes[0],[arr[~np.isnan(arr)] for arr in metrics_array.fpr.to_list() if len(arr[~np.isnan(arr)])>0],\n",
    "                   [arr[~np.isnan(arr)] for arr in metrics_array.tpr.to_list() if len(arr[~np.isnan(arr)])>0],\n",
    "                   [arr[~np.isnan(arr)] for arr in metrics_array.ROC_AUC.to_list() if len(arr[~np.isnan(arr)])>0],name=name,type='roc'\n",
    "              )\n",
    "plt_mean_auc_curve(axes[1],metrics_array.recall.to_list(),\n",
    "                   metrics_array.precision.to_list(),\n",
    "                   metrics_array.PR_AUC.to_list(),name=name,type='pr'\n",
    "              )\n",
    "n = round(len(y_estim),2)\n",
    "name = f'RF model predicting GVHD development (90 days)'\n",
    "\n",
    "fig.suptitle(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "36a5fb67-6d70-4754-93f1-e2ecccb87e73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "# categorization\n",
    "y_test_bin = pd.concat(metrics_array.y_test_bin.to_list())\n",
    "y_pred_bin = pd.concat(metrics_array.y_pred_bin.to_list())\n",
    "\n",
    "f1 = round(f1_score(y_test_bin,y_pred_bin, average='weighted'),2)\n",
    "name = f'RF, GVHD development prediction, F1={f1})'\n",
    "# confusion matrix\n",
    "plt_confusion_matrix(y_test_bin,y_pred_bin,thr=thr,name=name)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "default",
   "language": "python",
   "name": "default"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
